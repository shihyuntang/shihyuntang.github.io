<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shih-Yun Tang</title>
    <link>https://shihyuntang.github.io/</link>
      <atom:link href="https://shihyuntang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shih-Yun Tang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018-2024 Shih-Yun Tang</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shihyuntang.github.io/img/icon-192.png</url>
      <title>Shih-Yun Tang</title>
      <link>https://shihyuntang.github.io/</link>
    </image>
    
    <item>
      <title>Probability Basic</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</guid>
      <description>&lt;h2 id=&#34;some-terminology-in-statistics&#34;&gt;Some Terminology in Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;i.i.d. (Independent and Identically Distributed):&lt;/strong&gt;
This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis:&lt;/strong&gt;
In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P-value:&lt;/strong&gt;
The p-value is the probability of observing test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A p-value less than a pre-determined significance level, often 0.05, leads to the rejection of the null hypothesis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confidence Interval (CI):&lt;/strong&gt;
A confidence interval is a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter. The confidence level represents the probability that this interval will capture this parameter in repeated samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Type I and Type II Errors:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type I Error (False Positive):&lt;/strong&gt; Occurs when the null hypothesis is incorrectly rejected when it is actually true.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type II Error (False Negative):&lt;/strong&gt; Occurs when the null hypothesis is not rejected when it is actually false.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;first-and-second-moments&#34;&gt;First and Second Moments&lt;/h2&gt;
&lt;p&gt;In probability and statistics, the concepts of first and second moments are central to understanding the distributions of random variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;First Moment (Mean):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The first moment is the expected value of a random variable $X$, denoted as $E(X)$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt;
$$E(X) = \mu$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Second Moment (Variance):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The second moment about the mean is the variance of the random variable $X$, denoted as $\sigma^2$. It measures the spread of the data points around the mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The variance is calculated using the expectation of the &lt;em&gt;squared deviations from the mean&lt;/em&gt;:
$$
\sigma^2 = E[(X-\mu)^2] = E[X^2] - E(X)^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$$\begin{align}
\sigma^2 &amp;amp;= E[(X-\mu)^2] \\
&amp;amp;= E[X^2 - 2X\mu + \mu^2] \\
&amp;amp;= E[X^2] - 2\mu E(X) + E(\mu^2) \\
&amp;amp;= E[X^2] - 2\mu^2 + \mu^2 \\
&amp;amp;= E[X^2] - \mu^2 \\
&amp;amp;= E[X^2] - E(X)^2
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Third Moment (Skewness):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The third central moment describes the skewness of the distribution of a random variable, indicating the degree of asymmetry around the mean. Skewness can reveal whether the distribution tails off more on one side than the other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The third moment is calculated using the expectation of the cubed deviations from the mean:
$$
E[(X - E(X))^3]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive skewness indicates that the distribution has a long tail to the right (more positive side), while a negative skewness indicates a long tail to the left (more negative side).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Covariance:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Covariance measures the joint variability of two random variables, $X$ and $Y$. It assesses the degree to which two variables change together. If the greater values of one variable mainly correspond to the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The covariance between two variables $X$ and $Y$ is given by:
$$
\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive covariance indicates that as $X$ increases, $Y$ tends to increase. A negative covariance suggests that as $X$ increases, $Y$ tends to decrease. Zero covariance indicates that the variables are independent, assuming they are also uncorrelated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;p&gt;Transforming the random variable to with zero mean and unit variance. This tranasformation also removes the unit on the random variable.&lt;/p&gt;
&lt;p&gt;$$
X_{std} = \frac{X - \mu}{\sigma}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;z-test-vs-t-test&#34;&gt;Z-Test vs. T-Test&lt;/h2&gt;
&lt;p&gt;Both the z-test and the t-test are statistical methods used to test hypotheses about means, but they are suited to different situations based on the distribution of the data and sample sizes.&lt;/p&gt;
&lt;h3 id=&#34;z-test&#34;&gt;Z-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The z-test is used when the population &lt;em&gt;variance is known and the sample size is large&lt;/em&gt; (typically, n &amp;gt; 30). It can also be used for small samples if the data is known to follow a normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population variance is known.&lt;/li&gt;
&lt;li&gt;The sample size is large enough for the Central Limit Theorem to apply, which ensures that the means of the samples are normally distributed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a z-test is calculated as follows:
$$
Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The t-test is used when the population &lt;em&gt;variance is unknown and the sample size is small&lt;/em&gt;. It is the appropriate test when dealing with estimates of the standard deviation from a normally distributed sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population from which samples are drawn is normally distributed.&lt;/li&gt;
&lt;li&gt;The population variance is unknown, and the sample variance is used as an estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a t-test is calculated as follows:
$$
T = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $s$ is the sample standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Degrees of Freedom:&lt;/strong&gt; The degrees of freedom for the t-test are $n-1$, which affects the shape of the t-distribution used to determine the p-value.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-differences&#34;&gt;Key Differences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Deviation:&lt;/strong&gt; The z-test uses the population standard deviation, while the t-test uses the sample&amp;rsquo;s standard deviation as an estimate of the population’s standard deviation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size:&lt;/strong&gt; The z-test is typically used for larger sample sizes or when the population standard deviation is known, whereas the t-test is used for smaller sample sizes or when the population standard deviation is unknown.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution:&lt;/strong&gt; The z-test statistic follows a normal distribution, while the t-test statistic follows a t-distribution, which is more spread out with heavier tails, providing a more conservative test for small sample sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;A comparison plot between the t-distribution and the standard normal distribution can be find &lt;a href=&#34;https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html#:~:text=What&#39;s%20the%20key%20difference%20between,on%20the%20sample%20standard%20deviation.&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributions</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</guid>
      <description>&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;Probability distributions describe how the probabilities of a &lt;strong&gt;random variable&lt;/strong&gt; are distributed. Here are the two main types of probability functions associated with these distributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Probability Density Function (PDF, continuous):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The area under the PDF curve between two points represents the probability of the variable falling within that range.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Probability Mass Function (PMF, discrete):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The binomial distribution, and the Poisson distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Distribution Function (CDF):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The CDF is used to determine the probability that a random variable (X) is $\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chebyshevs-theorem&#34;&gt;Chebyshev&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem, also known as Chebyshev&amp;rsquo;s Inequality, is a fundamental result in probability theory that provides a way &lt;strong&gt;to estimate the probability that a random variable differs from its mean&lt;/strong&gt;. This theorem is not restricted to normally distributed data, making it very versatile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem Statement:&lt;/strong&gt;
$$
P(|X-\mu| &amp;lt; k\sigma) \geq 1 - \frac{1}{k^2}
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;($\mu$) is the mean of the random variable ( X ),&lt;/li&gt;
&lt;li&gt;($\sigma$) is the standard deviation of ( X ),&lt;/li&gt;
&lt;li&gt;($k$) is a positive number greater than 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generality:&lt;/strong&gt; Chebyshev&amp;rsquo;s theorem applies to any probability distribution where the mean and variance are defined.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implication:&lt;/strong&gt; The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\frac{1}{4} = 0.75)$).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discrete-distributions&#34;&gt;Discrete Distributions&lt;/h2&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as &amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are only two possible outcomes for each trial: success (1) and failure (0).&lt;/li&gt;
&lt;li&gt;The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability mass function (PMF) for the Binomial distribution is expressed as:
$$
P(X=x) = C^{n}_{x} \theta^x (1-\theta)^{n-x}
$$
where $\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Consider flipping a fair coin five times. What is the probability of getting exactly two heads?&lt;/p&gt;
&lt;p&gt;The calculation is as follows:
$$
P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \frac{5!}{2!3!} \times 0.25 \times 0.125 = 31.25%
$$&lt;/p&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;p&gt;The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF:&lt;/strong&gt;
$$
g(x; \theta) = \theta(1-\theta)^{x-1}
$$
where, $\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Properties:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean ($\mu$):&lt;/strong&gt; The expected number of trials to get the first success is given by:
$$
\mu = \frac{1}{\theta}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance ($\sigma^2$):&lt;/strong&gt; The variance of the number of trials to get the first success is:
$$
\sigma^2 = \frac{1-\theta}{\theta^2}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean and variance provide insights into the &amp;ldquo;spread&amp;rdquo; or variability of trials needed to achieve the first success, with higher values of $\theta$ leading to fewer expected trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distribution is commonly used in &lt;em&gt;quality control&lt;/em&gt;, &lt;em&gt;reliability testing&lt;/em&gt;, and other areas where the &amp;ldquo;time&amp;rdquo; or number of trials until the first success is of interest.&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;
&lt;p&gt;A practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Independence of events holds only if the CCD is not saturated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rate of event occurrence, $\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \Delta t$ is $\lambda \Delta t$.&lt;/li&gt;
&lt;li&gt;The probability of an event in the interval ${t, t+\Delta t}$ is independent of previous events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The probability mass function (PMF) of the Poisson distribution is defined as:
$$
P(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
where $\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.&lt;/p&gt;
&lt;p&gt;For the Poisson distribution, the expected value (mean) is $\mu = \lambda$, and the variance is $\sigma^2 = \lambda$.&lt;/p&gt;
&lt;h4 id=&#34;poisson-noise-shot-noise&#34;&gt;Poisson Noise (Shot Noise)&lt;/h4&gt;
&lt;p&gt;Since $\sigma^2 = \lambda$, the standard deviation (Poisson noise) is $\sigma = \sqrt{\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\sigma = \sqrt{N}$.&lt;/p&gt;
&lt;p&gt;Due to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows:
$$
SNR = \frac{\mu}{\sigma} = \frac{\mu}{\sqrt{\mu}} = \sqrt{\mu}
$$
This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bayes&#39; theorem</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</guid>
      <description>&lt;p&gt;Bayes Theorem is a fundamental principle in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It is particularly powerful in the context of predictive modeling and decision-making processes.&lt;/p&gt;
&lt;h3 id=&#34;mathematical-formulation&#34;&gt;Mathematical Formulation&lt;/h3&gt;
&lt;p&gt;If events $A$ and $B$ are independent, then the probability of $A$ given $B$ is simply the probability of $A$:
$$
P(A|B) = P(A)
$$
However, when events $A$ and $B$ are not independent, the relationship changes as follows:
$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{and} \quad P(B|A) = \frac{P(A \cap B)}{P(A)}
$$
From these relationships, we derive Bayes&amp;rsquo; Theorem:
$$
P(B|A) = \frac{P(B) P(A|B)}{P(A)}
$$
Bayes&amp;rsquo; Theorem can be interpreted in terms of updating beliefs:
$$
\text{Posterior} = \frac{\text{Prior} \times \text{Likelihood}}{\text{Evidence}}
$$&lt;/p&gt;
&lt;h3 id=&#34;components-of-bayes-theorem&#34;&gt;Components of Bayes&amp;rsquo; Theorem&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Likelihood ( $ P(A|B) $ ):&lt;/strong&gt;
The likelihood is a function that measures the plausibility of a model parameter value given specific observed data. In many applications, especially in statistical modeling, the likelihood is assumed to follow a normal distribution:
$$
L(\theta; x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}}
$$
where $\theta$ represents the parameter of interest, $x$ represents the data, and $\sigma^2$ is the variance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior ( $ P(B) $ ):&lt;/strong&gt;
The prior represents the initial belief about the distribution of the parameter before considering the current data. Priors can be subjective or based on previous studies:
$$
P(\theta)
$$
It can be uniform (representing no initial preference) or follow a specific distribution that reflects prior knowledge about the parameter, e.g., a Gaussian prior, gamma and beta distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evidence or Normalizing Constant ( $ P(A) $ ):&lt;/strong&gt;
Often considered as a normalizing factor, the evidence ensures that the posterior probabilities sum to one. It is calculated as:
$$
P(A) = \int P(A|B) P(B) dB
$$
This factor is essential for the probabilistic model to be valid but is usually more relevant in analytical calculations than in practical applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Posterior ( $ P(B|A) $ ):&lt;/strong&gt;
The posterior probability reflects the updated belief about the parameter after considering the new evidence. It combines the prior and the likelihood given new data:
$$
P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta) d\theta}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;practical-applications&#34;&gt;Practical Applications&lt;/h2&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem is used extensively in various fields including exoplanet study, machine learning, medical testing, and any scenario requiring iterative updating of beliefs upon new evidence. Understanding how to apply the theorem allows for more informed decision-making processes and predictions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;relationship-between-likelihood-and-chi-squared-statistic&#34;&gt;Relationship Between Likelihood and Chi-Squared Statistic&lt;/h2&gt;
&lt;p&gt;The likelihood function in statistical models often measures how well a set of parameters fits the data. When the data and the model predictions vary according to a normal distribution, the likelihood function can be directly linked to the chi-squared statistic.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-statistic&#34;&gt;Chi-Squared Statistic:&lt;/h3&gt;
&lt;p&gt;The chi-squared statistic is a measure of how expectations compare to actual observed data. In the context of likelihood calculations, the chi-squared statistic quantifies the discrepancy between observed data and the data predicted by the model, under the assumption that the discrepancies are normally distributed.&lt;/p&gt;
&lt;h3 id=&#34;calculation&#34;&gt;Calculation:&lt;/h3&gt;
&lt;p&gt;To calculate the chi-squared statistic as a measure of likelihood, you can use the following formula:
$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{\sigma_i^2}
$$
where $O_i$ are the observed values, $E_i$ are the expected values predicted by the model, and $\sigma_i^2$ is the variance associated with each observation.&lt;/p&gt;
&lt;h3 id=&#34;using-chi-squared-to-calculate-likelihood&#34;&gt;Using Chi-Squared to Calculate Likelihood:&lt;/h3&gt;
&lt;p&gt;The likelihood of observing the data given the model can be expressed as:
$$
L = e^{-\chi^2/2}
$$
This formulation arises from the exponential component of the PDF of the normal distribution, which is what a likelihood function usually take, thus, a easier way to remember how to calculate the likelihood.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distribution</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-1-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-1-distribution/</guid>
      <description>&lt;h2 id=&#34;empirical-distribution-function-edf&#34;&gt;Empirical Distribution Function (EDF)&lt;/h2&gt;
&lt;p&gt;The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as:
$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n I[x_i \leq x]
$$
where $I[\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\frac{1}{n}$. An example plot of an EDF is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/edf.png&#34; alt=&#34;edf&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kolmogorov-smirnov-test-ks-test&#34;&gt;Kolmogorov-Smirnov Test (KS Test)&lt;/h2&gt;
&lt;p&gt;The Kolmogorov-Smirnov (KS) Test measures the maximum distance ($D$) between two distribution functions. This can be used to compare an empirical distribution with a theoretical model (one-sample test) or two empirical distributions (two-sample test). If the distributions are identical, $D$ equals zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
D_n = \max_x |F_n(x) - S_n(x)|
$$
A table for the critical values of $D_n$ can be found &lt;a href=&#34;https://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If $D_n$ exceeds the critical value, we can &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that there is no significant difference between the two distributions.&lt;/p&gt;
&lt;h2 id=&#34;cramér-von-mises-statistic-cvm&#34;&gt;Cramér-von Mises Statistic (CvM)&lt;/h2&gt;
&lt;p&gt;The Cramér-von Mises statistic is used to quantify the goodness of fit of an empirical distribution to a theoretical model. It is particularly useful as it considers the squared differences over all points, providing a more sensitive measure to differences &lt;strong&gt;in the tails&lt;/strong&gt; of the distributions.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
C_n = n \int_{-\infty}^\infty [F_n(x) - S(x)]^2 dS(x)
$$ --&gt;
&lt;p&gt;This statistic assesses the integrated squared distance between the empirical distribution function $F_n(x)$ and the theoretical distribution $S(x)$, weighted by the number of observations $n$.&lt;/p&gt;
&lt;h2 id=&#34;anderson-darling-statistic-ad&#34;&gt;Anderson-Darling Statistic (AD)&lt;/h2&gt;
&lt;p&gt;The Anderson-Darling statistic is a modification of the Cramér-von Mises statistic that gives more weight to the tails of the distribution. It is particularly effective in identifying departures from a theoretical distribution in the tails.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
A^2 = n \int_{-\infty}^\infty \frac{[F_n(x) - S(x)]^2}{S(x)(1 - S(x))} \, dx
$$ --&gt;
&lt;p&gt;This weighted approach makes the AD statistic more sensitive to discrepancies in the distribution&amp;rsquo;s tails than the CvM statistic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Both CvM and AD tests are powerful tools for statistical hypothesis testing, especially in scenarios where understanding the tail behavior of distributions is crucial.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;wilcoxon-rank-sum-test&#34;&gt;Wilcoxon Rank Sum Test&lt;/h2&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test, also known as the Mann-Whitney Rank Sum test, is designed to assess &lt;em&gt;whether two independent samples come from the same distribution&lt;/em&gt;. Here’s how the test statistic is calculated:&lt;/p&gt;
&lt;!-- It is especially useful when the data does not meet the assumptions necessary for the t-test, primarily concerning normality --&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Combine and Rank the Data:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Combine all observations from both samples into a single dataset.&lt;/li&gt;
&lt;li&gt;Rank all observations from the smallest to largest. Ties are given a rank equal to the average of the ranks they would have otherwise occupied.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate the Rank Sums:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Calculate the sum of the ranks for observations from each sample separately. Let $T_1$ be the sum of ranks for the first sample, and $T_2$ for the second sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute the Test Statistic:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The test statistic $U$ is calculated using:
$$
U = T_1 - \frac{n_1(n_1+1)}{2}
$$
where $n_1$ is the number of observations in the first sample. $U$ can also be computed for the second sample, and the smaller of the two $U$ values is often used as the test statistic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine the Significance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The significance of the observed $T_1$ value is determined by comparing it to values in a reference distribution, which approximates a normal distribution under the null hypothesis when the sample sizes are sufficiently large. The mean and standard deviation of $T_1$ are used to compute a z-score:
$$
|z| = \left| \frac{T_1 - \text{mean}(T_1)}{\text{std dev}(T_1)} \right|
$$
where $\text{mean}(T_1) = \frac{n_1(n_1+n_2+1)}{2}$, and $\text{Var}(T_1) = \frac{n_1 n_2(n_1+n_2+1)}{2}$.&lt;/li&gt;
&lt;li&gt;The p-value is then calculated from the normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ### Interpretation

- If the p-value is less than the chosen significance level (commonly 0.05), then there is sufficient evidence to reject the null hypothesis, suggesting that there is a statistically significant difference in the distributions of the two groups.

- If the p-value is greater, then we do not reject the null hypothesis, suggesting that any observed differences could reasonably occur by random chance under the assumption of identical distributions. --&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test does not require the data to follow a specific distribution, making it a robust and widely applicable non-parametric method for comparing two samples.&lt;/p&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;p&gt;The Kruskal-Wallis (KW) test is used to determine if there are statistically significant differences between the distributions of two or more groups of an independent variable. It generalizes the Wilcoxon Rank Sum Test to more than two groups. The null hypothesis assumes that all groups come from identical distributions. The test statistic follows a chi-squared ($\chi^2$) distribution with $k-1$ degrees of freedom, where $k$ is the number of groups.&lt;/p&gt;
&lt;h2 id=&#34;comparison-of-statistical-distribution-tests&#34;&gt;Comparison of Statistical Distribution Tests&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Key Usage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirement&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Distribution Assumption&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;KS Test&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing a sample with a reference distribution&lt;/td&gt;
&lt;td&gt;One or two samples, continuous or ordinal&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in location, scale, and shape&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CvM Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit to a theoretical distribution&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Integrates squared differences; sensitive across entire distribution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AD Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit with emphasis on tail differences&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Increased weight to tails; highly sensitive to tail discrepancies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Wilcoxon Rank Sum&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing two independent samples&lt;/td&gt;
&lt;td&gt;Two independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in medians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kruskal-Wallis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing more than two independent samples&lt;/td&gt;
&lt;td&gt;Two or more independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Generalization of Wilcoxon, sensitive to differences across multiple samples&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-2-correlation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-2-correlation/</guid>
      <description>&lt;p&gt;Nonparametric correlation tests are essential tools for assessing the strength and direction of a relationship between two datasets, especially when the underlying distributions are unknown or non-normal. These tests are robust alternatives to the Pearson correlation coefficient, suitable for various types of relationships. The null hypothesis for these tests is that there is no correlation between the datasets.&lt;/p&gt;
&lt;h3 id=&#34;pearson-correlation-coefficient&#34;&gt;*Pearson Correlation Coefficient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson correlation test is a &lt;strong&gt;parametric test&lt;/strong&gt;, but I put it here for completeness and for comparison.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Pearson correlation coefficient measures the linear relationship between two continuous variables. It is a parametric test and assumes that the data is normally distributed. The coefficient varies between -1 and +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.&lt;/p&gt;
&lt;h3 id=&#34;spearmans-rank-correlation-coefficient-rho&#34;&gt;Spearman&amp;rsquo;s Rank Correlation Coefficient ($\rho$)&lt;/h3&gt;
&lt;p&gt;Spearman&amp;rsquo;s correlation assesses how well the relationship between two variables can be described using a monotonic function. It does not require the data to be normally distributed, as it uses the rank of the data rather than the actual values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\rho = 1 - \frac{6 \sum d_i^2 + T}{n(n^2 - 1)}
$$
where $d_i$ is the difference between the ranks of corresponding variables and $n$ is the number of observations. For handling ties, the correction term $T$ is applied:
$$
T = \sum t_x \left[ \frac{x^3 - x}{12} \right]
$$
where $t_x$ is the number of ties involving $x$ elements. To further estimate the $p$-value, the $z$ score can be calculated by:
$$
z = \sqrt{n-1} \rho
$$
which approximates a normal distribution under the null hypothesis. One can use the $\rho$ statistic and compare to the critical values &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/spearman.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kendalls-tau-tau&#34;&gt;Kendall&amp;rsquo;s Tau ($\tau$)&lt;/h3&gt;
&lt;p&gt;Kendall&amp;rsquo;s Tau measures the ordinal association between two variables by considering the number of concordant and discordant pairs. It is less sensitive to outliers than Pearson and can be more interpretable in terms of the proportion of concordant pairs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\tau = \frac{N_c - N_d}{\sqrt{(N_c + N_d + T)(N_c + N_d + U)}}
$$
where $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $T$ is the number of ties on one variable, and $U$ is the number of ties on the other variable.&lt;/p&gt;
&lt;p&gt;The critical values for $\tau$ can be find &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/kendall.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;comparison-of-correlation-tests&#34;&gt;Comparison of Correlation Tests&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Best Use Case&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirements&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Pearson&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Linear relationships&lt;/td&gt;
&lt;td&gt;Highly sensitive to linear trends&lt;/td&gt;
&lt;td&gt;Normal distribution, continuous data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Spearman&amp;rsquo;s rho&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Monotonic relationships, not necessarily linear&lt;/td&gt;
&lt;td&gt;Sensitive to monotonic trends, not affected by outliers&lt;/td&gt;
&lt;td&gt;Ordinal data or non-normal distributions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kendall&amp;rsquo;s tau&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;General increasing or decreasing trends, less intensive computation&lt;/td&gt;
&lt;td&gt;Less sensitive to errors in data, good for small samples or data with many ties&lt;/td&gt;
&lt;td&gt;Ordinal data, robust against outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Data smoothing-density estimation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/6-data-smoothing/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/6-data-smoothing/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/7-regression/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/7-regression/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multivariate Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/8-multivariate-analysis/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/8-multivariate-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustering and Classification</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-clustering-and-classification/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-clustering-and-classification/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Censored and Truncated Data</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/10-censored-and-truncated-data/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/10-censored-and-truncated-data/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timeseries Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/11-timeseries-analysis/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/11-timeseries-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variogram</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/12-1-variogram/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/12-1-variogram/</guid>
      <description>&lt;h2 id=&#34;introduction-to-variograms&#34;&gt;Introduction to Variograms&lt;/h2&gt;
&lt;p&gt;A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: &amp;ldquo;nugget,&amp;rdquo; &amp;ldquo;sill,&amp;rdquo; and &amp;ldquo;range.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nugget:&lt;/strong&gt; Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sill:&lt;/strong&gt; The plateau reached by the variogram, beyond which the increments in distance do not significantly increase the variance. It represents the level of total variance within the dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; The distance at which the variogram reaches the sill, beyond which locations are no longer correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-variogram&#34;&gt;Example of Spatial Data and Variogram&lt;/h2&gt;
&lt;p&gt;Plots below show an example of spatial data (left) and its associated variogram (right). The plot on the left shows synthetic data in spatial X-Y coordinates color-coded by the level of toxicity measured at that location. The size of the circle is associated with the measurement error, which is not used here. The variogram on the right shows how the semi-variance between points increases with distance. It features a &amp;ldquo;nugget&amp;rdquo; effect at the origin, indicating measurement noise or microscale variability. The curve approaches a &amp;ldquo;sill,&amp;rdquo; beyond which the variance stabilizes, suggesting that points beyond this &amp;ldquo;range&amp;rdquo; do not influence each other. This range is critical for understanding the spatial continuity and predicting values at unsampled locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/variogram.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-variogram&#34;&gt;Example R Code for the Variogram&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(gstat)
library(sp)

# Defining spatial coordinates
# df_2 is a dataframe with colums x and y 
coordinates(df_2) &amp;lt;- ~x+y  

# Creating Variograms
variogram_tox &amp;lt;- variogram(toxicity ~ 1, df_2)

# Fit the variogram and plot it out.
# gamma: the semi-variance
# vgm: &amp;quot;variogram model,&amp;quot; 
model_tox &amp;lt;- fit.variogram(
  variogram_tox, model = vgm(psill = max(variogram_tox$gamma), 
  model = &amp;quot;Sph&amp;quot;, 
  range = 30))

# Plot the empirical variogram and the fitted model
plot(variogram_tox, model = model_tox, 
  main = &amp;quot;Toxicity Variogram with Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kriging</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/12-2-kriging/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/12-2-kriging/</guid>
      <description>&lt;h2 id=&#34;introduction-to-kriging&#34;&gt;Introduction to Kriging&lt;/h2&gt;
&lt;p&gt;Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Kriging:&lt;/strong&gt; Assumes the mean of the random field is known and constant throughout the region of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ordinary Kriging:&lt;/strong&gt; Assumes the &lt;em&gt;mean is unknown&lt;/em&gt; but constant within the region of interest and is the most commonly used form as one do not know the mean in real world.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-on-variogram&#34;&gt;More on Variogram&lt;/h2&gt;
&lt;p&gt;The semivariance $\gamma(x_1, x_2)$ between two points can be expressed as:
$$
\gamma(d) = \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2
$$
where $n$ is the number of pairs, $d$ is the distance between two points, and $z$ represents the values at the locations. The calculation of $\gamma(d)$ involves several key assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stationary (homogeneous):&lt;/strong&gt; Assumes that the statistical properties (mean, variance) of the process do not change over space. This implies that the mean and variance are constant throughout the region of interest, and the covariance between any two points depends only on the distance and direction between them, not on their absolute locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isotropy:&lt;/strong&gt; Assumes that the statistical properties are the same in all directions. This means that the variogram is a function only of the distance between sample points, not of the direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Kriging, we use $\gamma$ to weight the data for interpolation:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\gamma(d) &amp;amp;= \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 \\
&amp;amp;= \frac{1}{2} E\left(\left[z(x+d) - z(x)\right]^2\right)
\text{(homogeneous assumption)} \\
&amp;amp;= \frac{1}{2} \left\{ E\left[ z^2(x+d) \right] + E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\} \\
&amp;amp;= \frac{1}{2} \left\{ 2E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\}
\text{(homogeneous assumption)} \\
&amp;amp;= \sigma_x^2 + \mu_x^2 - \text{cov}[z(x), z(x+d)] - \mu_x^2
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the relationships: $E[x^2] = \sigma_x^2 + [E(x)]^2$ and $\text{cov}[X,Y] = E(XY) - E(X)E(Y)$ were used to get the second to last equation. We have:
$$\begin{align}
\gamma(d) &amp;amp;= \sigma^2 - \text{cov}[z(x), z(x+d)] \\
&amp;amp;= \sigma^2 - \text{c}(d)
\end{align}$$&lt;/p&gt;
&lt;p&gt;Therefore, we have the semivariance at distance $d$ is the variance minus the covariance between points at this distance.&lt;/p&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-the-kriging-result&#34;&gt;Example of Spatial Data and the Kriging Result&lt;/h2&gt;
&lt;p&gt;The plots below show an example of spatial data (left) and its associated Kriging map (right). The data used here is the same as in the Variogram page, and the Kriging map uses the variogram model shown on the Variogram page to predict values at unsampled locations. This example demonstrates how Kriging utilizes the spatial structure of the data, as defined by the variogram, to provide a statistically optimal interpolation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/kriging.png&#34; alt=&#34;kriging&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-kriging&#34;&gt;Example R Code for the Kriging&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Fit a variogram model
vgm_model &amp;lt;- vgm(psill = max(variogram_tox$gamma), 
       model = &amp;quot;Sph&amp;quot;, range = 30)
       
x.range &amp;lt;- range(df_2$x)
y.range &amp;lt;- range(df_2$y)

grid.points &amp;lt;- expand.grid(
       x = seq(from = x.range[1], to = x.range[2], by = 1),
       y = seq(from = y.range[1], to = y.range[2], by = 1))

# Convert to SpatialPoints
grid &amp;lt;- SpatialPoints(grid.points)

# Perform ordinary kriging
kriged &amp;lt;- krige(toxicity ~ 1, df_2, model = vgm_model, newdata = grid)

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res=200)
# Create a map with overlay contours
spplot(kriged, &amp;quot;var1.pred&amp;quot;, main = &amp;quot;Kriging Map for Toxicity&amp;quot;, 
       xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;,
       sp.layout = list(&amp;quot;sp.points&amp;quot;, df_2, col = &amp;quot;green&amp;quot;),
       colorkey = TRUE,
       scales = list(draw = TRUE))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://shihyuntang.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Binary Star Evolution in Different Environments: Filamentary, Fractal, Halo and Tidal-tail Clusters</title>
      <link>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Star-Crossed Lovers DI Tau A and B: Orbit Characterization and Physical Properties Determination</title>
      <link>https://shihyuntang.github.io/publication/202303_ditau_apj/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202303_ditau_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical Origin for the Collinder 132-Gulliver 21 Stream: A Mixture of three Co-Moving Populations with an Age Difference of 250 Myr</title>
      <link>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3 II: Hierarchical Star Formation Revealed by Spatial and Kinematic Substructures</title>
      <link>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Disruption of Hierarchical Clustering in the Vela OB2 Complex and the Cluster Pair Collinder 135 and UBC 7 with Gaia EDR3: Evidence of Supernova Quenching</title>
      <link>https://shihyuntang.github.io/publication/202109_velob_apj-/</link>
      <pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202109_velob_apj-/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impacts of Water Latent Heat on the Thermal Structure of Ultra-Cool Objects: Brown Dwarfs and Free-Floating Planets</title>
      <link>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IGRINS RV: A Python Package for Precision Radial Velocities with Near-Infrared Spectra</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IGRINS RV: A Precision RV Pipeline for IGRINS Using Modified Forward-Modeling in the Near-Infrared</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</link>
      <pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3: its Relation to Cluster Dynamics</title>
      <link>https://shihyuntang.github.io/publication/202102_oc3d_apj/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202102_oc3d_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Different Fates of Young Star Clusters After Gas Expulsion</title>
      <link>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Near-Infrared Radial Velocity pipeline (IGRINS_RV)</title>
      <link>https://shihyuntang.github.io/project/near-infrared-radial-velocity-pipeline-igrins_rv/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/near-infrared-radial-velocity-pipeline-igrins_rv/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Cluster Dynamic Study</title>
      <link>https://shihyuntang.github.io/project/open-cluster-dynamic-study/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/open-cluster-dynamic-study/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Y-dwarf Atmosphere Study</title>
      <link>https://shihyuntang.github.io/project/y-dwarf-atmosphere-study/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/y-dwarf-atmosphere-study/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diagnosing the Stellar Population and Tidal Structure of the Blanco1 Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/202001_blanco1_apj/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202001_blanco1_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Discovery of Tidal Tails in Disrupting Open Clusters: Coma Berenices and a Neighbor Stellar Group</title>
      <link>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://shihyuntang.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterization of Stellar and Substellar Members in the Coma Berenices Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/201808_comber_apj/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201808_comber_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://shihyuntang.github.io/project_example/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://shihyuntang.github.io/project_example/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
