<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shih-Yun Tang</title>
    <link>https://shihyuntang.github.io/</link>
      <atom:link href="https://shihyuntang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shih-Yun Tang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018-2024 Shih-Yun Tang</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shihyuntang.github.io/img/icon-192.png</url>
      <title>Shih-Yun Tang</title>
      <link>https://shihyuntang.github.io/</link>
    </image>
    
    <item>
      <title>Probability Basic</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</guid>
      <description>&lt;h2 id=&#34;some-terminology-in-statistics&#34;&gt;Some Terminology in Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;i.i.d. (Independent and Identically Distributed):&lt;/strong&gt;
This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis:&lt;/strong&gt;
In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P-value:&lt;/strong&gt;
The p-value is the probability of observing test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A p-value less than a pre-determined significance level, often 0.05, leads to the rejection of the null hypothesis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confidence Interval (CI):&lt;/strong&gt;
A confidence interval is a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter. The confidence level represents the probability that this interval will capture this parameter in repeated samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- - **Type I and Type II Errors:**
  - **Type I Error (False Positive):** Occurs when the null hypothesis is incorrectly rejected when it is actually true.
  - **Type II Error (False Negative):** Occurs when the null hypothesis is not rejected when it is actually false. --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;first-and-second-moments&#34;&gt;First and Second Moments&lt;/h2&gt;
&lt;p&gt;In probability and statistics, the concepts of first and second moments are central to understanding the distributions of random variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;First Moment (Mean):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The first moment is the expected value of a random variable $X$, denoted as $E(X)$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt;
$$E(X) = \mu$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Moment (Variance):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The second moment about the mean is the variance of the random variable $X$, denoted as $\sigma^2$. It measures the spread of the data points around the mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The variance is calculated using the expectation of the &lt;em&gt;squared deviations from the mean&lt;/em&gt;:
$$
\sigma^2 = E[(X-\mu)^2] = E[X^2] - E(X)^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$$\begin{align}
\sigma^2 &amp;amp;= E[(X-\mu)^2] \\
&amp;amp;= E[X^2 - 2X\mu + \mu^2] \\
&amp;amp;= E[X^2] - 2\mu E(X) + E(\mu^2) \\
&amp;amp;= E[X^2] - 2\mu^2 + \mu^2 \\
&amp;amp;= E[X^2] - \mu^2 \\
&amp;amp;= E[X^2] - E(X)^2
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Third Moment (Skewness):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The third central moment describes the skewness of the distribution of a random variable, indicating the degree of asymmetry around the mean. Skewness can reveal whether the distribution tails off more on one side than the other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The third moment is calculated using the expectation of the cubed deviations from the mean:
$$
E[(X - E(X))^3]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive skewness indicates that the distribution has a long tail to the right (more positive side), while a negative skewness indicates a long tail to the left (more negative side).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Covariance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Covariance measures the joint variability of two random variables, $X$ and $Y$. It assesses the degree to which two variables change together. If the greater values of one variable mainly correspond to the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The covariance between two variables $X$ and $Y$ is given by:
$$
\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive covariance indicates that as $X$ increases, $Y$ tends to increase. A negative covariance suggests that as $X$ increases, $Y$ tends to decrease. Zero covariance indicates that the variables are independent, assuming they are also uncorrelated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;p&gt;Transforming the random variable to with zero mean and unit variance. This tranasformation also removes the unit on the random variable.&lt;/p&gt;
&lt;p&gt;$$
X_{std} = \frac{X - \mu}{\sigma}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;z-test-vs-t-test&#34;&gt;Z-Test vs. T-Test&lt;/h2&gt;
&lt;p&gt;Both the z-test and the t-test are statistical methods used to test hypotheses about means, but they are suited to different situations based on the distribution of the data and sample sizes.&lt;/p&gt;
&lt;h3 id=&#34;z-test&#34;&gt;Z-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The z-test is used when the population &lt;em&gt;variance is known and the sample size is large&lt;/em&gt; (typically, n &amp;gt; 30). It can also be used for small samples if the data is known to follow a normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population variance is known.&lt;/li&gt;
&lt;li&gt;The sample size is large enough for the Central Limit Theorem to apply, which ensures that the means of the samples are normally distributed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a z-test is calculated as follows:
$$
Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The t-test is used when the population &lt;em&gt;variance is unknown and the sample size is small&lt;/em&gt;. It is the appropriate test when dealing with estimates of the standard deviation from a normally distributed sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population from which samples are drawn is normally distributed.&lt;/li&gt;
&lt;li&gt;The population variance is unknown, and the sample variance is used as an estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a t-test is calculated as follows:
$$
T = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $s$ is the sample standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Degrees of Freedom:&lt;/strong&gt; The degrees of freedom for the t-test are $n-1$, which affects the shape of the t-distribution used to determine the p-value.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-differences&#34;&gt;Key Differences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Deviation:&lt;/strong&gt; The z-test uses the population standard deviation, while the t-test uses the sample&amp;rsquo;s standard deviation as an estimate of the population’s standard deviation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size:&lt;/strong&gt; The z-test is typically used for larger sample sizes or when the population standard deviation is known, whereas the t-test is used for smaller sample sizes or when the population standard deviation is unknown.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution:&lt;/strong&gt; The z-test statistic follows a normal distribution, while the t-test statistic follows a t-distribution, which is more spread out with heavier tails, providing a more conservative test for small sample sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;A comparison plot between the t-distribution and the standard normal distribution can be find &lt;a href=&#34;https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html#:~:text=What&#39;s%20the%20key%20difference%20between,on%20the%20sample%20standard%20deviation.&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributions</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</guid>
      <description>&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;Probability distributions describe how the probabilities of a &lt;strong&gt;random variable&lt;/strong&gt; are distributed. Here are the two main types of probability functions associated with these distributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Probability Density Function (PDF, continuous):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The area under the PDF curve between two points represents the probability of the variable falling within that range.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability Mass Function (PMF, discrete):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The binomial distribution, and the Poisson distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Distribution Function (CDF):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The CDF is used to determine the probability that a random variable (X) is $\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chebyshevs-theorem&#34;&gt;Chebyshev&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem, also known as Chebyshev&amp;rsquo;s Inequality, is a fundamental result in probability theory that provides a way &lt;strong&gt;to estimate the probability that a random variable differs from its mean&lt;/strong&gt;. This theorem is not restricted to normally distributed data, making it very versatile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem Statement:&lt;/strong&gt;
$$
P(|X-\mu| &amp;lt; k\sigma) \geq 1 - \frac{1}{k^2}
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;($\mu$) is the mean of the random variable ( X ),&lt;/li&gt;
&lt;li&gt;($\sigma$) is the standard deviation of ( X ),&lt;/li&gt;
&lt;li&gt;($k$) is a positive number greater than 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generality:&lt;/strong&gt; Chebyshev&amp;rsquo;s theorem applies to any probability distribution where the mean and variance are defined.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implication:&lt;/strong&gt; The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\frac{1}{4} = 0.75)$).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discrete-distributions&#34;&gt;Discrete Distributions&lt;/h2&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as &amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are only two possible outcomes for each trial: success (1) and failure (0).&lt;/li&gt;
&lt;li&gt;The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability mass function (PMF) for the Binomial distribution is expressed as:
$$
P(X=x) = C^{n}_{x} \theta^x (1-\theta)^{n-x}
$$
where $\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Consider flipping a fair coin five times. What is the probability of getting exactly two heads?&lt;/p&gt;
&lt;p&gt;The calculation is as follows:
$$
P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \frac{5!}{2!3!} \times 0.25 \times 0.125 = 31.25%
$$&lt;/p&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;p&gt;The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF:&lt;/strong&gt;
$$
g(x; \theta) = \theta(1-\theta)^{x-1}
$$
where, $\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Properties:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean ($\mu$):&lt;/strong&gt; The expected number of trials to get the first success is given by:
$$
\mu = \frac{1}{\theta}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance ($\sigma^2$):&lt;/strong&gt; The variance of the number of trials to get the first success is:
$$
\sigma^2 = \frac{1-\theta}{\theta^2}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean and variance provide insights into the &amp;ldquo;spread&amp;rdquo; or variability of trials needed to achieve the first success, with higher values of $\theta$ leading to fewer expected trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distribution is commonly used in &lt;em&gt;quality control&lt;/em&gt;, &lt;em&gt;reliability testing&lt;/em&gt;, and other areas where the &amp;ldquo;time&amp;rdquo; or number of trials until the first success is of interest.&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;
&lt;p&gt;A practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Independence of events holds only if the CCD is not saturated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rate of event occurrence, $\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \Delta t$ is $\lambda \Delta t$.&lt;/li&gt;
&lt;li&gt;The probability of an event in the interval ${t, t+\Delta t}$ is independent of previous events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The probability mass function (PMF) of the Poisson distribution is defined as:
$$
P(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
where $\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.&lt;/p&gt;
&lt;p&gt;For the Poisson distribution, the expected value (mean) is $\mu = \lambda$, and the variance is $\sigma^2 = \lambda$.&lt;/p&gt;
&lt;h4 id=&#34;poisson-noise-shot-noise&#34;&gt;Poisson Noise (Shot Noise)&lt;/h4&gt;
&lt;p&gt;Since $\sigma^2 = \lambda$, the standard deviation (Poisson noise) is $\sigma = \sqrt{\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\sigma = \sqrt{N}$.&lt;/p&gt;
&lt;p&gt;Due to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows:
$$
SNR = \frac{\mu}{\sigma} = \frac{\mu}{\sqrt{\mu}} = \sqrt{\mu}
$$
This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bayes&#39; theorem</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</guid>
      <description>&lt;p&gt;Bayes Theorem is a fundamental principle in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It is particularly powerful in the context of predictive modeling and decision-making processes.&lt;/p&gt;
&lt;h3 id=&#34;mathematical-formulation&#34;&gt;Mathematical Formulation&lt;/h3&gt;
&lt;p&gt;If events $A$ and $B$ are independent, then the probability of $A$ given $B$ is simply the probability of $A$:
$$
P(A|B) = P(A)
$$
However, when events $A$ and $B$ are not independent, the relationship changes as follows:
$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{and} \quad P(B|A) = \frac{P(A \cap B)}{P(A)}
$$
From these relationships, we derive Bayes&amp;rsquo; Theorem:
$$
P(B|A) = \frac{P(B) P(A|B)}{P(A)}
$$
Bayes&amp;rsquo; Theorem can be interpreted in terms of updating beliefs:
$$
\text{Posterior} = \frac{\text{Prior} \times \text{Likelihood}}{\text{Evidence}}
$$&lt;/p&gt;
&lt;h3 id=&#34;components-of-bayes-theorem&#34;&gt;Components of Bayes&amp;rsquo; Theorem&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Likelihood ( $ P(A|B) $ ):&lt;/strong&gt;
The likelihood is a function that measures the plausibility of a model parameter value given specific observed data. In many applications, especially in statistical modeling, the likelihood is assumed to follow a normal distribution:
$$
L(\theta; x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}}
$$
where $\theta$ represents the parameter of interest, $x$ represents the data, and $\sigma^2$ is the variance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior ( $ P(B) $ ):&lt;/strong&gt;
The prior represents the initial belief about the distribution of the parameter before considering the current data. Priors can be subjective or based on previous studies:
$$
P(\theta)
$$
It can be uniform (representing no initial preference) or follow a specific distribution that reflects prior knowledge about the parameter, e.g., a Gaussian prior, gamma and beta distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evidence or Normalizing Constant ( $ P(A) $ ):&lt;/strong&gt;
Often considered as a normalizing factor, the evidence ensures that the posterior probabilities sum to one. It is calculated as:
$$
P(A) = \int P(A|B) P(B) dB
$$
This factor is essential for the probabilistic model to be valid but is usually more relevant in analytical calculations than in practical applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Posterior ( $ P(B|A) $ ):&lt;/strong&gt;
The posterior probability reflects the updated belief about the parameter after considering the new evidence. It combines the prior and the likelihood given new data:
$$
P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta) d\theta}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;practical-applications&#34;&gt;Practical Applications&lt;/h2&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem is used extensively in various fields including exoplanet study, machine learning, medical testing, and any scenario requiring iterative updating of beliefs upon new evidence. Understanding how to apply the theorem allows for more informed decision-making processes and predictions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;relationship-between-likelihood-and-chi-squared-statistic&#34;&gt;Relationship Between Likelihood and Chi-Squared Statistic&lt;/h2&gt;
&lt;p&gt;The likelihood function in statistical models often measures how well a set of parameters fits the data. When the data and the model predictions vary according to a normal distribution, the likelihood function can be directly linked to the chi-squared statistic.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-statistic&#34;&gt;Chi-Squared Statistic:&lt;/h3&gt;
&lt;p&gt;The chi-squared statistic is a measure of how expectations compare to actual observed data. In the context of likelihood calculations, the chi-squared statistic quantifies the discrepancy between observed data and the data predicted by the model, under the assumption that the discrepancies are normally distributed.&lt;/p&gt;
&lt;h3 id=&#34;calculation&#34;&gt;Calculation:&lt;/h3&gt;
&lt;p&gt;To calculate the chi-squared statistic as a measure of likelihood, you can use the following formula:
$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{\sigma_i^2}
$$
where $O_i$ are the observed values, $E_i$ are the expected values predicted by the model, and $\sigma_i^2$ is the variance associated with each observation.&lt;/p&gt;
&lt;h3 id=&#34;using-chi-squared-to-calculate-likelihood&#34;&gt;Using Chi-Squared to Calculate Likelihood:&lt;/h3&gt;
&lt;p&gt;The likelihood of observing the data given the model can be expressed as:
$$
L = e^{-\chi^2/2}
$$
This formulation arises from the exponential component of the PDF of the normal distribution, which is what a likelihood function usually take, thus, a easier way to remember how to calculate the likelihood.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distribution</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/2-1-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/2-1-distribution/</guid>
      <description>&lt;h2 id=&#34;empirical-distribution-function-edf&#34;&gt;Empirical Distribution Function (EDF)&lt;/h2&gt;
&lt;p&gt;The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as:
$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n I[x_i \leq x]
$$
where $I[\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\frac{1}{n}$. An example plot of an EDF is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/edf.png&#34; alt=&#34;edf&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kolmogorov-smirnov-test-ks-test&#34;&gt;Kolmogorov-Smirnov Test (KS Test)&lt;/h2&gt;
&lt;p&gt;The Kolmogorov-Smirnov (KS) Test measures the maximum distance ($D$) between two distribution functions. This can be used to compare an empirical distribution with a theoretical model (one-sample test) or two empirical distributions (two-sample test). If the distributions are identical, $D$ equals zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
D_n = \max_x |F_n(x) - S_n(x)|
$$
A table for the critical values of $D_n$ can be found &lt;a href=&#34;https://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If $D_n$ exceeds the critical value, we can &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that there is no significant difference between the two distributions.&lt;/p&gt;
&lt;h2 id=&#34;cramér-von-mises-statistic-cvm&#34;&gt;Cramér-von Mises Statistic (CvM)&lt;/h2&gt;
&lt;p&gt;The Cramér-von Mises statistic is used to quantify the goodness of fit of an empirical distribution to a theoretical model. It is particularly useful as it considers the squared differences over all points, providing a more sensitive measure to differences &lt;strong&gt;in the tails&lt;/strong&gt; of the distributions.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
C_n = n \int_{-\infty}^\infty [F_n(x) - S(x)]^2 dS(x)
$$ --&gt;
&lt;p&gt;This statistic assesses the integrated squared distance between the empirical distribution function $F_n(x)$ and the theoretical distribution $S(x)$, weighted by the number of observations $n$.&lt;/p&gt;
&lt;h2 id=&#34;anderson-darling-statistic-ad&#34;&gt;Anderson-Darling Statistic (AD)&lt;/h2&gt;
&lt;p&gt;The Anderson-Darling statistic is a modification of the Cramér-von Mises statistic that gives more weight to the tails of the distribution. It is particularly effective in identifying departures from a theoretical distribution in the tails.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
A^2 = n \int_{-\infty}^\infty \frac{[F_n(x) - S(x)]^2}{S(x)(1 - S(x))} \, dx
$$ --&gt;
&lt;p&gt;This weighted approach makes the AD statistic more sensitive to discrepancies in the distribution&amp;rsquo;s tails than the CvM statistic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Both CvM and AD tests are powerful tools for statistical hypothesis testing, especially in scenarios where understanding the tail behavior of distributions is crucial.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;wilcoxon-rank-sum-test&#34;&gt;Wilcoxon Rank Sum Test&lt;/h2&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test, also known as the Mann-Whitney Rank Sum test, is designed to assess &lt;em&gt;whether two independent samples come from the same distribution&lt;/em&gt;. Here’s how the test statistic is calculated:&lt;/p&gt;
&lt;!-- It is especially useful when the data does not meet the assumptions necessary for the t-test, primarily concerning normality --&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Combine and Rank the Data:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Combine all observations from both samples into a single dataset.&lt;/li&gt;
&lt;li&gt;Rank all observations from the smallest to largest. Ties are given a rank equal to the average of the ranks they would have otherwise occupied.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate the Rank Sums:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Calculate the sum of the ranks for observations from each sample separately. Let $T_1$ be the sum of ranks for the first sample, and $T_2$ for the second sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute the Test Statistic:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The test statistic $U$ is calculated using:
$$
U = T_1 - \frac{n_1(n_1+1)}{2}
$$
where $n_1$ is the number of observations in the first sample. $U$ can also be computed for the second sample, and the smaller of the two $U$ values is often used as the test statistic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine the Significance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The significance of the observed $T_1$ value is determined by comparing it to values in a reference distribution, which approximates a normal distribution under the null hypothesis when the sample sizes are sufficiently large. The mean and standard deviation of $T_1$ are used to compute a z-score:
$$
|z| = \left| \frac{T_1 - \text{mean}(T_1)}{\text{std dev}(T_1)} \right|
$$
where $\text{mean}(T_1) = \frac{n_1(n_1+n_2+1)}{2}$, and $\text{Var}(T_1) = \frac{n_1 n_2(n_1+n_2+1)}{2}$.&lt;/li&gt;
&lt;li&gt;The p-value is then calculated from the normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ### Interpretation

- If the p-value is less than the chosen significance level (commonly 0.05), then there is sufficient evidence to reject the null hypothesis, suggesting that there is a statistically significant difference in the distributions of the two groups.

- If the p-value is greater, then we do not reject the null hypothesis, suggesting that any observed differences could reasonably occur by random chance under the assumption of identical distributions. --&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test does not require the data to follow a specific distribution, making it a robust and widely applicable non-parametric method for comparing two samples.&lt;/p&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;p&gt;The Kruskal-Wallis (KW) test is used to determine if there are statistically significant differences between the distributions of two or more groups of an independent variable. It generalizes the Wilcoxon Rank Sum Test to more than two groups. The null hypothesis assumes that all groups come from identical distributions. The test statistic follows a chi-squared ($\chi^2$) distribution with $k-1$ degrees of freedom, where $k$ is the number of groups.&lt;/p&gt;
&lt;h2 id=&#34;comparison-of-statistical-distribution-tests&#34;&gt;Comparison of Statistical Distribution Tests&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Key Usage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirement&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Distribution Assumption&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;KS Test&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing a sample with a reference distribution&lt;/td&gt;
&lt;td&gt;One or two samples, continuous or ordinal&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in location, scale, and shape&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CvM Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit to a theoretical distribution&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Integrates squared differences; sensitive across entire distribution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AD Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit with emphasis on tail differences&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Increased weight to tails; highly sensitive to tail discrepancies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Wilcoxon Rank Sum&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing two independent samples&lt;/td&gt;
&lt;td&gt;Two independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in medians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kruskal-Wallis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing more than two independent samples&lt;/td&gt;
&lt;td&gt;Two or more independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Generalization of Wilcoxon, sensitive to differences across multiple samples&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/2-2-correlation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/2-2-correlation/</guid>
      <description>&lt;p&gt;Nonparametric correlation tests are essential tools for assessing the strength and direction of a relationship between two datasets, especially when the underlying distributions are unknown or non-normal. These tests are robust alternatives to the Pearson correlation coefficient, suitable for various types of relationships. The null hypothesis for these tests is that there is no correlation between the datasets.&lt;/p&gt;
&lt;h3 id=&#34;pearson-correlation-coefficient&#34;&gt;*Pearson Correlation Coefficient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson correlation test is a &lt;strong&gt;parametric test&lt;/strong&gt;, but I put it here for completeness and for comparison.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Pearson correlation coefficient measures the linear relationship between two continuous variables. It is a parametric test and assumes that the data is normally distributed. The coefficient varies between -1 and +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.&lt;/p&gt;
&lt;h3 id=&#34;spearmans-rank-correlation-coefficient-rho&#34;&gt;Spearman&amp;rsquo;s Rank Correlation Coefficient ($\rho$)&lt;/h3&gt;
&lt;p&gt;Spearman&amp;rsquo;s correlation assesses how well the relationship between two variables can be described using a monotonic function. It does not require the data to be normally distributed, as it uses the rank of the data rather than the actual values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\rho = 1 - \frac{6 \sum d_i^2 + T}{n(n^2 - 1)}
$$
where $d_i$ is the difference between the ranks of corresponding variables and $n$ is the number of observations. For handling ties, the correction term $T$ is applied:
$$
T = \sum t_x \left[ \frac{x^3 - x}{12} \right]
$$
where $t_x$ is the number of ties involving $x$ elements. To further estimate the $p$-value, the $z$ score can be calculated by:
$$
z = \sqrt{n-1} \rho
$$
which approximates a normal distribution under the null hypothesis. One can use the $\rho$ statistic and compare to the critical values &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/spearman.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kendalls-tau-tau&#34;&gt;Kendall&amp;rsquo;s Tau ($\tau$)&lt;/h3&gt;
&lt;p&gt;Kendall&amp;rsquo;s Tau measures the ordinal association between two variables by considering the number of concordant and discordant pairs. It is less sensitive to outliers than Pearson and can be more interpretable in terms of the proportion of concordant pairs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\tau = \frac{N_c - N_d}{\sqrt{(N_c + N_d + T)(N_c + N_d + U)}}
$$
where $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $T$ is the number of ties on one variable, and $U$ is the number of ties on the other variable.&lt;/p&gt;
&lt;p&gt;The critical values for $\tau$ can be find &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/kendall.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;comparison-of-correlation-tests&#34;&gt;Comparison of Correlation Tests&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Best Use Case&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirements&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Pearson&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Linear relationships&lt;/td&gt;
&lt;td&gt;Highly sensitive to linear trends&lt;/td&gt;
&lt;td&gt;Normal distribution, continuous data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Spearman&amp;rsquo;s rho&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Monotonic relationships, not necessarily linear&lt;/td&gt;
&lt;td&gt;Sensitive to monotonic trends, not affected by outliers&lt;/td&gt;
&lt;td&gt;Ordinal data or non-normal distributions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kendall&amp;rsquo;s tau&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;General increasing or decreasing trends, less intensive computation&lt;/td&gt;
&lt;td&gt;Less sensitive to errors in data, good for small samples or data with many ties&lt;/td&gt;
&lt;td&gt;Ordinal data, robust against outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Data Smoothing-Density Estimation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/3-1-data-smoothing/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/3-1-data-smoothing/</guid>
      <description>&lt;p&gt;Proper smoothing of data is crucial in various applications, especially for interpolating and visualizing results. A classic example involves choosing the right bin size for histograms. Incorrect bin sizes can either obscure significant data features by being too large or create misleading features if they are too small.&lt;/p&gt;
&lt;h3 id=&#34;scotts-rule-for-bin-width&#34;&gt;Scott&amp;rsquo;s Rule for Bin Width&lt;/h3&gt;
&lt;p&gt;Scott&amp;rsquo;s rule provides a method to determine the optimal bin width for a histogram, balancing detail and smoothness. The formula for Scott&amp;rsquo;s bin width is:
$$
h_{\text{Scott}} = \frac{3.5 \cdot \text{std}}{n^{1/3}} \quad \text{or} \quad \frac{2 \cdot \text{IQR}}{n^{1/3}}
$$
where $\text{std}$ is the standard deviation, $\text{IQR}$ is the interquartile range, and $n$ is the number of data points. This method aims to minimize potential distortion in the histogram by accounting for the variability and size of the data set.&lt;/p&gt;
&lt;h3 id=&#34;average-smoothing-histogram&#34;&gt;Average Smoothing Histogram&lt;/h3&gt;
&lt;p&gt;Average smoothing, applied to histograms, involves averaging adjacent bins to reduce variance within the bins. This technique smooths out fluctuations that might be random and highlights broader trends in the data distribution.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density-estimation-kde&#34;&gt;Kernel Density Estimation (KDE)&lt;/h3&gt;
&lt;p&gt;Kernel Density Estimation is a non-parametric way to estimate the probability density function of a random variable. KDE smooths the data by convolving it with a kernel, which is a predefined function, typically Gaussian. The formula for KDE is:
$$
\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
$$
where $K$ is the kernel function, $x_i$ are the data points, $h$ is the bandwidth, and $n$ is the number of data points. The choice of bandwidth $h$ crucially affects the estimator&amp;rsquo;s bias and variance.&lt;/p&gt;
&lt;h3 id=&#34;adaptive-smoothing&#34;&gt;Adaptive Smoothing&lt;/h3&gt;
&lt;p&gt;Adaptive smoothing techniques adjust the smoothing parameters locally, depending on the density or other characteristics of the data. These methods aim to achieve &lt;em&gt;better smoothing in areas with higher variability or lower density,&lt;/em&gt; allowing more detailed features to emerge in dense regions while smoothing out noise in sparse regions.&lt;/p&gt;
&lt;h3 id=&#34;nadaraya-watson-estimator&#34;&gt;Nadaraya-Watson Estimator&lt;/h3&gt;
&lt;p&gt;The Nadaraya-Watson estimator is a type of kernel regression that uses locally weighted averages to estimate conditional expectations. It is particularly useful in regression analysis to model the relationship between variables.&lt;/p&gt;
&lt;!-- The estimator is defined as:
$$
  \hat{m}(x) = \frac{\sum_{i=1}^n K_h(x - x_i) y_i}{\sum_{i=1}^n K_h(x - x_i)}
$$
where $y_i$ are the response variables, and $K_h$ is a kernel weighted by a bandwidth $h$. This method is effective in capturing the local variability of the data without assuming a specific parametric form for the relationship.

These techniques in data smoothing and density estimation are invaluable tools in data analysis, offering different approaches to uncovering and representing the underlying patterns in the data. Each method has its strengths and is suited to different types of data challenges. --&gt;
</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/4-1-regression/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/4-1-regression/</guid>
      <description>&lt;p&gt;Regression analysis is fundamental in statistical modeling, used for predicting and forecasting, and understanding relationships between variables. Below are various regression methods:&lt;/p&gt;
&lt;h3 id=&#34;least-square-linear-regression-without-uncertainty&#34;&gt;Least-Square Linear Regression (without Uncertainty)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ordinary Least Squares (OLS)&lt;/strong&gt;: This method minimizes the residual sum of squares (RSS) between the observed values in the dataset and the values predicted by the linear model.
$$
\text{min RSS} = \text{min} \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
$$
where $X_i$ and $Y_i$ are the observed values, and $\beta_0$ and $\beta_1$ are the intercept and slope of the linear model, respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Symmetric Least Squares-Orthogonal Regression&lt;/strong&gt;: This method minimizes the summed squares of the residuals orthogonal to the regression line, providing a more general approach than OLS when errors in both variables need to be considered.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Regression - Thiel-Sen Estimator&lt;/strong&gt;: This technique uses the median of the slopes between pairs of points as the estimator, providing robustness against outliers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantile Regression&lt;/strong&gt;: Focuses on estimating either the conditional median or other quantiles of the response variable, providing a more comprehensive view of the possible outcome distribution than mean regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maximum Likelihood Estimation&lt;/strong&gt;: Assumes a probability distribution model for the data, often a normal distribution, and finds the parameter values that maximize the likelihood of observing the data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weighted-least-squares-with-uncertainty&#34;&gt;Weighted Least Squares (with Uncertainty)&lt;/h3&gt;
&lt;p&gt;In scenarios with heteroscedastic errors (errors that varies), weighted least squares (WLS) is more appropriate:&lt;/p&gt;
&lt;p&gt;$$
S_{r,wt} = \sum_{i=1}^n \frac{(Y_i - \beta_0 - \beta_1 X_i)^2}{\sigma_{Y,i}^2}
$$
where $\sigma_{Y,i}^2$ is the variance associated with each observation, weighting the residuals accordingly. This is related to the $\chi^2$ statistic used in minimum $\chi^2$ regression:&lt;/p&gt;
&lt;p&gt;$$
\chi^2_{me} = \sum_{i=1}^n (O_i - M_i)^2 / \sigma_{i,me}^2
$$&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;Logistic regression is used for modeling binary outcome variables by using the logistic function to estimate probabilities that can be transformed into binary values.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model-validation-and-selection&#34;&gt;Model Validation and Selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Coefficient of Determination (R²)&lt;/strong&gt;: Measures the proportion of variability in a dataset that is explained by the regression model.
$$
R^2 = 1 - \frac{ \sum_{i=1}^n (Y_i - \hat{Y_i})^2 }{ \sum_{i=1}^n (Y_i - \bar{Y})^2 }
$$
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{Y_i}$ is the predicted value of the dependent variable for the $i$-th observation from the model&lt;/li&gt;
&lt;li&gt;$\bar{Y}$ is the mean of the observed data $Y_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An $R^2$ value of 1 implies a perfect fit, meaning that the model explains all the variability of the response data around its mean. Conversely, an $R^2$ value of 0 indicates at bad fit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adjusted $R^2$&lt;/strong&gt;: Adjusts $R²$ for the number of predictors in the model, preventing overfitting by penalizing excessive use of parameters.
$$
R_a^2 = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$
where $p$ is the number of parameters, and $n$ is the number of data points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normal quantile-quantile plot (Q-Q Plot)&lt;/strong&gt;: Used to check the normality of residuals. If the points lie along a straight line, the residuals are normally distributed, an assumption in many regression models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Akaike Information Criterion (AIC)&lt;/strong&gt;: The AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.
$$
\text{AIC} = 2k - 2\ln(\hat{L})
$$
where $k$ is the number of parameters in the model, and $\hat{L}$ is the maximum likelihood value.&lt;/p&gt;
&lt;p&gt;The model with the lowest AIC among a set of models is typically chosen. The AIC is particularly useful when a model is being fit to data: minimizing the AIC maximizes the likelihood function given the data while penalizing for increasing numbers of parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Information Criterion (BIC)&lt;/strong&gt;: The BIC is similar to the AIC but introduces &lt;em&gt;a stronger penalty for including additional variables to the model&lt;/em&gt;. It is derived from Bayesian probability.
$$
\text{BIC} = \ln(n)k - 2\ln(\hat{L})
$$
where $n$ is the number of observations, $k$ is the number of parameters in the model, and $\hat{L}$ is the maximum likelihood of the model.&lt;/p&gt;
&lt;p&gt;The BIC is generally stricter than the AIC and can be preferable for models with large $n$, penalizing free parameters more heavily. Like the AIC, &lt;strong&gt;the model with the lowest BIC is generally preferred&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ## When to Use AIC vs. BIC

| **Criterion** | **When to Use**                                                                                               |
|---------------|---------------------------------------------------------------------------------------------------------------|
| **AIC**       | More appropriate when the focus is on goodness of fit. Suitable for models where the sample size \( n \) is much larger than the number of parameters \( k \). Less strict about adding parameters. |
| **BIC**       | Used when model parsimony is important. Especially useful in models with a large number of observations \( n \) and relatively less concern about capturing every parameter influence. More stringent in penalizing free parameters. | --&gt;
&lt;!-- ## Summary Table for Regression Methods

| **Method**                      | **Best Use**                                           | **Assumptions/Features**                                   |
|---------------------------------|--------------------------------------------------------|------------------------------------------------------------|
| **OLS Regression**              | Linear relationships with constant variance            | Assumes normality and linearity                            |
| **Orthogonal Regression**       | Errors in both variables                               | Minimizes perpendicular distances                          |
| **Robust Regression**           | Outlier-heavy data                                     | Resistant to outliers in data                              |
| **Quantile Regression**         | Non-normal data; interested in other quantiles         | Does not assume a specific distribution for residuals      |
| **Weighted Least Squares**      | Varied error size across data range                    | Weights observations by the inverse of their variance      |
| **Logistic Regression**         | Binary outcome data                                    | Estimates probability of occurrence                        |
 --&gt;
</description>
    </item>
    
    <item>
      <title>Multivariate Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustering and Classification</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/6-clustering-and-classification/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/6-clustering-and-classification/</guid>
      <description>&lt;p&gt;Clustering and classification are fundamental techniques in data analysis and machine learning, used to group data points based on similarities and to categorize them into distinct classes.&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Hierarchical clustering builds nested clusters by progressively merging or splitting them based on the distance between data points or groups. The method relies heavily on the choice of distance calculation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single Linkage (Nearest Neighbor)&lt;/strong&gt;: This method, also known as the friend-of-a-friend technique, considers the shortest distance between points in two clusters (see the first plot below). It can result in elongated, &amp;ldquo;chain-like&amp;rdquo; clusters that capture local structure but might miss broader data groupings (see the second plot below).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete Linkage&lt;/strong&gt;: Use the maximum distance between points in two clusters. This method tends to produce more compact and well-separated clusters, reducing the chain effect seen in single linkage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average Linkage&lt;/strong&gt;: Calculates the average distance between all pairs of points in two clusters. This method provides a balance between the sensitivity of single linkage and the strictness of complete linkage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ward&amp;rsquo;s Minimum Variance&lt;/strong&gt;: Minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum increase in total variance are merged. This method tends to create more regular-sized clusters (e.g., spherical or ellipsoidal), which can be advantageous for certain datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Pamela-Guevara/publication/281014334/figure/fig57/AS:418517879934980@1476793847581/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and.png&#34; alt=&#34;&#34;&gt;
(Figure credit: &lt;a href=&#34;https://www.researchgate.net/figure/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and_fig57_281014334&#34;&gt;https://www.researchgate.net/figure/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and_fig57_281014334&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Once distances are calculated, the hierarchical clustering algorithm uses these distances to merge or split clusters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: Start by assigning each data point to its own cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Merge Step&lt;/strong&gt;: At each step, merge the two clusters that are closest together, based on the distance calculation method chosen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update Distances&lt;/strong&gt;: Recalculate the distances between the new cluster and each of the old clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt;: Continue merging clusters until all data points are merged into a single cluster or until a desired number of clusters is reached.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These different linkage criteria can significantly impact the shapes and sizes of the clusters formed. A nice demonstration in various clustering scenarios can be found on the scikit-learn page, also shown below:
&lt;img src=&#34;https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png&#34; alt=&#34;&#34;&gt;
(Figure credit: &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;k-means-clustering&#34;&gt;k-Means Clustering&lt;/h3&gt;
&lt;p&gt;k-Means clustering partitions the data into $k$ mutually exclusive clusters, and returns the index of the cluster each point belongs to. This method aims to minimize the within-cluster sum of squares.&lt;/p&gt;
&lt;h3 id=&#34;density-based-spatial-clustering-of-applications-with-noise-dbscan&#34;&gt;Density-Based Spatial Clustering of Applications with Noise (DBSCAN)&lt;/h3&gt;
&lt;p&gt;DBSCAN groups together closely packed points and marks points in low-density regions as outliers. This method does not require specifying the number of clusters a priori, making it suitable for data with irregular or complex structures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;$\mu$: Minimum number of points required to form a dense region.&lt;/li&gt;
&lt;li&gt;$\varepsilon$: Specifies the &amp;ldquo;reach&amp;rdquo;, that is, the distance threshold within which points are considered neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;k-nearest-neighbor-k-nn&#34;&gt;k-Nearest Neighbor (k-NN)&lt;/h3&gt;
&lt;p&gt;k-Nearest Neighbors (k-NN) is primarily a classification technique renowned for its simplicity and effectiveness, especially suited for datasets where the decision boundaries between classes are not linear. The k-NN algorithm classifies new data points based on the majority vote of their k nearest neighbors in the feature space.&lt;/p&gt;
&lt;p&gt;To apply k-NN effectively, data is typically split into two sets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training Set&lt;/strong&gt;: This dataset is used to &amp;rsquo;train&amp;rsquo; or &amp;lsquo;fit&amp;rsquo; the model. It includes both the input features and the corresponding classification labels which are known.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test Set&lt;/strong&gt;: This dataset is used solely for testing the performance of the trained model. It helps to evaluate how well the k-NN model generalizes to new, previously unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the model has been trained on the training set, it can be used to predict the class labels of new data in the test set, providing a measure of its classification accuracy.&lt;/p&gt;
&lt;!-- ## Summary Table for Clustering and Classification Methods

| **Method**           | **Characteristics**                                       | **Best Used For**                                       |
|----------------------|-----------------------------------------------------------|---------------------------------------------------------|
| **Hierarchical Clustering** | Builds clusters based on various distance calculations between points. | Data with inherent hierarchical structure and when a visual representation of cluster formation is beneficial. |
| **k-Means Clustering**      | Partitions data into k predefined clusters by minimizing within-cluster variances. | Large datasets with well-separated clusters, where the number of clusters is known a priori. |
| **DBSCAN**                  | Groups densely packed points and identifies points in low-density areas as outliers. Does not require predefined cluster number. | Complex datasets with noise and irregular cluster shapes, not well-suited to global clustering criteria. |
| **k-Nearest Neighbors (k-NN)** | Classifies data based on the majority label among the nearest k neighbors. Utilizes training and test datasets to ensure model accuracy and generalization. | Classification tasks, especially in cases with non-linear decision boundaries and when model simplicity and interpretability are important. | --&gt;
</description>
    </item>
    
    <item>
      <title>Nondetections-Censored and Truncated Data</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/7-censored-and-truncated-data/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/7-censored-and-truncated-data/</guid>
      <description>&lt;p&gt;In the real world, data collection is often incomplete or constrained by various factors. To fully utilize available data, it is sometimes necessary to handle &amp;lsquo;censored&amp;rsquo; and &amp;rsquo;truncated&amp;rsquo; data, particularly in fields such as medical studies, reliability engineering, and astronomy.&lt;/p&gt;
&lt;h3 id=&#34;censored-data&#34;&gt;Censored Data&lt;/h3&gt;
&lt;p&gt;Censored data occurs when the value of a measurement exists, but we only know that it falls above or below certain limits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Left-Censored Data (Upper Limit)&lt;/strong&gt;: The actual data point is less than a certain value, but the exact value is unknown. This is common in astronomy; for example, a star&amp;rsquo;s luminosity might be below the detection limit of a telescope. We know only the upper limit of the star&amp;rsquo;s luminosity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right-Censored Data (Lower Limit)&lt;/strong&gt;: The actual data point is greater than a certain value, but the exact value is unknown.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;truncated-data&#34;&gt;Truncated Data&lt;/h3&gt;
&lt;p&gt;Truncated data occurs when data points below or above a certain threshold are not just unknown but completely absent from the dataset. Unlike censoring, with truncation, we do not have any information that data points exist outside of the observed range.&lt;/p&gt;
&lt;p&gt;These characteristics have implications for statistical analysis, including distribution function estimation, correlation analysis, regression modeling, and hypothesis testing.&lt;/p&gt;
&lt;h3 id=&#34;survival-and-hazard-functions-in-censoring&#34;&gt;Survival and Hazard Functions in Censoring&lt;/h3&gt;
&lt;p&gt;Censoring techniques often use concepts from survival analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Survival Function ($S(x)$)&lt;/strong&gt;: Represents the probability that a variable $X$ exceeds a certain value $x$.
$$
S(x) = P(X &amp;gt; x) = 1 - F(x)
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hazard Rate ($h(x)$)&lt;/strong&gt;: Represents the conditional failure rate at a certain value.
$$
h(x) = \frac{f(x)}{S(x)}
$$
Here, $f(x)$ is the PDF at $x$, and the hazard rate can be interpreted as the likelihood of an event occurring at $x$ given that it has not occurred before $x$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clarifying-the-example-on-hazard-rate&#34;&gt;Clarifying the Example on Hazard Rate&lt;/h3&gt;
&lt;p&gt;Consider an example where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f(95) = 1\%$: The probability of dying at the age of 95 is 1%.&lt;/li&gt;
&lt;li&gt;$S(95) = 2\%$: The probability of surviving past the age of 95 is 2%.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To find the hazard rate $h(x)$ at age 95:
$$
h(95) = \frac{0.01}{0.02} = 0.5 \text{ or } 50\%
$$
This indicates that, given reaching age 95, there is a 50% chance of dying at that age.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;advanced-estimators-for-censored-and-truncated-data&#34;&gt;Advanced Estimators for Censored and Truncated Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kaplan-Meier Nonparametric Estimator (Censored)&lt;/strong&gt;: The Kaplan-Meier estimator is crucial for analyzing survival data, particularly in medical research. It measures the fraction of subjects living for a certain amount of time after treatment. This estimator is particularly effective in handling right-censored data, where the survival time is only known to exceed a certain duration but the exact time of event (e.g., death, failure) is unknown. The Kaplan-Meier estimator uses the available data to estimate the survival function, $S(t)$, which provides insights into the likelihood of survival beyond observed time points.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below is an example plot of the KM estimated survival curve of synthetic data on stellar luminosity data.
&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/KM.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Sanya Arora&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lynden-Bell–Woodroofe Estimator (Truncated)&lt;/strong&gt;: Commonly utilized in astronomical studies, the Lynden-Bell–Woodroofe estimator addresses issues with truncated samples, where observations below or above certain thresholds are missing entirely from the dataset. This estimator operates under the assumption that all observations derive from the same underlying distribution. It uses the observed distribution of the available data to estimate the distribution functions of the truncated segments, facilitating a more comprehensive understanding of the overall data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both estimators are tailored to specific types of incomplete data: Kaplan-Meier for censored data, where some information about the survival time is available, and Lynden-Bell–Woodroofe for truncated data, where parts of the data are completely missing. Understanding their applications and differences is essential for accurately analyzing datasets characterized by incomplete observations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timeseries Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/8-timeseries-analysis/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/8-timeseries-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variogram</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-1-variogram/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-1-variogram/</guid>
      <description>&lt;h2 id=&#34;introduction-to-variograms&#34;&gt;Introduction to Variograms&lt;/h2&gt;
&lt;p&gt;A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: &amp;ldquo;nugget,&amp;rdquo; &amp;ldquo;sill,&amp;rdquo; and &amp;ldquo;range.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nugget:&lt;/strong&gt; Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sill:&lt;/strong&gt; The plateau reached by the variogram, beyond which the increments in distance do not significantly increase the variance. It represents the level of total variance within the dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; The distance at which the variogram reaches the sill, beyond which locations are no longer correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-variogram&#34;&gt;Example of Spatial Data and Variogram&lt;/h2&gt;
&lt;p&gt;Plots below show an example of spatial data (left) and its associated variogram (right). The plot on the left shows synthetic data in spatial X-Y coordinates color-coded by the level of toxicity measured at that location. The size of the circle is associated with the measurement error, which is not used here. The variogram on the right shows how the semi-variance between points increases with distance. It features a &amp;ldquo;nugget&amp;rdquo; effect at the origin, indicating measurement noise or microscale variability. The curve approaches a &amp;ldquo;sill,&amp;rdquo; beyond which the variance stabilizes, suggesting that points beyond this &amp;ldquo;range&amp;rdquo; do not influence each other. This range is critical for understanding the spatial continuity and predicting values at unsampled locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/variogram.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-variogram&#34;&gt;Example R Code for the Variogram&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(gstat)
library(sp)

# Defining spatial coordinates
# df_2 is a dataframe with colums x and y 
coordinates(df_2) &amp;lt;- ~x+y  

# Creating Variograms
variogram_tox &amp;lt;- variogram(toxicity ~ 1, df_2)

# Fit the variogram and plot it out.
# gamma: the semi-variance
# vgm: &amp;quot;variogram model,&amp;quot; 
model_tox &amp;lt;- fit.variogram(
  variogram_tox, model = vgm(psill = max(variogram_tox$gamma), 
  model = &amp;quot;Sph&amp;quot;, 
  range = 30))

# Plot the empirical variogram and the fitted model
plot(variogram_tox, model = model_tox, 
  main = &amp;quot;Toxicity Variogram with Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kriging</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-2-kriging/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-2-kriging/</guid>
      <description>&lt;h2 id=&#34;introduction-to-kriging&#34;&gt;Introduction to Kriging&lt;/h2&gt;
&lt;p&gt;Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Kriging:&lt;/strong&gt; Assumes the mean of the random field is known and constant throughout the region of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ordinary Kriging:&lt;/strong&gt; Assumes the &lt;em&gt;mean is unknown&lt;/em&gt; but constant within the region of interest and is the most commonly used form as one do not know the mean in real world.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-on-variogram&#34;&gt;More on Variogram&lt;/h2&gt;
&lt;p&gt;The semivariance $\gamma(x_1, x_2)$ between two points can be expressed as:
$$
\gamma(d) = \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2
$$
where $n$ is the number of pairs, $d$ is the distance between two points, and $z$ represents the values at the locations. The calculation of $\gamma(d)$ involves several key assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stationary (homogeneous):&lt;/strong&gt; Assumes that the statistical properties (mean, variance) of the process do not change over space. This implies that the mean and variance are constant throughout the region of interest, and the covariance between any two points depends only on the distance and direction between them, not on their absolute locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isotropy:&lt;/strong&gt; Assumes that the statistical properties are the same in all directions. This means that the variogram is a function only of the distance between sample points, not of the direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Kriging, we use $\gamma$ to weight the data for interpolation:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\gamma(d) &amp;amp;= \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 \\
&amp;amp;= \frac{1}{2} E\left(\left[z(x+d) - z(x)\right]^2\right)
\text{(homogeneous assumption)} \\
&amp;amp;= \frac{1}{2} \left\{ E\left[ z^2(x+d) \right] + E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\} \\
&amp;amp;= \frac{1}{2} \left\{ 2E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\}
\text{(homogeneous assumption)} \\
&amp;amp;= \sigma_x^2 + \mu_x^2 - \text{cov}[z(x), z(x+d)] - \mu_x^2
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the relationships: $E[x^2] = \sigma_x^2 + [E(x)]^2$ and $\text{cov}[X,Y] = E(XY) - E(X)E(Y)$ were used to get the second to last equation. We have:
$$\begin{align}
\gamma(d) &amp;amp;= \sigma^2 - \text{cov}[z(x), z(x+d)] \\
&amp;amp;= \sigma^2 - \text{c}(d)
\end{align}$$&lt;/p&gt;
&lt;p&gt;Therefore, we have the semivariance at distance $d$ is the variance minus the covariance between points at this distance.&lt;/p&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-the-kriging-result&#34;&gt;Example of Spatial Data and the Kriging Result&lt;/h2&gt;
&lt;p&gt;The plots below show an example of spatial data (left) and its associated Kriging map (right). The data used here is the same as in the Variogram page, and the Kriging map uses the variogram model shown on the Variogram page to predict values at unsampled locations. This example demonstrates how Kriging utilizes the spatial structure of the data, as defined by the variogram, to provide a statistically optimal interpolation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/kriging.png&#34; alt=&#34;kriging&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-kriging&#34;&gt;Example R Code for the Kriging&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Fit a variogram model
vgm_model &amp;lt;- vgm(psill = max(variogram_tox$gamma), 
       model = &amp;quot;Sph&amp;quot;, range = 30)
       
x.range &amp;lt;- range(df_2$x)
y.range &amp;lt;- range(df_2$y)

grid.points &amp;lt;- expand.grid(
       x = seq(from = x.range[1], to = x.range[2], by = 1),
       y = seq(from = y.range[1], to = y.range[2], by = 1))

# Convert to SpatialPoints
grid &amp;lt;- SpatialPoints(grid.points)

# Perform ordinary kriging
kriged &amp;lt;- krige(toxicity ~ 1, df_2, model = vgm_model, newdata = grid)

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res=200)
# Create a map with overlay contours
spplot(kriged, &amp;quot;var1.pred&amp;quot;, main = &amp;quot;Kriging Map for Toxicity&amp;quot;, 
       xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;,
       sp.layout = list(&amp;quot;sp.points&amp;quot;, df_2, col = &amp;quot;green&amp;quot;),
       colorkey = TRUE,
       scales = list(draw = TRUE))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://shihyuntang.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Binary Star Evolution in Different Environments: Filamentary, Fractal, Halo and Tidal-tail Clusters</title>
      <link>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Star-Crossed Lovers DI Tau A and B: Orbit Characterization and Physical Properties Determination</title>
      <link>https://shihyuntang.github.io/publication/202303_ditau_apj/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202303_ditau_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical Origin for the Collinder 132-Gulliver 21 Stream: A Mixture of three Co-Moving Populations with an Age Difference of 250 Myr</title>
      <link>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3 II: Hierarchical Star Formation Revealed by Spatial and Kinematic Substructures</title>
      <link>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Disruption of Hierarchical Clustering in the Vela OB2 Complex and the Cluster Pair Collinder 135 and UBC 7 with Gaia EDR3: Evidence of Supernova Quenching</title>
      <link>https://shihyuntang.github.io/publication/202109_velob_apj-/</link>
      <pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202109_velob_apj-/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impacts of Water Latent Heat on the Thermal Structure of Ultra-Cool Objects: Brown Dwarfs and Free-Floating Planets</title>
      <link>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IGRINS RV: A Python Package for Precision Radial Velocities with Near-Infrared Spectra</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IGRINS RV: A Precision RV Pipeline for IGRINS Using Modified Forward-Modeling in the Near-Infrared</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</link>
      <pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3: its Relation to Cluster Dynamics</title>
      <link>https://shihyuntang.github.io/publication/202102_oc3d_apj/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202102_oc3d_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Different Fates of Young Star Clusters After Gas Expulsion</title>
      <link>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Near-Infrared Radial Velocity pipeline (IGRINS_RV)</title>
      <link>https://shihyuntang.github.io/project/near-infrared-radial-velocity-pipeline-igrins_rv/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/near-infrared-radial-velocity-pipeline-igrins_rv/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Cluster Dynamic Study</title>
      <link>https://shihyuntang.github.io/project/open-cluster-dynamic-study/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/open-cluster-dynamic-study/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Y-dwarf Atmosphere Study</title>
      <link>https://shihyuntang.github.io/project/y-dwarf-atmosphere-study/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project/y-dwarf-atmosphere-study/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diagnosing the Stellar Population and Tidal Structure of the Blanco1 Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/202001_blanco1_apj/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202001_blanco1_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Discovery of Tidal Tails in Disrupting Open Clusters: Coma Berenices and a Neighbor Stellar Group</title>
      <link>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://shihyuntang.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterization of Stellar and Substellar Members in the Coma Berenices Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/201808_comber_apj/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201808_comber_apj/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://shihyuntang.github.io/project_example/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://shihyuntang.github.io/project_example/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
