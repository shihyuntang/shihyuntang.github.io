<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shih-Yun Tang</title>
    <link>https://shihyuntang.github.io/</link>
      <atom:link href="https://shihyuntang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shih-Yun Tang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018-2025 Shih-Yun Tang</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shihyuntang.github.io/img/icon-192.png</url>
      <title>Shih-Yun Tang</title>
      <link>https://shihyuntang.github.io/</link>
    </image>
    
    <item>
      <title>Probability Basic</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/</guid>
      <description>&lt;h2 id=&#34;some-terminology-in-statistics&#34;&gt;Some Terminology in Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;i.i.d. (Independent and Identically Distributed):&lt;/strong&gt;
This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis:&lt;/strong&gt;
In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P-value:&lt;/strong&gt;
The p-value is the probability of observing test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A p-value less than a pre-determined significance level, often 0.05, leads to the rejection of the null hypothesis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confidence Interval (CI):&lt;/strong&gt;
A confidence interval is a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter. The confidence level represents the probability that this interval will capture this parameter in repeated samples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- - **Type I and Type II Errors:**
  - **Type I Error (False Positive):** Occurs when the null hypothesis is incorrectly rejected when it is actually true.
  - **Type II Error (False Negative):** Occurs when the null hypothesis is not rejected when it is actually false. --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;first-and-second-moments&#34;&gt;First and Second Moments&lt;/h2&gt;
&lt;p&gt;In probability and statistics, the concepts of first and second moments are central to understanding the distributions of random variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;First Moment (Mean):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The first moment is the expected value of a random variable $X$, denoted as $E(X)$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt;
$$E(X) = \mu$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Second Moment (Variance):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The second moment about the mean is the variance of the random variable $X$, denoted as $\sigma^2$. It measures the spread of the data points around the mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The variance is calculated using the expectation of the &lt;em&gt;squared deviations from the mean&lt;/em&gt;:
$$
\sigma^2 = E[(X-\mu)^2] = E[X^2] - E(X)^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$$\begin{align}
\sigma^2 &amp;amp;= E[(X-\mu)^2] \\
&amp;amp;= E[X^2 - 2X\mu + \mu^2] \\
&amp;amp;= E[X^2] - 2\mu E(X) + E(\mu^2) \\
&amp;amp;= E[X^2] - 2\mu^2 + \mu^2 \\
&amp;amp;= E[X^2] - \mu^2 \\
&amp;amp;= E[X^2] - E(X)^2
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Third Moment (Skewness):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The third central moment describes the skewness of the distribution of a random variable, indicating the degree of asymmetry around the mean. Skewness can reveal whether the distribution tails off more on one side than the other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The third moment is calculated using the expectation of the cubed deviations from the mean:
$$
E[(X - E(X))^3]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive skewness indicates that the distribution has a long tail to the right (more positive side), while a negative skewness indicates a long tail to the left (more negative side).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Covariance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Covariance measures the joint variability of two random variables, $X$ and $Y$. It assesses the degree to which two variables change together. If the greater values of one variable mainly correspond to the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The covariance between two variables $X$ and $Y$ is given by:
$$
\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive covariance indicates that as $X$ increases, $Y$ tends to increase. A negative covariance suggests that as $X$ increases, $Y$ tends to decrease. Zero covariance indicates that the variables are independent, assuming they are also uncorrelated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;p&gt;Transforming the random variable to with zero mean and unit variance. This tranasformation also removes the unit on the random variable.&lt;/p&gt;
&lt;p&gt;$$
X_{std} = \frac{X - \mu}{\sigma}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;z-test-vs-t-test&#34;&gt;Z-Test vs. T-Test&lt;/h2&gt;
&lt;p&gt;Both the z-test and the t-test are statistical methods used to test hypotheses about means, but they are suited to different situations based on the distribution of the data and sample sizes.&lt;/p&gt;
&lt;h3 id=&#34;z-test&#34;&gt;Z-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The z-test is used when the population &lt;em&gt;variance is known and the sample size is large&lt;/em&gt; (typically, n &amp;gt; 30). It can also be used for small samples if the data is known to follow a normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population variance is known.&lt;/li&gt;
&lt;li&gt;The sample size is large enough for the Central Limit Theorem to apply, which ensures that the means of the samples are normally distributed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a z-test is calculated as follows:
$$
Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The t-test is used when the population &lt;em&gt;variance is unknown and the sample size is small&lt;/em&gt;. It is the appropriate test when dealing with estimates of the standard deviation from a normally distributed sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population from which samples are drawn is normally distributed.&lt;/li&gt;
&lt;li&gt;The population variance is unknown, and the sample variance is used as an estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a t-test is calculated as follows:
$$
T = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $s$ is the sample standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Degrees of Freedom:&lt;/strong&gt; The degrees of freedom for the t-test are $n-1$, which affects the shape of the t-distribution used to determine the p-value.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-differences&#34;&gt;Key Differences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Deviation:&lt;/strong&gt; The z-test uses the population standard deviation, while the t-test uses the sample&amp;rsquo;s standard deviation as an estimate of the population’s standard deviation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size:&lt;/strong&gt; The z-test is typically used for larger sample sizes or when the population standard deviation is known, whereas the t-test is used for smaller sample sizes or when the population standard deviation is unknown.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution:&lt;/strong&gt; The z-test statistic follows a normal distribution, while the t-test statistic follows a t-distribution, which is more spread out with heavier tails, providing a more conservative test for small sample sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;A comparison plot between the t-distribution and the standard normal distribution can be find &lt;a href=&#34;https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html#:~:text=What&#39;s%20the%20key%20difference%20between,on%20the%20sample%20standard%20deviation.&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributions</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/</guid>
      <description>&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;Probability distributions describe how the probabilities of a &lt;strong&gt;random variable&lt;/strong&gt; are distributed. Here are the two main types of probability functions associated with these distributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Probability Density Function (PDF, continuous):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The area under the PDF curve between two points represents the probability of the variable falling within that range.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability Mass Function (PMF, discrete):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The binomial distribution, and the Poisson distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Distribution Function (CDF):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The CDF is used to determine the probability that a random variable (X) is $\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chebyshevs-theorem&#34;&gt;Chebyshev&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem, also known as Chebyshev&amp;rsquo;s Inequality, is a fundamental result in probability theory that provides a way &lt;strong&gt;to estimate the probability that a random variable differs from its mean&lt;/strong&gt;. This theorem is not restricted to normally distributed data, making it very versatile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem Statement:&lt;/strong&gt;
$$
P(|X-\mu| &amp;lt; k\sigma) \geq 1 - \frac{1}{k^2}
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;($\mu$) is the mean of the random variable ( X ),&lt;/li&gt;
&lt;li&gt;($\sigma$) is the standard deviation of ( X ),&lt;/li&gt;
&lt;li&gt;($k$) is a positive number greater than 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generality:&lt;/strong&gt; Chebyshev&amp;rsquo;s theorem applies to any probability distribution where the mean and variance are defined.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implication:&lt;/strong&gt; The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\frac{1}{4} = 0.75)$).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discrete-distributions&#34;&gt;Discrete Distributions&lt;/h2&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as &amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are only two possible outcomes for each trial: success (1) and failure (0).&lt;/li&gt;
&lt;li&gt;The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability mass function (PMF) for the Binomial distribution is expressed as:
$$
P(X=x) = C^{n}_{x} \theta^x (1-\theta)^{n-x}
$$
where $\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Consider flipping a fair coin five times. What is the probability of getting exactly two heads?&lt;/p&gt;
&lt;p&gt;The calculation is as follows:
$$
P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \frac{5!}{2!3!} \times 0.25 \times 0.125 = 31.25%
$$&lt;/p&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;p&gt;The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF:&lt;/strong&gt;
$$
g(x; \theta) = \theta(1-\theta)^{x-1}
$$
where, $\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Properties:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean ($\mu$):&lt;/strong&gt; The expected number of trials to get the first success is given by:
$$
\mu = \frac{1}{\theta}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance ($\sigma^2$):&lt;/strong&gt; The variance of the number of trials to get the first success is:
$$
\sigma^2 = \frac{1-\theta}{\theta^2}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean and variance provide insights into the &amp;ldquo;spread&amp;rdquo; or variability of trials needed to achieve the first success, with higher values of $\theta$ leading to fewer expected trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distribution is commonly used in &lt;em&gt;quality control&lt;/em&gt;, &lt;em&gt;reliability testing&lt;/em&gt;, and other areas where the &amp;ldquo;time&amp;rdquo; or number of trials until the first success is of interest.&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;
&lt;p&gt;A practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Independence of events holds only if the CCD is not saturated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rate of event occurrence, $\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \Delta t$ is $\lambda \Delta t$.&lt;/li&gt;
&lt;li&gt;The probability of an event in the interval ${t, t+\Delta t}$ is independent of previous events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The probability mass function (PMF) of the Poisson distribution is defined as:
$$
P(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
where $\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.&lt;/p&gt;
&lt;p&gt;For the Poisson distribution, the expected value (mean) is $\mu = \lambda$, and the variance is $\sigma^2 = \lambda$.&lt;/p&gt;
&lt;h4 id=&#34;poisson-noise-shot-noise&#34;&gt;Poisson Noise (Shot Noise)&lt;/h4&gt;
&lt;p&gt;Since $\sigma^2 = \lambda$, the standard deviation (Poisson noise) is $\sigma = \sqrt{\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\sigma = \sqrt{N}$.&lt;/p&gt;
&lt;p&gt;Due to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows:
$$
SNR = \frac{\mu}{\sigma} = \frac{\mu}{\sqrt{\mu}} = \sqrt{\mu}
$$
This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Distributions</title>
      <link>https://shihyuntang.github.io/tutorials/tools/1-2-probability/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/tools/1-2-probability/</guid>
      <description>&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;Probability distributions describe how the probabilities of a &lt;strong&gt;random variable&lt;/strong&gt; are distributed. Here are the two main types of probability functions associated with these distributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Probability Density Function (PDF, continuous):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The area under the PDF curve between two points represents the probability of the variable falling within that range.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability Mass Function (PMF, discrete):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The binomial distribution, and the Poisson distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Distribution Function (CDF):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The CDF is used to determine the probability that a random variable (X) is $\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chebyshevs-theorem&#34;&gt;Chebyshev&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem, also known as Chebyshev&amp;rsquo;s Inequality, is a fundamental result in probability theory that provides a way &lt;strong&gt;to estimate the probability that a random variable differs from its mean&lt;/strong&gt;. This theorem is not restricted to normally distributed data, making it very versatile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem Statement:&lt;/strong&gt;
$$
P(|X-\mu| &amp;lt; k\sigma) \geq 1 - \frac{1}{k^2}
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;($\mu$) is the mean of the random variable ( X ),&lt;/li&gt;
&lt;li&gt;($\sigma$) is the standard deviation of ( X ),&lt;/li&gt;
&lt;li&gt;($k$) is a positive number greater than 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generality:&lt;/strong&gt; Chebyshev&amp;rsquo;s theorem applies to any probability distribution where the mean and variance are defined.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implication:&lt;/strong&gt; The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\frac{1}{4} = 0.75)$).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discrete-distributions&#34;&gt;Discrete Distributions&lt;/h2&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as &amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are only two possible outcomes for each trial: success (1) and failure (0).&lt;/li&gt;
&lt;li&gt;The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability mass function (PMF) for the Binomial distribution is expressed as:
$$
P(X=x) = C^{n}_{x} \theta^x (1-\theta)^{n-x}
$$
where $\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Consider flipping a fair coin five times. What is the probability of getting exactly two heads?&lt;/p&gt;
&lt;p&gt;The calculation is as follows:
$$
P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \frac{5!}{2!3!} \times 0.25 \times 0.125 = 31.25%
$$&lt;/p&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;p&gt;The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF:&lt;/strong&gt;
$$
g(x; \theta) = \theta(1-\theta)^{x-1}
$$
where, $\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Properties:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean ($\mu$):&lt;/strong&gt; The expected number of trials to get the first success is given by:
$$
\mu = \frac{1}{\theta}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance ($\sigma^2$):&lt;/strong&gt; The variance of the number of trials to get the first success is:
$$
\sigma^2 = \frac{1-\theta}{\theta^2}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean and variance provide insights into the &amp;ldquo;spread&amp;rdquo; or variability of trials needed to achieve the first success, with higher values of $\theta$ leading to fewer expected trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distribution is commonly used in &lt;em&gt;quality control&lt;/em&gt;, &lt;em&gt;reliability testing&lt;/em&gt;, and other areas where the &amp;ldquo;time&amp;rdquo; or number of trials until the first success is of interest.&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;
&lt;p&gt;A practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Independence of events holds only if the CCD is not saturated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rate of event occurrence, $\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \Delta t$ is $\lambda \Delta t$.&lt;/li&gt;
&lt;li&gt;The probability of an event in the interval ${t, t+\Delta t}$ is independent of previous events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The probability mass function (PMF) of the Poisson distribution is defined as:
$$
P(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
where $\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.&lt;/p&gt;
&lt;p&gt;For the Poisson distribution, the expected value (mean) is $\mu = \lambda$, and the variance is $\sigma^2 = \lambda$.&lt;/p&gt;
&lt;h4 id=&#34;poisson-noise-shot-noise&#34;&gt;Poisson Noise (Shot Noise)&lt;/h4&gt;
&lt;p&gt;Since $\sigma^2 = \lambda$, the standard deviation (Poisson noise) is $\sigma = \sqrt{\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\sigma = \sqrt{N}$.&lt;/p&gt;
&lt;p&gt;Due to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows:
$$
SNR = \frac{\mu}{\sigma} = \frac{\mu}{\sqrt{\mu}} = \sqrt{\mu}
$$
This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bayes&#39; theorem</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/</guid>
      <description>&lt;p&gt;Bayes Theorem is a fundamental principle in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It is particularly powerful in the context of predictive modeling and decision-making processes.&lt;/p&gt;
&lt;h3 id=&#34;mathematical-formulation&#34;&gt;Mathematical Formulation&lt;/h3&gt;
&lt;p&gt;If events $A$ and $B$ are independent, then the probability of $A$ given $B$ is simply the probability of $A$:
$$
P(A|B) = P(A)
$$
However, when events $A$ and $B$ are not independent, the relationship changes as follows:
$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{and} \quad P(B|A) = \frac{P(A \cap B)}{P(A)}
$$
From these relationships, we derive Bayes&amp;rsquo; Theorem:
$$
P(B|A) = \frac{P(B) P(A|B)}{P(A)}
$$
Bayes&amp;rsquo; Theorem can be interpreted in terms of updating beliefs:
$$
\text{Posterior} = \frac{\text{Prior} \times \text{Likelihood}}{\text{Evidence}}
$$&lt;/p&gt;
&lt;h3 id=&#34;components-of-bayes-theorem&#34;&gt;Components of Bayes&amp;rsquo; Theorem&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Likelihood ( $ P(x|\theta) $ ):&lt;/strong&gt;
The likelihood is a function that gives the plausibility of a model parameter value ($\theta$) given specific observed data ($x$). That is, In many applications, the likelihood is assumed to follow a normal distribution:
$$
L(x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\theta)^2}{2\sigma^2}}
$$
where $\theta$ represents the parameter of interest, $x$ represents the data, and $\sigma^2$ is the variance.
&lt;blockquote&gt;
&lt;p&gt;Likelihood ask: If I plug in this $\theta$ value to the model, how well does the results explain the observed data?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior ( $ P(\theta) $ ):&lt;/strong&gt;
The prior represents the initial belief about the distribution of the parameter before considering the current data. Priors can be subjective or based on previous studies:
$$
P(\theta)
$$
It can be uniform (representing no initial preference) or follow a specific distribution that reflects prior knowledge about the parameter, e.g., a Gaussian prior, gamma and beta distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evidence or Normalizing Constant ( $ P(A) $ ):&lt;/strong&gt;
Often considered as a normalizing factor, the evidence ensures that the posterior probabilities sum to one. It is calculated as:
$$
P(x) = \int P(x|\theta) P(\theta) d\theta
$$
This factor is essential for method like Nested Sampling (NS), and usually been ignored in the MCMC method.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Posterior ( $ P(\theta|x) $ ):&lt;/strong&gt;
The posterior probability reflects the &lt;strong&gt;updated belief&lt;/strong&gt; about the parameter after considering the new evidence. It combines the prior and the likelihood given new data:
$$
P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta) d\theta}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;practical-applications&#34;&gt;Practical Applications&lt;/h2&gt;
&lt;p&gt;Bayes&amp;rsquo; Theorem is used extensively in various fields including exoplanet study, machine learning, medical testing, and any scenario requiring iterative updating of beliefs upon new evidence. Understanding how to apply the theorem allows for more informed decision-making processes and predictions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;relationship-between-likelihood-and-chi-squared-statistic&#34;&gt;Relationship Between Likelihood and Chi-Squared Statistic&lt;/h2&gt;
&lt;p&gt;The likelihood function in statistical models often measures how well a set of parameters fits the data. When the data and the model predictions vary according to a normal distribution, the likelihood function can be directly linked to the chi-squared statistic.&lt;/p&gt;
&lt;h3 id=&#34;chi-squared-statistic&#34;&gt;Chi-Squared Statistic:&lt;/h3&gt;
&lt;p&gt;The chi-squared statistic is a measure of how expectations compare to actual observed data. In the context of likelihood calculations, the chi-squared statistic quantifies the discrepancy between observed data and the data predicted by the model, under the assumption that the discrepancies are normally distributed.&lt;/p&gt;
&lt;h3 id=&#34;calculation&#34;&gt;Calculation:&lt;/h3&gt;
&lt;p&gt;To calculate the chi-squared statistic as a measure of likelihood, you can use the following formula:
$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{\sigma_i^2}
$$
where $O_i$ are the observed values, $E_i$ are the expected values predicted by the model, and $\sigma_i^2$ is the variance associated with each observation.&lt;/p&gt;
&lt;h3 id=&#34;using-chi-squared-to-calculate-likelihood&#34;&gt;Using Chi-Squared to Calculate Likelihood:&lt;/h3&gt;
&lt;p&gt;The likelihood of observing the data given the model can be expressed as:
$$
L = e^{-\chi^2/2}
$$
A more memorable form is:
$$
\chi^2 = -2 \ln{L}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The $\chi^2$ is just $-2$ times the log-likelihood of a Gaussian model on observation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These formulation arises from the exponential component of the PDF of the normal distribution, which is what a likelihood function usually take, thus, a easier way to remember how to calculate the likelihood.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Difference Between MCMC and NS</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/</guid>
      <description>&lt;p&gt;Both &lt;strong&gt;Markov Chain Monte Carlo (MCMC)&lt;/strong&gt; and &lt;strong&gt;Nested Sampling (NS)&lt;/strong&gt; are Bayesian inference techniques. They both rely on Bayes’ theorem, but they explore parameter space in fundamentally different ways. The biggest conceptual contrast is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MCMC samples in the &lt;em&gt;posterior&lt;/em&gt; space&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NS samples in the &lt;em&gt;prior&lt;/em&gt; space&lt;/strong&gt;, gradually restricting it to higher-likelihood regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before digging into the differences, let’s recall Bayes’ theorem:
$$
\text{Posterior}(\mathcal{P}) = \frac{
\text{Likelihood}(\mathcal{L}) \times \text{Prior($\pi$)}}{\text{Evidence($\mathcal{Z}$)}}
$$
$$
\mathcal{P}(\theta|x)
= \frac{
\mathcal{L}(x|\theta)\times \pi(\theta)
}{\int \mathcal{L}(x|\theta) \pi(\theta) d\theta
}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;markov-chain-monte-carlo-mcmc&#34;&gt;Markov Chain Monte Carlo (MCMC)&lt;/h2&gt;
&lt;p&gt;MCMC focuses directly on sampling from the &lt;strong&gt;posterior distribution&lt;/strong&gt;. The idea is simple:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start with an initial guess&lt;/strong&gt; for the parameters ($\theta$) for each &lt;em&gt;walker&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;For each proposed step, compute
&lt;ul&gt;
&lt;li&gt;the likelihood: $\mathcal{L}(\theta) = P(x|\theta)$&lt;/li&gt;
&lt;li&gt;the prior: $\pi(\theta) = P(\theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Combine them to get the &lt;strong&gt;unnormalized posterior&lt;/strong&gt;:
$$
\mathcal{P}(\theta|x) \propto \mathcal{L}(\theta)\pi(\theta)
$$
(We ignore the evidence because it’s just a constant normalization factor.)&lt;/li&gt;
&lt;li&gt;Use this posterior to decide whether the walker accepts or rejects the proposed move&amp;hellip;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Over time, the chain spends more time in regions where the posterior is high. This produces samples distributed according to the true posterior — which is why we say that &lt;strong&gt;MCMC samples from the posterior&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;MCMC is excellent for &lt;strong&gt;parameter estimation&lt;/strong&gt;, but it does not naturally compute the &lt;strong&gt;evidence&lt;/strong&gt;, which is required for comparing between different models.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;nested-sampling-ns&#34;&gt;Nested Sampling (NS)&lt;/h2&gt;
&lt;p&gt;Nested Sampling takes a very different approach from MCMC.&lt;br&gt;
Instead of wandering around the posterior, NS &lt;strong&gt;systematically explores the prior&lt;/strong&gt;, gradually shrinking it toward regions of higher likelihood.&lt;/p&gt;
&lt;p&gt;The basic flow looks like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialize a set of random parameters&lt;/strong&gt; (called &lt;em&gt;live points&lt;/em&gt;)&lt;br&gt;
These are drawn directly from the prior distribution $\pi(\theta)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each live point, compute the likelihood:&lt;br&gt;
$$
\mathcal{L}(\theta) = P(x \mid \theta).
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identify the live point with the lowest likelihood&lt;/strong&gt; and remove it.&lt;br&gt;
This becomes a &lt;em&gt;dead point&lt;/em&gt; (a point we won&amp;rsquo;t revisit, but it &lt;em&gt;will&lt;/em&gt; help build the posterior).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Replace it&lt;/strong&gt; with a new sample drawn from the prior, &lt;strong&gt;subject to a constraint:&lt;/strong&gt;&lt;br&gt;
the new point must have a likelihood higher than the one just removed.&lt;br&gt;
This gradually forces the live points into smaller and smaller regions of higher likelihood.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We repeat steps 3–4 many times. Eventually, the remaining prior volume becomes so small (or the change in the evidence becomes negligible) that continuing does not help, and we stop.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And this is why we say that &lt;strong&gt;NS samples from the prior&lt;/strong&gt;!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;more-about-ns&#34;&gt;More About NS&lt;/h2&gt;
&lt;p&gt;At the stopping point (after step 5), what do we have?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A list of &lt;strong&gt;dead points&lt;/strong&gt; $\theta_i$ with likelihoods&lt;br&gt;
$$
\mathcal{L}_i = \mathcal{L}(\theta_i).
$$&lt;/li&gt;
&lt;li&gt;An estimate of the &lt;strong&gt;remaining prior volume&lt;/strong&gt; $X_i$ at each iteration,&lt;br&gt;
which tells us how much of the prior space is left when that point is removed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To compute the posterior, we first need the normalization factor: the &lt;strong&gt;Evidence&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;evidence-mathcalz&#34;&gt;Evidence ($\mathcal{Z}$)&lt;/h3&gt;
&lt;p&gt;Recall that the evidence is:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{Z} = \int P(x \mid \theta)\ \pi(\theta)\ d\theta,
$$&lt;/p&gt;
&lt;p&gt;the &lt;strong&gt;volume under the likelihood curve, weighted by the prior&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Directly computing this in many dimensions is hard.&lt;br&gt;
But NS cleverly tracks how the prior volume shrinks as we move to higher likelihoods.&lt;/p&gt;
&lt;p&gt;Define the remaining prior volume as:&lt;/p&gt;
&lt;p&gt;$$
X(\lambda) = \int_{\mathcal{L}(\theta) &amp;gt; \lambda} \pi(\theta) d\theta,
$$&lt;/p&gt;
&lt;p&gt;the fraction of prior volume where $\mathcal{L} &amp;gt; \lambda$.&lt;/p&gt;
&lt;p&gt;Skilling (2004) showed that the evidence ($\mathcal{Z}$) then becomes a &lt;strong&gt;1D integral&lt;/strong&gt; over this volume:
$$
\mathcal{Z} = \int_0^1 \mathcal{L}(X)\ dX.
$$&lt;/p&gt;
&lt;p&gt;NS approximates this integral by a sum over the dead points:&lt;/p&gt;
&lt;p&gt;$$
Z \approx \sum_i L_i \ \Delta X_i
= \sum_i L_i (X_{i-1} - X_i).
$$&lt;/p&gt;
&lt;h3 id=&#34;unnormalized-posterior-the-weight-w_i&#34;&gt;Unnormalized Posterior (the weight $w_i$)&lt;/h3&gt;
&lt;p&gt;Each dead point represents a slice of the shrinking prior volume. The &lt;strong&gt;unnormalized posterior contribution&lt;/strong&gt; for each dead point is:&lt;/p&gt;
&lt;p&gt;$$
w_i = L_i\ (X_{i-1} - X_i),
$$
which reads as: &lt;strong&gt;likelihood of that slice × size of that slice in prior volume&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This automatically gives:
$$
\mathcal{Z} \approx \sum_i w_i.
$$&lt;/p&gt;
&lt;h3 id=&#34;posterior-mathcalp&#34;&gt;Posterior ($\mathcal{P}$)&lt;/h3&gt;
&lt;p&gt;Once we have the weights and the evidence, the posterior samples are simply:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{P}(\theta_i \mid x) = \frac{w_i}{\mathcal{Z}} = \frac{w_i}{\sum_i w_i}.
$$&lt;/p&gt;
&lt;p&gt;That’s it — the dead points (with the right weights) become your posterior samples.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ns-in-the-python-dynesty-package&#34;&gt;NS in the Python &lt;code&gt;dynesty&lt;/code&gt; Package&lt;/h2&gt;
&lt;p&gt;Here we connect the quantities discussed above to the actual outputs of a Nested Sampling
run using the Python package &lt;strong&gt;dynesty&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;logl      = results.logl       # log-likelihoods of dead points
logwt     = results.logwt      # log-weights (unnormalized posterior weights)
logvol    = results.logvol     # log prior volumes of dead points
logz      = results.logz       # cumulative log-evidence estimates
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of these correspond directly to the core definitions of Nested Sampling.&lt;/p&gt;
&lt;p&gt;Note, dynesty includes an extra final contribution from the last batch of
live points. That means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;logwt[:-1]&lt;/code&gt;: weights from dead points only&lt;/li&gt;
&lt;li&gt;&lt;code&gt;logwt[-1]&lt;/code&gt;: contribution from remaining live points at termination&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Distribution</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/2-1-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/2-1-distribution/</guid>
      <description>&lt;h2 id=&#34;empirical-distribution-function-edf&#34;&gt;Empirical Distribution Function (EDF)&lt;/h2&gt;
&lt;p&gt;The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as:
$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n I[x_i \leq x]
$$
where $I[\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\frac{1}{n}$. An example plot of an EDF is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/edf.png&#34; alt=&#34;edf&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kolmogorov-smirnov-test-ks-test&#34;&gt;Kolmogorov-Smirnov Test (KS Test)&lt;/h2&gt;
&lt;p&gt;The Kolmogorov-Smirnov (KS) Test measures the maximum distance ($D$) between two distribution functions. This can be used to compare an empirical distribution with a theoretical model (one-sample test) or two empirical distributions (two-sample test). If the distributions are identical, $D$ equals zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
D_n = \max_x |F_n(x) - S_n(x)|
$$
A table for the critical values of $D_n$ can be found &lt;a href=&#34;https://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If $D_n$ exceeds the critical value, we can &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that there is no significant difference between the two distributions.&lt;/p&gt;
&lt;h2 id=&#34;cramér-von-mises-statistic-cvm&#34;&gt;Cramér-von Mises Statistic (CvM)&lt;/h2&gt;
&lt;p&gt;The Cramér-von Mises statistic is used to quantify the goodness of fit of an empirical distribution to a theoretical model. It is particularly useful as it considers the squared differences over all points, providing a more sensitive measure to differences &lt;strong&gt;in the tails&lt;/strong&gt; of the distributions.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
C_n = n \int_{-\infty}^\infty [F_n(x) - S(x)]^2 dS(x)
$$ --&gt;
&lt;p&gt;This statistic assesses the integrated squared distance between the empirical distribution function $F_n(x)$ and the theoretical distribution $S(x)$, weighted by the number of observations $n$.&lt;/p&gt;
&lt;h2 id=&#34;anderson-darling-statistic-ad&#34;&gt;Anderson-Darling Statistic (AD)&lt;/h2&gt;
&lt;p&gt;The Anderson-Darling statistic is a modification of the Cramér-von Mises statistic that gives more weight to the tails of the distribution. It is particularly effective in identifying departures from a theoretical distribution in the tails.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
A^2 = n \int_{-\infty}^\infty \frac{[F_n(x) - S(x)]^2}{S(x)(1 - S(x))} \, dx
$$ --&gt;
&lt;p&gt;This weighted approach makes the AD statistic more sensitive to discrepancies in the distribution&amp;rsquo;s tails than the CvM statistic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Both CvM and AD tests are powerful tools for statistical hypothesis testing, especially in scenarios where understanding the tail behavior of distributions is crucial.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;wilcoxon-rank-sum-test&#34;&gt;Wilcoxon Rank Sum Test&lt;/h2&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test, also known as the Mann-Whitney Rank Sum test, is designed to assess &lt;em&gt;whether two independent samples come from the same distribution&lt;/em&gt;. Here’s how the test statistic is calculated:&lt;/p&gt;
&lt;!-- It is especially useful when the data does not meet the assumptions necessary for the t-test, primarily concerning normality --&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Combine and Rank the Data:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Combine all observations from both samples into a single dataset.&lt;/li&gt;
&lt;li&gt;Rank all observations from the smallest to largest. Ties are given a rank equal to the average of the ranks they would have otherwise occupied.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate the Rank Sums:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Calculate the sum of the ranks for observations from each sample separately. Let $T_1$ be the sum of ranks for the first sample, and $T_2$ for the second sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute the Test Statistic:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The test statistic $U$ is calculated using:
$$
U = T_1 - \frac{n_1(n_1+1)}{2}
$$
where $n_1$ is the number of observations in the first sample. $U$ can also be computed for the second sample, and the smaller of the two $U$ values is often used as the test statistic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine the Significance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The significance of the observed $T_1$ value is determined by comparing it to values in a reference distribution, which approximates a normal distribution under the null hypothesis when the sample sizes are sufficiently large. The mean and standard deviation of $T_1$ are used to compute a z-score:
$$
|z| = \left| \frac{T_1 - \text{mean}(T_1)}{\text{std dev}(T_1)} \right|
$$
where $\text{mean}(T_1) = \frac{n_1(n_1+n_2+1)}{2}$, and $\text{Var}(T_1) = \frac{n_1 n_2(n_1+n_2+1)}{2}$.&lt;/li&gt;
&lt;li&gt;The p-value is then calculated from the normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ### Interpretation

- If the p-value is less than the chosen significance level (commonly 0.05), then there is sufficient evidence to reject the null hypothesis, suggesting that there is a statistically significant difference in the distributions of the two groups.

- If the p-value is greater, then we do not reject the null hypothesis, suggesting that any observed differences could reasonably occur by random chance under the assumption of identical distributions. --&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test does not require the data to follow a specific distribution, making it a robust and widely applicable non-parametric method for comparing two samples.&lt;/p&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;p&gt;The Kruskal-Wallis (KW) test is used to determine if there are statistically significant differences between the distributions of two or more groups of an independent variable. It generalizes the Wilcoxon Rank Sum Test to more than two groups. The null hypothesis assumes that all groups come from identical distributions. The test statistic follows a chi-squared ($\chi^2$) distribution with $k-1$ degrees of freedom, where $k$ is the number of groups.&lt;/p&gt;
&lt;h2 id=&#34;comparison-of-statistical-distribution-tests&#34;&gt;Comparison of Statistical Distribution Tests&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Key Usage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirement&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Distribution Assumption&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;KS Test&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing a sample with a reference distribution&lt;/td&gt;
&lt;td&gt;One or two samples, continuous or ordinal&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in location, scale, and shape&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CvM Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit to a theoretical distribution&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Integrates squared differences; sensitive across entire distribution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AD Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit with emphasis on tail differences&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Increased weight to tails; highly sensitive to tail discrepancies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Wilcoxon Rank Sum&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing two independent samples&lt;/td&gt;
&lt;td&gt;Two independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in medians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kruskal-Wallis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing more than two independent samples&lt;/td&gt;
&lt;td&gt;Two or more independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Generalization of Wilcoxon, sensitive to differences across multiple samples&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/2-2-correlation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/2-2-correlation/</guid>
      <description>&lt;p&gt;Nonparametric correlation tests are essential tools for assessing the strength and direction of a relationship between two datasets, especially when the underlying distributions are unknown or non-normal. These tests are robust alternatives to the Pearson correlation coefficient, suitable for various types of relationships. The null hypothesis for these tests is that there is no correlation between the datasets.&lt;/p&gt;
&lt;h3 id=&#34;pearson-correlation-coefficient&#34;&gt;*Pearson Correlation Coefficient&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Pearson correlation test is a &lt;strong&gt;parametric test&lt;/strong&gt;, but I put it here for completeness and for comparison.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Pearson correlation coefficient measures the linear relationship between two continuous variables. It is a parametric test and assumes that the data is normally distributed. The coefficient varies between -1 and +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.&lt;/p&gt;
&lt;h3 id=&#34;spearmans-rank-correlation-coefficient-rho&#34;&gt;Spearman&amp;rsquo;s Rank Correlation Coefficient ($\rho$)&lt;/h3&gt;
&lt;p&gt;Spearman&amp;rsquo;s correlation assesses how well the relationship between two variables can be described using a monotonic function. It does not require the data to be normally distributed, as it uses the rank of the data rather than the actual values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\rho = 1 - \frac{6 \sum d_i^2 + T}{n(n^2 - 1)}
$$
where $d_i$ is the difference between the ranks of corresponding variables and $n$ is the number of observations. For handling ties, the correction term $T$ is applied:
$$
T = \sum t_x \left[ \frac{x^3 - x}{12} \right]
$$
where $t_x$ is the number of ties involving $x$ elements. To further estimate the $p$-value, the $z$ score can be calculated by:
$$
z = \sqrt{n-1} \rho
$$
which approximates a normal distribution under the null hypothesis. One can use the $\rho$ statistic and compare to the critical values &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/spearman.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kendalls-tau-tau&#34;&gt;Kendall&amp;rsquo;s Tau ($\tau$)&lt;/h3&gt;
&lt;p&gt;Kendall&amp;rsquo;s Tau measures the ordinal association between two variables by considering the number of concordant and discordant pairs. It is less sensitive to outliers than Pearson and can be more interpretable in terms of the proportion of concordant pairs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
\tau = \frac{N_c - N_d}{\sqrt{(N_c + N_d + T)(N_c + N_d + U)}}
$$
where $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $T$ is the number of ties on one variable, and $U$ is the number of ties on the other variable.&lt;/p&gt;
&lt;p&gt;The critical values for $\tau$ can be find &lt;a href=&#34;https://www.york.ac.uk/depts/maths/tables/kendall.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;comparison-of-correlation-tests&#34;&gt;Comparison of Correlation Tests&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Best Use Case&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirements&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Pearson&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Linear relationships&lt;/td&gt;
&lt;td&gt;Highly sensitive to linear trends&lt;/td&gt;
&lt;td&gt;Normal distribution, continuous data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Spearman&amp;rsquo;s rho&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Monotonic relationships, not necessarily linear&lt;/td&gt;
&lt;td&gt;Sensitive to monotonic trends, not affected by outliers&lt;/td&gt;
&lt;td&gt;Ordinal data or non-normal distributions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kendall&amp;rsquo;s tau&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;General increasing or decreasing trends, less intensive computation&lt;/td&gt;
&lt;td&gt;Less sensitive to errors in data, good for small samples or data with many ties&lt;/td&gt;
&lt;td&gt;Ordinal data, robust against outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Data Smoothing-Density Estimation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/3-1-data-smoothing/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/3-1-data-smoothing/</guid>
      <description>&lt;p&gt;Proper smoothing of data is crucial in various applications, especially for interpolating and visualizing results. A classic example involves choosing the right bin size for histograms. Incorrect bin sizes can either obscure significant data features by being too large or create misleading features if they are too small.&lt;/p&gt;
&lt;h3 id=&#34;scotts-rule-for-bin-width&#34;&gt;Scott&amp;rsquo;s Rule for Bin Width&lt;/h3&gt;
&lt;p&gt;Scott&amp;rsquo;s rule provides a method to determine the optimal bin width for a histogram, balancing detail and smoothness. The formula for Scott&amp;rsquo;s bin width is:
$$
h_{\text{Scott}} = \frac{3.5 \cdot \text{std}}{n^{1/3}} \quad \text{or} \quad \frac{2 \cdot \text{IQR}}{n^{1/3}}
$$
where $\text{std}$ is the standard deviation, $\text{IQR}$ is the interquartile range, and $n$ is the number of data points. This method aims to minimize potential distortion in the histogram by accounting for the variability and size of the data set.&lt;/p&gt;
&lt;h3 id=&#34;average-smoothing-histogram&#34;&gt;Average Smoothing Histogram&lt;/h3&gt;
&lt;p&gt;Average smoothing, applied to histograms, involves averaging adjacent bins to reduce variance within the bins. This technique smooths out fluctuations that might be random and highlights broader trends in the data distribution.&lt;/p&gt;
&lt;h3 id=&#34;kernel-density-estimation-kde&#34;&gt;Kernel Density Estimation (KDE)&lt;/h3&gt;
&lt;p&gt;Kernel Density Estimation is a non-parametric way to estimate the probability density function of a random variable. KDE smooths the data by convolving it with a kernel, which is a predefined function, typically Gaussian. The formula for KDE is:
$$
\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
$$
where $K$ is the kernel function, $x_i$ are the data points, $h$ is the bandwidth, and $n$ is the number of data points. The choice of bandwidth $h$ crucially affects the estimator&amp;rsquo;s bias and variance.&lt;/p&gt;
&lt;h3 id=&#34;adaptive-smoothing&#34;&gt;Adaptive Smoothing&lt;/h3&gt;
&lt;p&gt;Adaptive smoothing techniques adjust the smoothing parameters locally, depending on the density or other characteristics of the data. These methods aim to achieve &lt;em&gt;better smoothing in areas with higher variability or lower density,&lt;/em&gt; allowing more detailed features to emerge in dense regions while smoothing out noise in sparse regions.&lt;/p&gt;
&lt;h3 id=&#34;nadaraya-watson-estimator&#34;&gt;Nadaraya-Watson Estimator&lt;/h3&gt;
&lt;p&gt;The Nadaraya-Watson estimator is a type of kernel regression that uses locally weighted averages to estimate conditional expectations. It is particularly useful in regression analysis to model the relationship between variables.&lt;/p&gt;
&lt;!-- The estimator is defined as:
$$
  \hat{m}(x) = \frac{\sum_{i=1}^n K_h(x - x_i) y_i}{\sum_{i=1}^n K_h(x - x_i)}
$$
where $y_i$ are the response variables, and $K_h$ is a kernel weighted by a bandwidth $h$. This method is effective in capturing the local variability of the data without assuming a specific parametric form for the relationship.

These techniques in data smoothing and density estimation are invaluable tools in data analysis, offering different approaches to uncovering and representing the underlying patterns in the data. Each method has its strengths and is suited to different types of data challenges. --&gt;
</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/4-1-regression/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/4-1-regression/</guid>
      <description>&lt;p&gt;Regression analysis is fundamental in statistical modeling, used for predicting and forecasting, and understanding relationships between variables. Below are various regression methods:&lt;/p&gt;
&lt;h3 id=&#34;least-square-linear-regression-without-uncertainty&#34;&gt;Least-Square Linear Regression (without Uncertainty)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ordinary Least Squares (OLS)&lt;/strong&gt;: This method minimizes the residual sum of squares (RSS) between the observed values in the dataset and the values predicted by the linear model.
$$
\text{min RSS} = \text{min} \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
$$
where $X_i$ and $Y_i$ are the observed values, and $\beta_0$ and $\beta_1$ are the intercept and slope of the linear model, respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Symmetric Least Squares-Orthogonal Regression&lt;/strong&gt;: This method minimizes the summed squares of the residuals orthogonal to the regression line, providing a more general approach than OLS when errors in both variables need to be considered.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Regression - Thiel-Sen Estimator&lt;/strong&gt;: This technique uses the median of the slopes between pairs of points as the estimator, providing robustness against outliers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantile Regression&lt;/strong&gt;: Focuses on estimating either the conditional median or other quantiles of the response variable, providing a more comprehensive view of the possible outcome distribution than mean regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maximum Likelihood Estimation&lt;/strong&gt;: Assumes a probability distribution model for the data, often a normal distribution, and finds the parameter values that maximize the likelihood of observing the data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;weighted-least-squares-with-uncertainty&#34;&gt;Weighted Least Squares (with Uncertainty)&lt;/h3&gt;
&lt;p&gt;In scenarios with heteroscedastic errors (errors that varies), weighted least squares (WLS) is more appropriate:&lt;/p&gt;
&lt;p&gt;$$
S_{r,wt} = \sum_{i=1}^n \frac{(Y_i - \beta_0 - \beta_1 X_i)^2}{\sigma_{Y,i}^2}
$$
where $\sigma_{Y,i}^2$ is the variance associated with each observation, weighting the residuals accordingly. This is related to the $\chi^2$ statistic used in minimum $\chi^2$ regression:&lt;/p&gt;
&lt;p&gt;$$
\chi^2_{me} = \sum_{i=1}^n (O_i - M_i)^2 / \sigma_{i,me}^2
$$&lt;/p&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;Logistic regression is used for modeling binary outcome variables by using the logistic function to estimate probabilities that can be transformed into binary values.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model-validation-and-selection&#34;&gt;Model Validation and Selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Coefficient of Determination (R²)&lt;/strong&gt;: Measures the proportion of variability in a dataset that is explained by the regression model.
$$
R^2 = 1 - \frac{ \sum_{i=1}^n (Y_i - \hat{Y_i})^2 }{ \sum_{i=1}^n (Y_i - \bar{Y})^2 }
$$
where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{Y_i}$ is the predicted value of the dependent variable for the $i$-th observation from the model&lt;/li&gt;
&lt;li&gt;$\bar{Y}$ is the mean of the observed data $Y_i$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An $R^2$ value of 1 implies a perfect fit, meaning that the model explains all the variability of the response data around its mean. Conversely, an $R^2$ value of 0 indicates at bad fit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adjusted $R^2$&lt;/strong&gt;: Adjusts $R²$ for the number of predictors in the model, preventing overfitting by penalizing excessive use of parameters.
$$
R_a^2 = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$
where $p$ is the number of parameters, and $n$ is the number of data points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normal quantile-quantile plot (Q-Q Plot)&lt;/strong&gt;: Used to check the normality of residuals. If the points lie along a straight line, the residuals are normally distributed, an assumption in many regression models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Akaike Information Criterion (AIC)&lt;/strong&gt;: The AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.
$$
\text{AIC} = 2k - 2\ln(\hat{L})
$$
where $k$ is the number of parameters in the model, and $\hat{L}$ is the maximum likelihood value.&lt;/p&gt;
&lt;p&gt;The model with the lowest AIC among a set of models is typically chosen. The AIC is particularly useful when a model is being fit to data: minimizing the AIC maximizes the likelihood function given the data while penalizing for increasing numbers of parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Information Criterion (BIC)&lt;/strong&gt;: The BIC is similar to the AIC but introduces &lt;em&gt;a stronger penalty for including additional variables to the model&lt;/em&gt;. It is derived from Bayesian probability.
$$
\text{BIC} = \ln(n)k - 2\ln(\hat{L})
$$
where $n$ is the number of observations, $k$ is the number of parameters in the model, and $\hat{L}$ is the maximum likelihood of the model.&lt;/p&gt;
&lt;p&gt;The BIC is generally stricter than the AIC and can be preferable for models with large $n$, penalizing free parameters more heavily. Like the AIC, &lt;strong&gt;the model with the lowest BIC is generally preferred&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ## When to Use AIC vs. BIC

| **Criterion** | **When to Use**                                                                                               |
|---------------|---------------------------------------------------------------------------------------------------------------|
| **AIC**       | More appropriate when the focus is on goodness of fit. Suitable for models where the sample size \( n \) is much larger than the number of parameters \( k \). Less strict about adding parameters. |
| **BIC**       | Used when model parsimony is important. Especially useful in models with a large number of observations \( n \) and relatively less concern about capturing every parameter influence. More stringent in penalizing free parameters. | --&gt;
&lt;!-- ## Summary Table for Regression Methods

| **Method**                      | **Best Use**                                           | **Assumptions/Features**                                   |
|---------------------------------|--------------------------------------------------------|------------------------------------------------------------|
| **OLS Regression**              | Linear relationships with constant variance            | Assumes normality and linearity                            |
| **Orthogonal Regression**       | Errors in both variables                               | Minimizes perpendicular distances                          |
| **Robust Regression**           | Outlier-heavy data                                     | Resistant to outliers in data                              |
| **Quantile Regression**         | Non-normal data; interested in other quantiles         | Does not assume a specific distribution for residuals      |
| **Weighted Least Squares**      | Varied error size across data range                    | Weights observations by the inverse of their variance      |
| **Logistic Regression**         | Binary outcome data                                    | Estimates probability of occurrence                        |
 --&gt;
</description>
    </item>
    
    <item>
      <title>Multivariate Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/</guid>
      <description>&lt;!-- 
## Principle Component Analysis (PCA)

### Procedure

1. Make data matrix $\mathbf{d}$
2. Center data to form matrix $\mathbf{x}$
3. Construct covariance matrix $\mathbf{\Sigma_x}$ from $\mathbf{x}$
4. Find eigenvalues and eigenvectors of $\mathbf{\Sigma_x}$. The eigenvalues is are the variance and the eigenvectors is the PCA components.
5. Plot each data point in PCA space
6. Try to make sense to what each PCA vector physically means.

- **Step 1**
  The matrix $\mathbf{d}$ has it rows as attribute, like the properties of each targets, or people, etc. And in columns are for different targets or people for example. 
  $$
    \mathbf{d} = \begin{bmatrix}
    d_{11} &amp; d_{21} &amp; \cdots &amp; d_{t1} \\\
    d_{12} &amp; d_{22} &amp; \cdots &amp; d_{t2} \\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\
    d_{1n} &amp; d_{2n} &amp; \cdots &amp; d_{tn}
    \end{bmatrix}
  $$
- **Setp 2**
   Center data to form matrix $\mathbf{x}$ via 
   $$
    \mathbf{x} = \mathbf{d} - \mathbf{&lt;\beta&gt;}
   $$
   where $\mathbf{&lt;\beta_j&gt;} = \frac{1}{t} \sum_{j=1}^t d_{ji}$. This is basically means that for each attribute (each row) in $\mathbf{d}$, each values in it minus that attribute&#39;s mean value. So we have centered data 
  $$
    \mathbf{x} = \begin{bmatrix}
    x_{11} &amp; x_{21} &amp; \cdots &amp; x_{t1} \\\
    x_{12} &amp; x_{22} &amp; \cdots &amp; x_{t2} \\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\
    x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{tn}
    \end{bmatrix}
  $$
- **Setp 3**
  Construct covariance matrix $\mathbf{\Sigma_x}$ from $\mathbf{x}$. We have
  $$
    \frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T) = E\begin{bmatrix}
    \frac{x_{11}^2+x_{21}^2+\cdots+x_{t1}^2}{t} &amp; \frac{x_{11}x_{12}+x_{21}x_{22}+\cdots+x_{t1}x_{t2}}{t} &amp; \cdots \\\
    \frac{x_{11}x_{12}+x_{21}x_{22}+\cdots+x_{t1}x_{t2}}{t} &amp; \frac{x_{12}^2+x_{22}^2+\cdots+x_{t2}^2}{t} &amp; \cdots \\\
    \vdots &amp; \vdots &amp; \ddots
    \end{bmatrix}
  $$
  Because $\mathbf{x}$ has been centered, meaning that $E(x_i)^2 = 0$, thus $\sigma_{x_i}^2 = E(x_i^2)$. And also that $\sigma_{x_i,x_j} = E(x_ix_j)$. We have $\frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T)$ equals to the covariance matrix $\mathbf{\Sigma_x}$:
  $$
    \frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T) = \mathbf{\Sigma_x} = \begin{bmatrix}
    \sigma_{1}^2 &amp; \sigma_{12} &amp; \cdots &amp; \cdots \\\
    \sigma_{12} &amp; \sigma_{2}^2 &amp; \cdots &amp; \cdots \\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\
    \cdots &amp; \cdots &amp; \cdots &amp; \sigma_{tn}^2
    \end{bmatrix}
  $$
- **Setp 4**
  Now, want to find a new axis that have a vectors which is at the direction that give is largest variance of the data. This new axis is then the PCA axes, or say the eigenvector of PC1. This new axis, is often made of linear combinations of wights $w_i$ $i = 1-&gt;n$ (where $n$ is the number of attributes). These weights will give a maximum $E(y_1^2)$ where 
  $$
    y_{1j} = w_{11}x_{j1} + w_{21}x_{j2} + \cdots + w_{n1}x_{jn}.
  $$
  where $j=1-&gt;t$, for data points. Note that $E(y_1)=0$, so $var(y_1)$ maximized. Now set:
  $$
    \vec{w_1} = \begin{bmatrix}
    w_{11}  \\\
    w_{21}  \\\
    \vdots  \\\
    w_{n1} 
    \end{bmatrix}
  $$ 
  and 
  $$
    \vec{w_1}^T = \begin{bmatrix}
    w_{11}  &amp; w_{21} &amp; \cdots &amp; w_{n1} 
    \end{bmatrix}
  $$ 
  we get
  $$
    \vec{y_1}^T = \vec{w_1}^T \cdot \mathbf{x}
  $$
  Therefore, we get
  $$
    E(y_1^2) = \frac{1}{t}E(\vec{y_1}^T \cdot \vec{y_1}) 
              = \frac{1}{t}E(\vec{w_1}^T \cdot \mathbf{x} \cdot \mathbf{x}^T \cdot \vec{w_1} ) 
              = \vec{w_1}^T E\left( \frac{\mathbf{x} \cdot \mathbf{x}^T }{t} \right)  \vec{w_1}
              = \vec{w_1}^T \mathbf{\Sigma_x}  \vec{w_1}
  $$
  Then we just need to maximize $\vec{w_1}^T \mathbf{\Sigma_x}  \vec{w_1}$ with the constrain of $\vec{w_1}^T \cdot \vec{w_1} = 1$. This involves Lagrange multipliers which I will skip here (for now...). In the end one will get 
  $$
    (\Sigma_x - \lambda_1 I) \vec{w_1} = 0
  $$
  So, $w_1$ is an eigenvector of $\Sigma_x$, and that $\lambda_1$ is the eigenvalue. Want to maximize $\vec{w_1}^T \mathbf{\Sigma_x} \vec{w_1} = \vec{w_1}^T \lambda_1 \vec{w_1} = \lambda_1$. The first PCA component, PC1, is the one with the largest $\lambda$.

--- --&gt;
&lt;h2 id=&#34;principal-component-analysis-pca&#34;&gt;Principal Component Analysis (PCA)&lt;/h2&gt;
&lt;h3 id=&#34;procedure&#34;&gt;Procedure&lt;/h3&gt;
&lt;p&gt;PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It&amp;rsquo;s especially powerful when dealing with high-dimensional data. Here&amp;rsquo;s how it works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 1: Create the Data Matrix&lt;/strong&gt;:
Construct matrix $\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person).
$$
\mathbf{d} = \begin{bmatrix}
d_{11} &amp;amp; d_{21} &amp;amp; \cdots &amp;amp; d_{t1} \\
d_{12} &amp;amp; d_{22} &amp;amp; \cdots &amp;amp; d_{t2} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
d_{1n} &amp;amp; d_{2n} &amp;amp; \cdots &amp;amp; d_{tn}
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 2: Center the Data&lt;/strong&gt;:
Subtract the mean of each attribute from the corresponding values to center the data around the origin. This helps in aligning the PCA with the directions of maximum variance.
$$
\mathbf{x} = \mathbf{d} - \mathbf{\langle\beta\rangle}
$$
where $\mathbf{\langle\beta_j\rangle} = \frac{1}{t} \sum_{j=1}^t d_{ji}$ represents the mean of each attribute across all data points. The centered data matrix $\mathbf{x}$ looks like this:
$$
\mathbf{x} = \begin{bmatrix}
x_{11} &amp;amp; x_{21} &amp;amp; \cdots &amp;amp; x_{t1} \\
x_{12} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{t2} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
x_{1n} &amp;amp; x_{2n} &amp;amp; \cdots &amp;amp; x_{tn}
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 3: Construct the Covariance Matrix&lt;/strong&gt;:
The covariance matrix $\mathbf{\Sigma_x}$ is constructed from $\mathbf{x}$. Since $\mathbf{x}$ is centered, its expected value matrix $\mathbf{E(x)}$ is zero. The covariance matrix is then:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{\Sigma_x} = \frac{1}{t} \mathbf{x} \mathbf{x}^T
$$&lt;/p&gt;
&lt;p&gt;This matrix captures the variance shared between the attributes, and its diagonal elements represent the variance of each attribute.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 4: Eigenvalue Decomposition&lt;/strong&gt;:
Eigenvalues and eigenvectors of the covariance matrix $\mathbf{\Sigma_x}$ are computed. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues represent the magnitude of the variance along each principal component.&lt;/p&gt;
&lt;p&gt;To find the principal components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Maximize $\vec{w}^T \mathbf{\Sigma_x} \vec{w}$ subject to $\vec{w}^T \vec{w} = 1$.&lt;/li&gt;
&lt;li&gt;This leads to solving $(\mathbf{\Sigma_x} - \lambda I) \vec{w} = 0$, where $\lambda$ is the eigenvalue.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The principal component associated with the largest eigenvalue ($\lambda_1$) captures the most variance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 5: Transform Data to PCA Space&lt;/strong&gt;
Each data point is then projected onto the PCA space using the eigenvectors, effectively reducing dimensionality while retaining the most significant variance features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step 6: Interpretation&lt;/strong&gt;
Interpret the physical meaning of each principal component, which might involve understanding how original attributes combine to form the component.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;independent-component-analysis-ica&#34;&gt;Independent Component Analysis (ICA)&lt;/h2&gt;
&lt;p&gt;Independent Component Analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents that are maximally independent. This technique is particularly useful in the field of blind source separation, where the goal is to separate a mixture of signals (like audio tracks) into their individual, independent components without prior knowledge about the source signals.&lt;/p&gt;
&lt;h3 id=&#34;understanding-the-context&#34;&gt;Understanding the Context&lt;/h3&gt;
&lt;p&gt;In applications like audio processing, ICA can be used to separate different speaking voices from a single audio track recorded with multiple voices overlapping. Each recording may capture the same set of voices but with varying amplitudes depending on the distance and orientation of each voice relative to the microphone. Unlike Principal Component Analysis (PCA), which seeks directions of maximum variance and might mix sources further, ICA focuses on maximizing statistical independence between the components. This characteristic makes ICA suitable for tasks where the &lt;em&gt;underlying sources are non-Gaussian and independent.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;procedure-for-applying-ica&#34;&gt;Procedure for Applying ICA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Preprocessing with PCA&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dimensionality Reduction&lt;/strong&gt;: Initially, PCA is applied to reduce the dimensionality of the data. This step is crucial because it removes noise and reduces the complexity of the data, which simplifies the subsequent ICA.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Whitening&lt;/strong&gt;: The data is transformed into components that are uncorrelated and have unit variance. This transformation, often performed as part of PCA, is also known as &amp;ldquo;whitening&amp;rdquo;. It ensures that the ICA algorithm focuses only on finding the independent sources without being misled by possible correlations in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Applying ICA&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Identify Independent Components (ICs)&lt;/strong&gt;: After whitening, the ICA algorithm seeks to rotate the whitened data to a new coordinate system where the statistical independence of the resulting signals is maximized. This involves optimizing an objective function that measures non-Gaussianity (since independence implies non-Gaussianity under certain conditions, see below section on Non-Gaussianity).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extraction of Sources&lt;/strong&gt;: The independent components correspond to the original signals/sources that were mixed in the observed data. Each component should represent one source, such as a single voice in the context of audio processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;example-application&#34;&gt;Example Application&lt;/h3&gt;
&lt;p&gt;For instance, if you have a recording from a busy restaurant, ICA can help isolate single voices from the background noise and other voices. This technique is invaluable in environments where multiple sources overlap significantly but retain their independence in terms of their statistical signatures.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;understanding-independence-non-gaussianity-and-the-central-limit-theorem&#34;&gt;Understanding Independence, Non-Gaussianity, and the Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) states that the sum of a large number of independent random variables, each with finite mean and variance, tends toward a Gaussian distribution, regardless of the original distributions of the variables. This principle is critical in the context of Independent Component Analysis (ICA), which is used to separate mixed signals into their original, independent components.&lt;/p&gt;
&lt;h3 id=&#34;implications-for-ica&#34;&gt;Implications for ICA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Role of the CLT&lt;/strong&gt;: The CLT implies that a mixture of multiple independent, non-Gaussian signals tends to be &lt;em&gt;more Gaussian-like&lt;/em&gt; than any of the individual signals. ICA utilizes this property by assuming that the original sources are non-Gaussian. The mixed signal, being more Gaussian, provides a clue that separation is possible by identifying and maximizing the non-Gaussian aspects of its components.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-Gaussianity as a Tool&lt;/strong&gt;: ICA algorithms focus on maximizing non-Gaussianity to separate the independent components. This is because non-Gaussian signals, when summed, lose some of their distinct statistical features, making the mixture more Gaussian. By finding projections of the data that are maximally non-Gaussian, ICA can effectively identify and separate the original independent sources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Challenge with Gaussian Sources&lt;/strong&gt;: If the original sources were Gaussian, their sum would also be Gaussian, offering no statistical advantage for separation. This is why ICA is particularly powerful for non-Gaussian data sources, where it can exploit statistical differences to distinguish between the sources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practical-considerations&#34;&gt;Practical Considerations&lt;/h3&gt;
&lt;p&gt;ICA is particularly effective in scenarios where the underlying sources of a signal are known to be non-Gaussian, such as in audio signal processing or in biomedical signal analysis. The ability to reverse the effects of the CLT and uncover the original signals from a complex mixture is what makes ICA a valuable tool in modern data analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustering and Classification</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/6-clustering-and-classification/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/6-clustering-and-classification/</guid>
      <description>&lt;p&gt;Clustering and classification are fundamental techniques in data analysis and machine learning, used to group data points based on similarities and to categorize them into distinct classes.&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-clustering&#34;&gt;Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Hierarchical clustering builds nested clusters by progressively merging or splitting them based on the distance between data points or groups. The method relies heavily on the choice of distance calculation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single Linkage (Nearest Neighbor)&lt;/strong&gt;: This method, also known as the friend-of-a-friend technique, considers the shortest distance between points in two clusters (see the first plot below). It can result in elongated, &amp;ldquo;chain-like&amp;rdquo; clusters that capture local structure but might miss broader data groupings (see the second plot below).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete Linkage&lt;/strong&gt;: Use the maximum distance between points in two clusters. This method tends to produce more compact and well-separated clusters, reducing the chain effect seen in single linkage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average Linkage&lt;/strong&gt;: Calculates the average distance between all pairs of points in two clusters. This method provides a balance between the sensitivity of single linkage and the strictness of complete linkage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ward&amp;rsquo;s Minimum Variance&lt;/strong&gt;: Minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum increase in total variance are merged. This method tends to create more regular-sized clusters (e.g., spherical or ellipsoidal), which can be advantageous for certain datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Pamela-Guevara/publication/281014334/figure/fig57/AS:418517879934980@1476793847581/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and.png&#34; alt=&#34;&#34;&gt;
(Figure credit: &lt;a href=&#34;https://www.researchgate.net/figure/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and_fig57_281014334&#34;&gt;https://www.researchgate.net/figure/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and_fig57_281014334&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Once distances are calculated, the hierarchical clustering algorithm uses these distances to merge or split clusters:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: Start by assigning each data point to its own cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Merge Step&lt;/strong&gt;: At each step, merge the two clusters that are closest together, based on the distance calculation method chosen.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update Distances&lt;/strong&gt;: Recalculate the distances between the new cluster and each of the old clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt;: Continue merging clusters until all data points are merged into a single cluster or until a desired number of clusters is reached.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These different linkage criteria can significantly impact the shapes and sizes of the clusters formed. A nice demonstration in various clustering scenarios can be found on the scikit-learn page, also shown below:
&lt;img src=&#34;https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png&#34; alt=&#34;&#34;&gt;
(Figure credit: &lt;a href=&#34;https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py&#34;&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;k-means-clustering&#34;&gt;k-Means Clustering&lt;/h3&gt;
&lt;p&gt;k-Means clustering partitions the data into $k$ mutually exclusive clusters, and returns the index of the cluster each point belongs to. This method aims to minimize the within-cluster sum of squares.&lt;/p&gt;
&lt;h3 id=&#34;density-based-spatial-clustering-of-applications-with-noise-dbscan&#34;&gt;Density-Based Spatial Clustering of Applications with Noise (DBSCAN)&lt;/h3&gt;
&lt;p&gt;DBSCAN groups together closely packed points and marks points in low-density regions as outliers. This method does not require specifying the number of clusters a priori, making it suitable for data with irregular or complex structures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;$\mu$: Minimum number of points required to form a dense region.&lt;/li&gt;
&lt;li&gt;$\varepsilon$: Specifies the &amp;ldquo;reach&amp;rdquo;, that is, the distance threshold within which points are considered neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;k-nearest-neighbor-k-nn&#34;&gt;k-Nearest Neighbor (k-NN)&lt;/h3&gt;
&lt;p&gt;k-Nearest Neighbors (k-NN) is primarily a classification technique renowned for its simplicity and effectiveness, especially suited for datasets where the decision boundaries between classes are not linear. The k-NN algorithm classifies new data points based on the majority vote of their k nearest neighbors in the feature space.&lt;/p&gt;
&lt;p&gt;To apply k-NN effectively, data is typically split into two sets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training Set&lt;/strong&gt;: This dataset is used to &amp;rsquo;train&amp;rsquo; or &amp;lsquo;fit&amp;rsquo; the model. It includes both the input features and the corresponding classification labels which are known.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test Set&lt;/strong&gt;: This dataset is used solely for testing the performance of the trained model. It helps to evaluate how well the k-NN model generalizes to new, previously unseen data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the model has been trained on the training set, it can be used to predict the class labels of new data in the test set, providing a measure of its classification accuracy.&lt;/p&gt;
&lt;!-- ## Summary Table for Clustering and Classification Methods

| **Method**           | **Characteristics**                                       | **Best Used For**                                       |
|----------------------|-----------------------------------------------------------|---------------------------------------------------------|
| **Hierarchical Clustering** | Builds clusters based on various distance calculations between points. | Data with inherent hierarchical structure and when a visual representation of cluster formation is beneficial. |
| **k-Means Clustering**      | Partitions data into k predefined clusters by minimizing within-cluster variances. | Large datasets with well-separated clusters, where the number of clusters is known a priori. |
| **DBSCAN**                  | Groups densely packed points and identifies points in low-density areas as outliers. Does not require predefined cluster number. | Complex datasets with noise and irregular cluster shapes, not well-suited to global clustering criteria. |
| **k-Nearest Neighbors (k-NN)** | Classifies data based on the majority label among the nearest k neighbors. Utilizes training and test datasets to ensure model accuracy and generalization. | Classification tasks, especially in cases with non-linear decision boundaries and when model simplicity and interpretability are important. | --&gt;
</description>
    </item>
    
    <item>
      <title>Nondetections-Censored and Truncated Data</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/7-censored-and-truncated-data/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/7-censored-and-truncated-data/</guid>
      <description>&lt;p&gt;In the real world, data collection is often incomplete or constrained by various factors. To fully utilize available data, it is sometimes necessary to handle &amp;lsquo;censored&amp;rsquo; and &amp;rsquo;truncated&amp;rsquo; data, particularly in fields such as medical studies, reliability engineering, and astronomy.&lt;/p&gt;
&lt;h3 id=&#34;censored-data&#34;&gt;Censored Data&lt;/h3&gt;
&lt;p&gt;Censored data occurs when the value of a measurement exists, but we only know that it falls above or below certain limits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Left-Censored Data (Upper Limit)&lt;/strong&gt;: The actual data point is less than a certain value, but the exact value is unknown. This is common in astronomy; for example, a star&amp;rsquo;s luminosity might be below the detection limit of a telescope. We know only the upper limit of the star&amp;rsquo;s luminosity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right-Censored Data (Lower Limit)&lt;/strong&gt;: The actual data point is greater than a certain value, but the exact value is unknown.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;truncated-data&#34;&gt;Truncated Data&lt;/h3&gt;
&lt;p&gt;Truncated data occurs when data points below or above a certain threshold are not just unknown but completely absent from the dataset. Unlike censoring, with truncation, we do not have any information that data points exist outside of the observed range.&lt;/p&gt;
&lt;p&gt;These characteristics have implications for statistical analysis, including distribution function estimation, correlation analysis, regression modeling, and hypothesis testing.&lt;/p&gt;
&lt;h3 id=&#34;survival-and-hazard-functions-in-censoring&#34;&gt;Survival and Hazard Functions in Censoring&lt;/h3&gt;
&lt;p&gt;Censoring techniques often use concepts from survival analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Survival Function ($S(x)$)&lt;/strong&gt;: Represents the probability that a variable $X$ exceeds a certain value $x$.
$$
S(x) = P(X &amp;gt; x) = 1 - F(x)
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hazard Rate ($h(x)$)&lt;/strong&gt;: Represents the conditional failure rate at a certain value.
$$
h(x) = \frac{f(x)}{S(x)}
$$
Here, $f(x)$ is the PDF at $x$, and the hazard rate can be interpreted as the likelihood of an event occurring at $x$ given that it has not occurred before $x$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clarifying-the-example-on-hazard-rate&#34;&gt;Clarifying the Example on Hazard Rate&lt;/h3&gt;
&lt;p&gt;Consider an example where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f(95) = 1\%$: The probability of dying at the age of 95 is 1%.&lt;/li&gt;
&lt;li&gt;$S(95) = 2\%$: The probability of surviving past the age of 95 is 2%.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To find the hazard rate $h(x)$ at age 95:
$$
h(95) = \frac{0.01}{0.02} = 0.5 \text{ or } 50\%
$$
This indicates that, given reaching age 95, there is a 50% chance of dying at that age.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;advanced-estimators-for-censored-and-truncated-data&#34;&gt;Advanced Estimators for Censored and Truncated Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kaplan-Meier Nonparametric Estimator (Censored)&lt;/strong&gt;: The Kaplan-Meier estimator is crucial for analyzing survival data, particularly in medical research. It measures the fraction of subjects living for a certain amount of time after treatment. This estimator is particularly effective in handling right-censored data, where the survival time is only known to exceed a certain duration but the exact time of event (e.g., death, failure) is unknown. The Kaplan-Meier estimator uses the available data to estimate the survival function, $S(t)$, which provides insights into the likelihood of survival beyond observed time points.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below is an example plot of the KM estimated survival curve of synthetic data on stellar luminosity data.
&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/KM.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Sanya Arora&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lynden-Bell–Woodroofe Estimator (Truncated)&lt;/strong&gt;: Commonly utilized in astronomical studies, the Lynden-Bell–Woodroofe estimator addresses issues with truncated samples, where observations below or above certain thresholds are missing entirely from the dataset. This estimator operates under the assumption that all observations derive from the same underlying distribution. It uses the observed distribution of the available data to estimate the distribution functions of the truncated segments, facilitating a more comprehensive understanding of the overall data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both estimators are tailored to specific types of incomplete data: Kaplan-Meier for censored data, where some information about the survival time is available, and Lynden-Bell–Woodroofe for truncated data, where parts of the data are completely missing. Understanding their applications and differences is essential for accurately analyzing datasets characterized by incomplete observations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timeseries Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/8-timeseries-analysis/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/8-timeseries-analysis/</guid>
      <description>&lt;h2 id=&#34;evenly-spaced-time-series-data&#34;&gt;Evenly Spaced Time Series Data&lt;/h2&gt;
&lt;h3 id=&#34;autocorrelation-function-acf&#34;&gt;Autocorrelation Function (ACF)&lt;/h3&gt;
&lt;p&gt;The Autocorrelation Function (ACF) measures the correlation between a time series and a lagged version of itself over various time intervals. It&amp;rsquo;s akin to performing a Pearson correlation test between the original time series data and the same data shifted by a lag time $k$. Notably, ACF(k=0) always equals 1, reflecting perfect self-correlation at zero lag.&lt;/p&gt;
&lt;h3 id=&#34;partial-autocorrelation-function-pacf&#34;&gt;Partial Autocorrelation Function (PACF)&lt;/h3&gt;
&lt;p&gt;The Partial Autocorrelation Function (PACF) quantifies the correlation between observations in a time series separated by $k$ time units, specifically adjusting for the correlations at shorter lags. This adjustment helps isolate the direct influence of past data points on the current observation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At lag 1, PACF equals ACF as there are no previous terms to adjust for.&lt;/li&gt;
&lt;li&gt;At lag 2, PACF assesses the correlation between points two units apart (e.g., $X_i$ - $X_{i+2}$), adjusting for the influence of the intervening lag (e.g., corelation between $X_i$ - $X_{i+1}$ and $X_{i+1}$ - $X_{i+2}$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PACF is crucial for model building, especially in autoregressive models, as it helps identify the effective number of past observations (lags) that significantly influence future values.&lt;/p&gt;
&lt;h3 id=&#34;autoregressive-ar-model&#34;&gt;Autoregressive (AR) Model&lt;/h3&gt;
&lt;p&gt;In an AR model, each point in the series is modeled as a linear combination of its previous values:
$$
X_t = \alpha_1 X_{t-1} + \alpha_2 X_{t-2} + \cdots + \alpha_p X_{t-p} + \varepsilon_t
$$
where $\alpha_i$ are coefficients to be estimated and $\varepsilon_t$ is white noise.&lt;/p&gt;
&lt;h3 id=&#34;moving-average-ma-model&#34;&gt;Moving Average (MA) Model&lt;/h3&gt;
&lt;p&gt;An MA model expresses each point in the series as a linear combination of past noise terms:
$$
X_t = \varepsilon_t + \beta_1 \varepsilon_{t-1} + \beta_2 \varepsilon_{t-2} + \cdots + \beta_q \varepsilon_{t-q}
$$
where $\varepsilon_t = N(0, \sigma^2)$ and $\beta_i$ are coefficients to be estimated.&lt;/p&gt;
&lt;h3 id=&#34;autoregressive-moving-average-arma-model&#34;&gt;Autoregressive Moving Average (ARMA) Model&lt;/h3&gt;
&lt;p&gt;The ARMA $(p,q)$ model effectively combines autoregressive (AR) and moving average (MA) components:
$$
X_t = \alpha_1 X_{t-1} + \cdots + \alpha_p X_{t-p} + \varepsilon_t + \beta_1 \varepsilon_{t-1} + \cdots + \beta_q \varepsilon_{t-q}
$$
In this model, the parameters $p$ and $q$ denote the orders of the AR and MA components, respectively. These parameters are crucial as they determine the &amp;ldquo;memory&amp;rdquo; length of the model, indicating how far back in time the data&amp;rsquo;s dependencies and shocks influence the current observation.&lt;/p&gt;
&lt;h3 id=&#34;autoregressive-integrated-moving-average-arima-model&#34;&gt;Autoregressive Integrated Moving Average (ARIMA) Model&lt;/h3&gt;
&lt;p&gt;Building on the ARMA model, the ARIMA $(p,d,q)$ model incorporates an additional differencing component to render non-stationary time series data stationary. This adjustment is necessary for dealing with underlying trends or seasonality:&lt;/p&gt;
&lt;!-- $$
\text{Differenced Series: } Y_t = (1 - B)^d X_t
$$
$$
Y_t = \alpha_1 Y_{t-1} + \cdots + \alpha_p Y_{t-p} + \varepsilon_t + \beta_1 \varepsilon_{t-1} + \cdots + \beta_q \varepsilon_{t-q}
$$ --&gt;
&lt;p&gt;In ARIMA, $d$ represents the degree of differencing required to flatten trends in the data, addressing long-term drift and ensuring stationarity. The parameters $p$ and $q$ continue to represent the AR and MA orders, respectively. ARIMA models are extensively used not only in modeling but also in forecasting scenarios, such as predicting financial market trends or stock prices.&lt;/p&gt;
&lt;h3 id=&#34;fourier-power-spectrum&#34;&gt;Fourier Power Spectrum&lt;/h3&gt;
&lt;p&gt;Fourier analysis converts time series data into the frequency domain, providing insights into periodicity and cyclic patterns within the series.&lt;/p&gt;
&lt;h2 id=&#34;unevenly-spaced-time-series-data&#34;&gt;Unevenly Spaced Time Series Data&lt;/h2&gt;
&lt;h3 id=&#34;lomb-scargle-periodogram-lsp&#34;&gt;Lomb-Scargle Periodogram (LSP)&lt;/h3&gt;
&lt;p&gt;The LSP method extends Fourier analysis to unevenly spaced time series data, facilitating the detection of periodic signals. It also has another form that use leat-squares regression of the dataset to sine and cosine waves in a range of frequencies. A common form is:
$$
P_{LS}(\nu) = \frac{1}{2\sigma^2} \left[ \frac{\left(\sum_{i=1}^n X_i \cos(2\pi \nu t_i)\right)^2}{\sum_{i=1}^n \cos^2(2\pi \nu (t_i - \tau(\nu)))} + \frac{\left(\sum_{i=1}^n X_i \sin(2\pi \nu t_i)\right)^2}{\sum_{i=1}^n \sin^2(2\pi \nu (t_i - \tau(\nu)))} \right]
$$
where $\tau$ is defined as
$$
\tan(4\pi \nu \tau) = \frac{\sum_{i=1}^n \sin(4\pi \nu t_i)}{\sum_{i=1}^n \cos(4\pi \nu t_i)}
$$&lt;/p&gt;
&lt;h2 id=&#34;other-analysis-techniques&#34;&gt;Other Analysis Techniques&lt;/h2&gt;
&lt;h3 id=&#34;wavelet-analysis&#34;&gt;Wavelet Analysis&lt;/h3&gt;
&lt;p&gt;Fourier Transform (FT) is a powerful tool for analyzing frequency components within a time series. However, a key limitation of a single FT is its lack of temporal resolution: it provides frequency information ($\nu$) but no insight into when these frequencies occur, especially if the data is non-stationary—meaning the frequency content changes over time.&lt;/p&gt;
&lt;h4 id=&#34;problem-with-standard-fourier-transform&#34;&gt;Problem with Standard Fourier Transform&lt;/h4&gt;
&lt;p&gt;For instance, if a time series exhibits seasonal variations, the frequency of these variations might change over different time intervals. A simple FT would average these changes across the entire dataset, potentially obscuring meaningful patterns.&lt;/p&gt;
&lt;h4 id=&#34;solution-via-wavelet-analysis&#34;&gt;Solution via Wavelet Analysis&lt;/h4&gt;
&lt;p&gt;Wavelet analysis addresses this limitation by allowing the examination of different frequencies at different times. This is achieved by dividing the time series into segments and analyzing each segment separately—a process that optimizes the extraction of temporal information at various frequencies.&lt;/p&gt;
&lt;h4 id=&#34;visualizing-wavelet-transforms&#34;&gt;Visualizing Wavelet Transforms&lt;/h4&gt;
&lt;p&gt;A wavelet plot typically displays frequency space ($\nu$) versus time ($t$). It offers a detailed view where, at shorter periods on the y-axis, there is higher time resolution, and conversely, longer periods offer broader frequency insights but lower time specificity. The parabolic shapes in the plot, often shaded, mark the boundary edge effects, indicating where data becomes less reliable due to edge artifacts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Victor-Manuel-Velasco-Herrera/publication/261322980/figure/fig2/AS:296841208975361@1447783867682/Wavelet-transform-analysis-of-the-newly-proposed-solar-activity-proxy-nitrate.png&#34; alt=&#34;Wavelet Transform Analysis Example&#34;&gt;
(Credit: &lt;a href=&#34;https://www.researchgate.net/figure/Wavelet-transform-analysis-of-the-newly-proposed-solar-activity-proxy-nitrate_fig2_261322980&#34;&gt;ResearchGate&lt;/a&gt;)&lt;/p&gt;
&lt;h4 id=&#34;wavelets-versus-fourier-transforms&#34;&gt;Wavelets Versus Fourier Transforms&lt;/h4&gt;
&lt;p&gt;Instead of using Fourier transforms, wavelet analysis involves convoluting the time-sliced data with a wavelet function. This method is more effective because it adjusts to the localized variations in the time series through the use of variable-sized &amp;lsquo;windows&amp;rsquo;—smaller windows for higher frequencies and larger ones for lower frequencies.&lt;/p&gt;
&lt;h4 id=&#34;common-wavelets&#34;&gt;Common Wavelets&lt;/h4&gt;
&lt;p&gt;Commonly used wavelet functions include the Morlet and Daubechies wavelets, each with specific characteristics that make them suitable for different types of data analysis tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Steven-Vandeput/publication/267403305/figure/fig15/AS:654064610197505@1532952563244/Illustration-of-several-types-of-mother-wavelet-functions-Morlet-wavelet-top-left.png&#34; alt=&#34;Types of Mother Wavelet Functions&#34;&gt;
(Credit: &lt;a href=&#34;https://www.researchgate.net/figure/Illustration-of-several-types-of-mother-wavelet-functions-Morlet-wavelet-top-left_fig15_267403305&#34;&gt;ResearchGate&lt;/a&gt;)&lt;/p&gt;
&lt;h4 id=&#34;mathematical-expression&#34;&gt;Mathematical Expression&lt;/h4&gt;
&lt;p&gt;The mathematical expression for a wavelet transform is given by:
$$
W[f(\lambda,t)] = \int_{-\infty}^{\infty} f(\nu) \frac{1}{\sqrt{\lambda}} \overline{\phi}\left(\frac{u-t}{\lambda}\right) du
$$
where $\lambda$ is the scaling factor that stretches or compresses the wavelet, adapting it to the signal&amp;rsquo;s local characteristics.&lt;/p&gt;
&lt;p&gt;Wavelet analysis thus provides a more flexible and nuanced approach to understanding time series data, especially when the signal contains non-stationary or frequency-varying components.&lt;/p&gt;
&lt;h3 id=&#34;nyquist-frequency&#34;&gt;Nyquist Frequency&lt;/h3&gt;
&lt;p&gt;The Nyquist frequency $\nu_N = 1/(2 \Delta t)$ is the maximum frequency that can be effectively captured by the data, where $\Delta t$ is the sampling interval. It represents the boundary beyond which the sampling rate is insufficient to capture detailed variations in the data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variogram</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-1-variogram/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-1-variogram/</guid>
      <description>&lt;h2 id=&#34;introduction-to-variograms&#34;&gt;Introduction to Variograms&lt;/h2&gt;
&lt;p&gt;A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: &amp;ldquo;nugget,&amp;rdquo; &amp;ldquo;sill,&amp;rdquo; and &amp;ldquo;range.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nugget:&lt;/strong&gt; Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sill:&lt;/strong&gt; The plateau reached by the variogram, beyond which the increments in distance do not significantly increase the variance. It represents the level of total variance within the dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; The distance at which the variogram reaches the sill, beyond which locations are no longer correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-variogram&#34;&gt;Example of Spatial Data and Variogram&lt;/h2&gt;
&lt;p&gt;Plots below show an example of spatial data (left) and its associated variogram (right). The plot on the left shows synthetic data in spatial X-Y coordinates color-coded by the level of toxicity measured at that location. The size of the circle is associated with the measurement error, which is not used here. The variogram on the right shows how the semi-variance between points increases with distance. It features a &amp;ldquo;nugget&amp;rdquo; effect at the origin, indicating measurement noise or microscale variability. The curve approaches a &amp;ldquo;sill,&amp;rdquo; beyond which the variance stabilizes, suggesting that points beyond this &amp;ldquo;range&amp;rdquo; do not influence each other. This range is critical for understanding the spatial continuity and predicting values at unsampled locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/variogram.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-variogram&#34;&gt;Example R Code for the Variogram&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(gstat)
library(sp)

# Defining spatial coordinates
# df_2 is a dataframe with colums x and y 
coordinates(df_2) &amp;lt;- ~x+y  

# Creating Variograms
variogram_tox &amp;lt;- variogram(toxicity ~ 1, df_2)

# Fit the variogram and plot it out.
# gamma: the semi-variance
# vgm: &amp;quot;variogram model,&amp;quot; 
model_tox &amp;lt;- fit.variogram(
  variogram_tox, model = vgm(psill = max(variogram_tox$gamma), 
  model = &amp;quot;Sph&amp;quot;, 
  range = 30))

# Plot the empirical variogram and the fitted model
plot(variogram_tox, model = model_tox, 
  main = &amp;quot;Toxicity Variogram with Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kriging</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-2-kriging/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-2-kriging/</guid>
      <description>&lt;h2 id=&#34;introduction-to-kriging&#34;&gt;Introduction to Kriging&lt;/h2&gt;
&lt;p&gt;Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Kriging:&lt;/strong&gt; Assumes the mean of the random field is known and constant throughout the region of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ordinary Kriging:&lt;/strong&gt; Assumes the &lt;em&gt;mean is unknown&lt;/em&gt; but constant within the region of interest and is the most commonly used form as one do not know the mean in real world.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-on-variogram&#34;&gt;More on Variogram&lt;/h2&gt;
&lt;p&gt;The semivariance $\gamma(x_1, x_2)$ between two points can be expressed as:
$$
\gamma(d) = \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2
$$
where $n$ is the number of pairs, $d$ is the distance between two points, and $z$ represents the values at the locations. The calculation of $\gamma(d)$ involves several key assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stationary (homogeneous):&lt;/strong&gt; Assumes that the statistical properties (mean, variance) of the process do not change over space. This implies that the mean and variance are constant throughout the region of interest, and the covariance between any two points depends only on the distance and direction between them, not on their absolute locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isotropy:&lt;/strong&gt; Assumes that the statistical properties are the same in all directions. This means that the variogram is a function only of the distance between sample points, not of the direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Kriging, we use $\gamma$ to weight the data for interpolation:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\gamma(d) &amp;amp;= \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 \\
&amp;amp;= \frac{1}{2} E\left(\left[z(x+d) - z(x)\right]^2\right)
\text{(homogeneous assumption)} \\
&amp;amp;= \frac{1}{2} \left\{ E\left[ z^2(x+d) \right] + E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\} \\
&amp;amp;= \frac{1}{2} \left\{ 2E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\}
\text{(homogeneous assumption)} \\
&amp;amp;= \sigma_x^2 + \mu_x^2 - \text{cov}[z(x), z(x+d)] - \mu_x^2
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the relationships: $E[x^2] = \sigma_x^2 + [E(x)]^2$ and $\text{cov}[X,Y] = E(XY) - E(X)E(Y)$ were used to get the second to last equation. We have:
$$\begin{align}
\gamma(d) &amp;amp;= \sigma^2 - \text{cov}[z(x), z(x+d)] \\
&amp;amp;= \sigma^2 - \text{c}(d)
\end{align}$$&lt;/p&gt;
&lt;p&gt;Therefore, we have the semivariance at distance $d$ is the variance minus the covariance between points at this distance.&lt;/p&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-the-kriging-result&#34;&gt;Example of Spatial Data and the Kriging Result&lt;/h2&gt;
&lt;p&gt;The plots below show an example of spatial data (left) and its associated Kriging map (right). The data used here is the same as in the Variogram page, and the Kriging map uses the variogram model shown on the Variogram page to predict values at unsampled locations. This example demonstrates how Kriging utilizes the spatial structure of the data, as defined by the variogram, to provide a statistically optimal interpolation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/kriging.png&#34; alt=&#34;kriging&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-kriging&#34;&gt;Example R Code for the Kriging&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Fit a variogram model
vgm_model &amp;lt;- vgm(psill = max(variogram_tox$gamma), 
       model = &amp;quot;Sph&amp;quot;, range = 30)
       
x.range &amp;lt;- range(df_2$x)
y.range &amp;lt;- range(df_2$y)

grid.points &amp;lt;- expand.grid(
       x = seq(from = x.range[1], to = x.range[2], by = 1),
       y = seq(from = y.range[1], to = y.range[2], by = 1))

# Convert to SpatialPoints
grid &amp;lt;- SpatialPoints(grid.points)

# Perform ordinary kriging
kriged &amp;lt;- krige(toxicity ~ 1, df_2, model = vgm_model, newdata = grid)

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res=200)
# Create a map with overlay contours
spplot(kriged, &amp;quot;var1.pred&amp;quot;, main = &amp;quot;Kriging Map for Toxicity&amp;quot;, 
       xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;,
       sp.layout = list(&amp;quot;sp.points&amp;quot;, df_2, col = &amp;quot;green&amp;quot;),
       colorkey = TRUE,
       scales = list(draw = TRUE))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://shihyuntang.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring the Spot Variability of T Tauri Stars Using Near-IR Atomic Fe and Molecular OH Lines</title>
      <link>https://shihyuntang.github.io/publication/202406_ewr_obs_apj/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202406_ewr_obs_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202406_EWR_obs_apj.png&#34; alt=&#34;&#34;&gt;
&lt;em&gt;Image credit: Figure 7 in Tang et al. 2024.&lt;/em&gt;: The empirical Teff vs. equivalent width ratio plot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Binary Star Evolution in Different Environments: Filamentary, Fractal, Halo and Tidal-tail Clusters</title>
      <link>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202307_binaryetar_evo_env_aj.png&#34; alt=&#34;&#34;&gt;
&lt;em&gt;Image credit: Figure 5 in Pang et al. 2023.&lt;/em&gt;: Box plot showing radial binary fraction as a function of cluster-centric distance as half-mass radius (rh). For each panel shows four types of clusters: (a) filamentary , (b) fractal, (c) halo , and (d) tidal-tail.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Star-Crossed Lovers DI Tau A and B: Orbit Characterization and Physical Properties Determination</title>
      <link>https://shihyuntang.github.io/publication/202303_ditau_apj/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202303_ditau_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202303_ditau_apj.png&#34; alt=&#34;orbit&#34;&gt;
&lt;em&gt;Image credit: Figure 2 in Tang et al. 2023.&lt;/em&gt;: Best model fit for DITau AB&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical Origin for the Collinder 132-Gulliver 21 Stream: A Mixture of three Co-Moving Populations with an Age Difference of 250 Myr</title>
      <link>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202209_col132gul21_apjl/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202209_col132gul21_apjl.png&#34; alt=&#34;3d&#34;&gt;
&lt;em&gt;Image credit: Figure 3 in Pang et al. 2022.&lt;/em&gt;: 3D morphology of the Collinder 132-Gulliver 21 stream. The oldest population surrounded by young populations can be seen in panel (b) with age of each group labeled.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3 II: Hierarchical Star Formation Revealed by Spatial and Kinematic Substructures</title>
      <link>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202204_oc3dii_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202204_oc3dII_apj.png&#34; alt=&#34;animation&#34;&gt;
&lt;em&gt;Image credit: Figure 3 in Pang et al. 2022.&lt;/em&gt;: 3D morphology of 85 open clusters in the solar neighborhood with the color of the cluster scaled with the logarithm of age. An interactive version of this figure is available at &lt;a href=&#34;http://3doc-morphology.lowell.edu&#34;&gt;http://3doc-morphology.lowell.edu&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disruption of Hierarchical Clustering in the Vela OB2 Complex and the Cluster Pair Collinder 135 and UBC 7 with Gaia EDR3: Evidence of Supernova Quenching</title>
      <link>https://shihyuntang.github.io/publication/202109_velob_apj/</link>
      <pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202109_velob_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202108_VelOB_apj1.png&#34; alt=&#34;map&#34;&gt;
&lt;em&gt;Image credit: Figure 8 in Pang et al. 2021.&lt;/em&gt;: IRAS-IRIS infrared image of the 60 $\mu m$ band. The members of Huluwa 1&amp;ndash;5, and the cluster pair Collinder135 and UBC7 are displayed as colored dots.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Impacts of Water Latent Heat on the Thermal Structure of Ultra-Cool Objects: Brown Dwarfs and Free-Floating Planets</title>
      <link>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202105_ydwarf_moist_apj.png&#34; alt=&#34;atmprofile&#34;&gt;
&lt;em&gt;Image credit: Figure 1 in Tang et al. 2021.&lt;/em&gt;: Thermal structure profiles from converged solutions of the radiative-convective model at logg = 4.0 for YB ([M/H] of 0.0, here) and YP ([M/H] of 0.7 and 1.5) cases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IGRINS RV: A Python Package for Precision Radial Velocities with Near-Infrared Spectra</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_joss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IGRINS RV: A Precision RV Pipeline for IGRINS Using Modified Forward-Modeling in the Near-Infrared</title>
      <link>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</link>
      <pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202104_igrinsrv_aj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202104_igrinsrv_aj.png&#34; alt=&#34;rv&#34;&gt;
&lt;em&gt;Image credit: Figure 5 in Stahl et al. 2021.&lt;/em&gt;: Final RVs for RV standard stars&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3: its Relation to Cluster Dynamics</title>
      <link>https://shihyuntang.github.io/publication/202102_oc3d_apj/</link>
      <pubDate>Mon, 10 May 2021 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202102_oc3d_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202102_oc3d_apj.png&#34; alt=&#34;3d&#34;&gt;
&lt;em&gt;Image credit: Figure 8 in Pang et al. 2021.&lt;/em&gt;: Elipsoid fitting for the 3D spatial positions for the cluster members within tidal radius of NGC 2516, after distance correction through a Bayesian approach (see Section 4.1).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Different Fates of Young Star Clusters After Gas Expulsion</title>
      <link>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202006_ngc2232_apjl/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202006_ngc2232_apjl.png&#34; alt=&#34;MST&#34;&gt;
&lt;em&gt;Image credit: Figure 5 in Pang et al. 2020.&lt;/em&gt;: Number density, mass density, and mean mass distributions along a clustercentric distance r for NGC 2232 (blue curves) and LP 2439 (red curves).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diagnosing the Stellar Population and Tidal Structure of the Blanco1 Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/202001_blanco1_apj/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/202001_blanco1_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;202001_blanco1_apj.png&#34; alt=&#34;spatial&#34;&gt;
&lt;em&gt;Image credit: Figure 4 in Zhang et al. 2020.&lt;/em&gt; (a) Top view of Blanco1; (b) PM plot&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Discovery of Tidal Tails in Disrupting Open Clusters: Coma Berenices and a Neighbor Stellar Group</title>
      <link>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201905_comaber_tail_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;201905_comaber_tail_apj.png&#34; alt=&#34;spatial&#34;&gt;
&lt;em&gt;Image credit: Tang et al. 2019.&lt;/em&gt;: 3D spatial, proper motion, and radial velocity plot for Coma Ber &amp;amp; Group-X.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://shihyuntang.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterization of Stellar and Substellar Members in the Coma Berenices Star Cluster</title>
      <link>https://shihyuntang.github.io/publication/201808_comber_apj/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/publication/201808_comber_apj/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;201808_comaber_apj.png&#34; alt=&#34;spectra&#34;&gt;
&lt;em&gt;Image credit: Tang et al. 2018. Figure 17&lt;/em&gt;: The &lt;strong&gt;present day&lt;/strong&gt; mass function (MF) of the ~800 Myr old Coma Ber star cluster.&lt;/p&gt;
&lt;h3 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Mass Function (MF) of Coma Berenices peaks at 0.3 $M_\odot$:
&lt;ul&gt;
&lt;li&gt;The slope of the MF, $\alpha$, is approximately 0.49 $\pm$ 0.03 for the mass range 0.3 to 2.3 $M_{\odot}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The value of $\alpha$ for Coma Berenices is lower than that of the Salpeter initial mass function, which is 2.35 for masses between 1&amp;ndash;10 $M_{\odot}$ in the solar neighborhood.&lt;/li&gt;
&lt;li&gt;The value of $\alpha$ for Coma Berenices is also lower than the present-day MF for field M dwarfs, which is approximately 1.3 for masses between 0.1&amp;ndash;0.7 $M_{\odot}$ (Reid et al. 2002).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://shihyuntang.github.io/project_example/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://shihyuntang.github.io/project_example/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/project_example/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
