[{"authors":["admin"],"categories":null,"content":"Hi, I am Tang, Shih-Yun (湯士昀) from Taiwan, currently pursuing my Ph.D. at the Rice University Department of Physics and Astronomy. I work with Prof. Christopher Johns-Krull at Rice and Dr. Lisa Prato at the Lowell Observatory. Our project aims to discover the youngest exoplanets around T Tauri stars, which involves developing a precision radial velocity (RV) pipeline and isolating star spot-induced RV signals on these young active stars. My current focus is characterizing the spot activity on these stars by analyzing the variation in their surface effective temperature using the line ratio method.\nI earned my B.S. and M.S. in Physics from the National Central University (NCU), Taiwan. My initial research, under the mentorship of Prof. Wen-Ping Chen, involved characterizing stellar and substellar members of the Coma Berenices star cluster. For my Master’s thesis, I was the first to discover the tidal tails of the Coma Berenices star cluster, a project initiated during my 2018 summer internship at the Max-Planck-Institute for Astronomy (MPIA) and in collaboration with Prof. Xiaoying Pang. Together, we have been exploring the solar neighborhood using Gaia data to study stellar groups.\nI am also passionate about teaching and outreach. I have delivered guest lectures, served as a teaching assistant, and mentored undergraduate research projects. In my spare time, I enjoy playing pickleball and have a keen interest in machine learning for data analysis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1721319445,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shihyuntang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, I am Tang, Shih-Yun (湯士昀) from Taiwan, currently pursuing my Ph.D. at the Rice University Department of Physics and Astronomy. I work with Prof. Christopher Johns-Krull at Rice and Dr. Lisa Prato at the Lowell Observatory. Our project aims to discover the youngest exoplanets around T Tauri stars, which involves developing a precision radial velocity (RV) pipeline and isolating star spot-induced RV signals on these young active stars. My current focus is characterizing the spot activity on these stars by analyzing the variation in their surface effective temperature using the line ratio method.","tags":null,"title":"Shih-Yun Tang","type":"authors"},{"authors":null,"categories":null,"content":"A working on project by Christian Tai Udovicic to give scientists practical programming concepts in a hurry.\nScientific Programming in Real Life\n","date":1599264000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599343914,"objectID":"759e522e1e996b2861cc97e06aeb4e36","permalink":"https://shihyuntang.github.io/tutorials/spirl/","publishdate":"2020-09-05T00:00:00Z","relpermalink":"/tutorials/spirl/","section":"tutorials","summary":"A working on project by Christian Tai Udovicic to give scientists practical programming concepts in a hurry.","tags":null,"title":"SPIRL","type":"docs"},{"authors":null,"categories":null,"content":"This website serves as a personal note on\u0026hellip;\nThe notes and content presented on this webpage are intended for personal use and reference. While I strive to ensure accuracy and completeness, please be aware that the information may not be 100% correct. Additionally, this site have been reviewed and polished with the assistance of ChatGPT, an AI language model from OpenAI.\n","date":1714262400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1718936365,"objectID":"2e4dceffa9d16f408d3ddf4421ad2d05","permalink":"https://shihyuntang.github.io/tutorials/tools/","publishdate":"2024-04-28T00:00:00Z","relpermalink":"/tutorials/tools/","section":"tutorials","summary":"My notes on codes  ..","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"This website serves as a personal summary and resource hub for materials derived from \u0026ldquo;Modern Statistical Methods for Astronomy\u0026rdquo; by Eric D. Feigelson and G. Jogesh Babu, as well as from the course \u0026ldquo;Statistical Analysis in Physics and Astronomy\u0026rdquo; (ASTR 408/508), taught by Professor Dr. Patrick Hartigan in Spring 2024 at Rice University. For detailed course content and materials, please visit the course page here.\nThe notes and content presented on this webpage are intended for personal use and reference. While I strive to ensure accuracy and completeness, please be aware that the information may not be 100% correct. Additionally, this site have been reviewed and polished with the assistance of ChatGPT, an AI language model from OpenAI.\n","date":1714262400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1714777903,"objectID":"5928b4470c8579bd7fe12c5b13748433","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/","publishdate":"2024-04-28T00:00:00Z","relpermalink":"/tutorials/astro-stat/","section":"tutorials","summary":"A summary site for the astro-statistic course (astr 408/508) at Rice, Spring 2024.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Some Terminology in Statistics i.i.d. (Independent and Identically Distributed): This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.\nNull Hypothesis: In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.\nP-value: The p-value is the probability of observing test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A p-value less than a pre-determined significance level, often 0.05, leads to the rejection of the null hypothesis.\nConfidence Interval (CI): A confidence interval is a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter. The confidence level represents the probability that this interval will capture this parameter in repeated samples.\nFirst and Second Moments In probability and statistics, the concepts of first and second moments are central to understanding the distributions of random variables:\nFirst Moment (Mean): Definition: The first moment is the expected value of a random variable $X$, denoted as $E(X)$. Formula: $$E(X) = \\mu$$ Second Moment (Variance): Definition: The second moment about the mean is the variance of the random variable $X$, denoted as $\\sigma^2$. It measures the spread of the data points around the mean. Formula: The variance is calculated using the expectation of the squared deviations from the mean: $$ \\sigma^2 = E[(X-\\mu)^2] = E[X^2] - E(X)^2 $$ $$\\begin{align} \\sigma^2 \u0026amp;= E[(X-\\mu)^2] \\\\ \u0026amp;= E[X^2 - 2X\\mu + \\mu^2] \\\\ \u0026amp;= E[X^2] - 2\\mu E(X) + E(\\mu^2) \\\\ \u0026amp;= E[X^2] - 2\\mu^2 + \\mu^2 \\\\ \u0026amp;= E[X^2] - \\mu^2 \\\\ \u0026amp;= E[X^2] - E(X)^2 \\end{align}$$\nThird Moment (Skewness): Definition: The third central moment describes the skewness of the distribution of a random variable, indicating the degree of asymmetry around the mean. Skewness can reveal whether the distribution tails off more on one side than the other. Formula: The third moment is calculated using the expectation of the cubed deviations from the mean: $$ E[(X - E(X))^3] $$ Interpretation: A positive skewness indicates that the distribution has a long tail to the right (more positive side), while a negative skewness indicates a long tail to the left (more negative side). Covariance: Definition: Covariance measures the joint variability of two random variables, $X$ and $Y$. It assesses the degree to which two variables change together. If the greater values of one variable mainly correspond to the greater values of the other variable, and the same holds for the lesser values, the covariance is positive. Formula: The covariance between two variables $X$ and $Y$ is given by: $$ \\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))] $$ Interpretation: A positive covariance indicates that as $X$ increases, $Y$ tends to increase. A negative covariance suggests that as $X$ increases, $Y$ tends to decrease. Zero covariance indicates that the variables are independent, assuming they are also uncorrelated. Standardization Transforming the random variable to with zero mean and unit variance. This tranasformation also removes the unit on the random variable.\n$$ X_{std} = \\frac{X - \\mu}{\\sigma} $$\nZ-Test vs. T-Test Both the z-test and the t-test are statistical methods used to test hypotheses about means, but they are suited to different situations based on the distribution of the data and sample sizes.\nZ-Test Usage: The z-test is used when the population variance is known and the sample size is large (typically, n \u0026gt; 30). It can also be used for small samples if the data is known to follow a normal distribution. Assumptions: The population variance is known. The sample size is large enough for the Central Limit Theorem to apply, which ensures that the means of the samples are normally distributed. Formula: The test statistic for a z-test is calculated as follows: $$ Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} $$ where $\\bar{X}$ is the sample mean, $\\mu_0$ is the hypothesized population mean, $\\sigma$ is the population standard deviation, and $n$ is the sample size. T-Test Usage: The t-test is used when the population variance is unknown and the sample size is small. It is the appropriate test when dealing with estimates of the standard deviation from a normally distributed sample. Assumptions: The population from which samples are drawn is normally distributed. The population variance is unknown, and the sample variance is used as an estimate. Formula: The test statistic for a t-test is calculated as follows: $$ T = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} $$ where $\\bar{X}$ is the sample mean, $\\mu_0$ is the hypothesized population mean, $s$ is the sample standard deviation, and $n$ is the sample size. Degrees of Freedom: The degrees of freedom for the t-test are $n-1$, which affects the shape of the t-distribution used to determine the p-value. Key Differences Standard Deviation: The z-test uses the population standard deviation, while the t-test uses the sample\u0026rsquo;s standard deviation as an estimate of the population’s standard deviation. Sample Size: The z-test is typically used for larger sample sizes or when the population standard deviation is known, whereas the t-test is used for smaller sample sizes or when the population standard deviation is unknown. Distribution: The z-test statistic follows a normal distribution, while the t-test statistic follows a t-distribution, which is more spread out with heavier tails, providing a more conservative test for small sample sizes. A comparison plot between the t-distribution and the standard normal distribution can be find here.\n","date":1714345200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714707060,"objectID":"b47a407bf063eac022e0176cfde7c71c","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/1-1-some-basic/","publishdate":"2024-04-29T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/1-1-some-basic/","section":"tutorials","summary":"Some Terminology in Statistics i.i.d. (Independent and Identically Distributed): This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.\nNull Hypothesis: In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.","tags":null,"title":"Probability Basic","type":"docs"},{"authors":null,"categories":null,"content":"Probability Distributions Probability distributions describe how the probabilities of a random variable are distributed. Here are the two main types of probability functions associated with these distributions:\nProbability Density Function (PDF, continuous): Description: The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables. Key Characteristic: The area under the PDF curve between two points represents the probability of the variable falling within that range. Example: The normal distribution. Probability Mass Function (PMF, discrete): Description: The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value. Key Characteristic: The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take. Example: The binomial distribution, and the Poisson distribution. Cumulative Distribution Function (CDF):\nDescription: The CDF is used to determine the probability that a random variable (X) is $\\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables. Key Characteristic: The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF. Example: For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation. Chebyshev\u0026rsquo;s Theorem Chebyshev\u0026rsquo;s Theorem, also known as Chebyshev\u0026rsquo;s Inequality, is a fundamental result in probability theory that provides a way to estimate the probability that a random variable differs from its mean. This theorem is not restricted to normally distributed data, making it very versatile.\nTheorem Statement: $$ P(|X-\\mu| \u0026lt; k\\sigma) \\geq 1 - \\frac{1}{k^2} $$ where:\n($\\mu$) is the mean of the random variable ( X ), ($\\sigma$) is the standard deviation of ( X ), ($k$) is a positive number greater than 1. Key Points:\nGenerality: Chebyshev\u0026rsquo;s theorem applies to any probability distribution where the mean and variance are defined. Implication: The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \\frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\\frac{1}{4} = 0.75)$). Limitations: While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean. Chebyshev\u0026rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.\nDiscrete Distributions Binomial Distribution The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as \u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;.\nKey Points:\nThere are only two possible outcomes for each trial: success (1) and failure (0). The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials. The probability mass function (PMF) for the Binomial distribution is expressed as: $$ P(X=x) = C^{n}_{x} \\theta^x (1-\\theta)^{n-x} $$ where $\\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.\nExample: Consider flipping a fair coin five times. What is the probability of getting exactly two heads?\nThe calculation is as follows: $$ P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \\frac{5!}{2!3!} \\times 0.25 \\times 0.125 = 31.25% $$\nGeometric Distribution The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.\nPMF: $$ g(x; \\theta) = \\theta(1-\\theta)^{x-1} $$ where, $\\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.\nKey Properties:\nMean ($\\mu$): The expected number of trials to get the first success is given by: $$ \\mu = \\frac{1}{\\theta} $$ Variance ($\\sigma^2$): The variance of the number of trials to get the first success is: $$ \\sigma^2 = \\frac{1-\\theta}{\\theta^2} $$ Interpretation:\nThe mean and variance provide insights into the \u0026ldquo;spread\u0026rdquo; or variability of trials needed to achieve the first success, with higher values of $\\theta$ leading to fewer expected trials. This distribution is commonly used in quality control, reliability testing, and other areas where the \u0026ldquo;time\u0026rdquo; or number of trials until the first success is of interest.\nPoisson Distribution The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.\nA practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.\nNote: Independence of events holds only if the CCD is not saturated.\nAssumptions:\nThe rate of event occurrence, $\\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \\Delta t$ is $\\lambda \\Delta t$. The probability of an event in the interval ${t, t+\\Delta t}$ is independent of previous events. The probability mass function (PMF) of the Poisson distribution is defined as: $$ P(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$ where $\\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.\nFor the Poisson distribution, the expected value (mean) is $\\mu = \\lambda$, and the variance is $\\sigma^2 = \\lambda$.\nPoisson Noise (Shot Noise) Since $\\sigma^2 = \\lambda$, the standard deviation (Poisson noise) is $\\sigma = \\sqrt{\\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\\sigma = \\sqrt{N}$.\nDue to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows: $$ SNR = \\frac{\\mu}{\\sigma} = \\frac{\\mu}{\\sqrt{\\mu}} = \\sqrt{\\mu} $$ This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.\n","date":1714345200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714707060,"objectID":"5160afd5dbae32f58673f5ad32466df8","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/1-2-probability/","publishdate":"2024-04-29T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/1-2-probability/","section":"tutorials","summary":"Probability Distributions Probability distributions describe how the probabilities of a random variable are distributed. Here are the two main types of probability functions associated with these distributions:\nProbability Density Function (PDF, continuous): Description: The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables. Key Characteristic: The area under the PDF curve between two points represents the probability of the variable falling within that range.","tags":null,"title":"Distributions","type":"docs"},{"authors":null,"categories":null,"content":"Probability Distributions Probability distributions describe how the probabilities of a random variable are distributed. Here are the two main types of probability functions associated with these distributions:\nProbability Density Function (PDF, continuous): Description: The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables. Key Characteristic: The area under the PDF curve between two points represents the probability of the variable falling within that range. Example: The normal distribution. Probability Mass Function (PMF, discrete): Description: The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value. Key Characteristic: The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take. Example: The binomial distribution, and the Poisson distribution. Cumulative Distribution Function (CDF):\nDescription: The CDF is used to determine the probability that a random variable (X) is $\\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables. Key Characteristic: The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF. Example: For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation. Chebyshev\u0026rsquo;s Theorem Chebyshev\u0026rsquo;s Theorem, also known as Chebyshev\u0026rsquo;s Inequality, is a fundamental result in probability theory that provides a way to estimate the probability that a random variable differs from its mean. This theorem is not restricted to normally distributed data, making it very versatile.\nTheorem Statement: $$ P(|X-\\mu| \u0026lt; k\\sigma) \\geq 1 - \\frac{1}{k^2} $$ where:\n($\\mu$) is the mean of the random variable ( X ), ($\\sigma$) is the standard deviation of ( X ), ($k$) is a positive number greater than 1. Key Points:\nGenerality: Chebyshev\u0026rsquo;s theorem applies to any probability distribution where the mean and variance are defined. Implication: The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \\frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\\frac{1}{4} = 0.75)$). Limitations: While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean. Chebyshev\u0026rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.\nDiscrete Distributions Binomial Distribution The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as \u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;.\nKey Points:\nThere are only two possible outcomes for each trial: success (1) and failure (0). The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials. The probability mass function (PMF) for the Binomial distribution is expressed as: $$ P(X=x) = C^{n}_{x} \\theta^x (1-\\theta)^{n-x} $$ where $\\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.\nExample: Consider flipping a fair coin five times. What is the probability of getting exactly two heads?\nThe calculation is as follows: $$ P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \\frac{5!}{2!3!} \\times 0.25 \\times 0.125 = 31.25% $$\nGeometric Distribution The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.\nPMF: $$ g(x; \\theta) = \\theta(1-\\theta)^{x-1} $$ where, $\\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.\nKey Properties:\nMean ($\\mu$): The expected number of trials to get the first success is given by: $$ \\mu = \\frac{1}{\\theta} $$ Variance ($\\sigma^2$): The variance of the number of trials to get the first success is: $$ \\sigma^2 = \\frac{1-\\theta}{\\theta^2} $$ Interpretation:\nThe mean and variance provide insights into the \u0026ldquo;spread\u0026rdquo; or variability of trials needed to achieve the first success, with higher values of $\\theta$ leading to fewer expected trials. This distribution is commonly used in quality control, reliability testing, and other areas where the \u0026ldquo;time\u0026rdquo; or number of trials until the first success is of interest.\nPoisson Distribution The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.\nA practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.\nNote: Independence of events holds only if the CCD is not saturated.\nAssumptions:\nThe rate of event occurrence, $\\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \\Delta t$ is $\\lambda \\Delta t$. The probability of an event in the interval ${t, t+\\Delta t}$ is independent of previous events. The probability mass function (PMF) of the Poisson distribution is defined as: $$ P(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$ where $\\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.\nFor the Poisson distribution, the expected value (mean) is $\\mu = \\lambda$, and the variance is $\\sigma^2 = \\lambda$.\nPoisson Noise (Shot Noise) Since $\\sigma^2 = \\lambda$, the standard deviation (Poisson noise) is $\\sigma = \\sqrt{\\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\\sigma = \\sqrt{N}$.\nDue to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows: $$ SNR = \\frac{\\mu}{\\sigma} = \\frac{\\mu}{\\sqrt{\\mu}} = \\sqrt{\\mu} $$ This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.\n","date":1714345200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"7a453612d49841799c4201885cadcf93","permalink":"https://shihyuntang.github.io/tutorials/tools/1-2-probability/","publishdate":"2024-04-29T00:00:00+01:00","relpermalink":"/tutorials/tools/1-2-probability/","section":"tutorials","summary":"Probability Distributions Probability distributions describe how the probabilities of a random variable are distributed. Here are the two main types of probability functions associated with these distributions:\nProbability Density Function (PDF, continuous): Description: The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables. Key Characteristic: The area under the PDF curve between two points represents the probability of the variable falling within that range.","tags":null,"title":"Distributions","type":"docs"},{"authors":null,"categories":null,"content":"Bayes Theorem is a fundamental principle in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It is particularly powerful in the context of predictive modeling and decision-making processes.\nMathematical Formulation If events $A$ and $B$ are independent, then the probability of $A$ given $B$ is simply the probability of $A$: $$ P(A|B) = P(A) $$ However, when events $A$ and $B$ are not independent, the relationship changes as follows: $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{and} \\quad P(B|A) = \\frac{P(A \\cap B)}{P(A)} $$ From these relationships, we derive Bayes\u0026rsquo; Theorem: $$ P(B|A) = \\frac{P(B) P(A|B)}{P(A)} $$ Bayes\u0026rsquo; Theorem can be interpreted in terms of updating beliefs: $$ \\text{Posterior} = \\frac{\\text{Prior} \\times \\text{Likelihood}}{\\text{Evidence}} $$\nComponents of Bayes\u0026rsquo; Theorem Likelihood ( $ P(A|B) $ ): The likelihood is a function that measures the plausibility of a model parameter value given specific observed data. In many applications, especially in statistical modeling, the likelihood is assumed to follow a normal distribution: $$ L(\\theta; x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\theta)^2}{2\\sigma^2}} $$ where $\\theta$ represents the parameter of interest, $x$ represents the data, and $\\sigma^2$ is the variance. Prior ( $ P(B) $ ): The prior represents the initial belief about the distribution of the parameter before considering the current data. Priors can be subjective or based on previous studies: $$ P(\\theta) $$ It can be uniform (representing no initial preference) or follow a specific distribution that reflects prior knowledge about the parameter, e.g., a Gaussian prior, gamma and beta distribution. Evidence or Normalizing Constant ( $ P(A) $ ): Often considered as a normalizing factor, the evidence ensures that the posterior probabilities sum to one. It is calculated as: $$ P(A) = \\int P(A|B) P(B) dB $$ This factor is essential for the probabilistic model to be valid but is usually more relevant in analytical calculations than in practical applications. Posterior ( $ P(B|A) $ ): The posterior probability reflects the updated belief about the parameter after considering the new evidence. It combines the prior and the likelihood given new data: $$ P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{\\int P(x|\\theta)P(\\theta) d\\theta} $$ Practical Applications Bayes\u0026rsquo; Theorem is used extensively in various fields including exoplanet study, machine learning, medical testing, and any scenario requiring iterative updating of beliefs upon new evidence. Understanding how to apply the theorem allows for more informed decision-making processes and predictions.\nRelationship Between Likelihood and Chi-Squared Statistic The likelihood function in statistical models often measures how well a set of parameters fits the data. When the data and the model predictions vary according to a normal distribution, the likelihood function can be directly linked to the chi-squared statistic.\nChi-Squared Statistic: The chi-squared statistic is a measure of how expectations compare to actual observed data. In the context of likelihood calculations, the chi-squared statistic quantifies the discrepancy between observed data and the data predicted by the model, under the assumption that the discrepancies are normally distributed.\nCalculation: To calculate the chi-squared statistic as a measure of likelihood, you can use the following formula: $$ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{\\sigma_i^2} $$ where $O_i$ are the observed values, $E_i$ are the expected values predicted by the model, and $\\sigma_i^2$ is the variance associated with each observation.\nUsing Chi-Squared to Calculate Likelihood: The likelihood of observing the data given the model can be expressed as: $$ L = e^{-\\chi^2/2} $$ This formulation arises from the exponential component of the PDF of the normal distribution, which is what a likelihood function usually take, thus, a easier way to remember how to calculate the likelihood.\n","date":1714604400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714665589,"objectID":"01c5981139bf557b8975f61444eb8a1e","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/1-3-bayesian/","publishdate":"2024-05-02T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/1-3-bayesian/","section":"tutorials","summary":"Bayes Theorem is a fundamental principle in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It is particularly powerful in the context of predictive modeling and decision-making processes.\nMathematical Formulation If events $A$ and $B$ are independent, then the probability of $A$ given $B$ is simply the probability of $A$: $$ P(A|B) = P(A) $$ However, when events $A$ and $B$ are not independent, the relationship changes as follows: $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{and} \\quad P(B|A) = \\frac{P(A \\cap B)}{P(A)} $$ From these relationships, we derive Bayes\u0026rsquo; Theorem: $$ P(B|A) = \\frac{P(B) P(A|B)}{P(A)} $$ Bayes\u0026rsquo; Theorem can be interpreted in terms of updating beliefs: $$ \\text{Posterior} = \\frac{\\text{Prior} \\times \\text{Likelihood}}{\\text{Evidence}} $$","tags":null,"title":"Bayes' theorem","type":"docs"},{"authors":null,"categories":null,"content":"Empirical Distribution Function (EDF) The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as: $$ F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I[x_i \\leq x] $$ where $I[\\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\\frac{1}{n}$. An example plot of an EDF is shown below.\nKolmogorov-Smirnov Test (KS Test) The Kolmogorov-Smirnov (KS) Test measures the maximum distance ($D$) between two distribution functions. This can be used to compare an empirical distribution with a theoretical model (one-sample test) or two empirical distributions (two-sample test). If the distributions are identical, $D$ equals zero.\nStatistic: $$ D_n = \\max_x |F_n(x) - S_n(x)| $$ A table for the critical values of $D_n$ can be found here.\nIf $D_n$ exceeds the critical value, we can reject the null hypothesis that there is no significant difference between the two distributions.\nCramér-von Mises Statistic (CvM) The Cramér-von Mises statistic is used to quantify the goodness of fit of an empirical distribution to a theoretical model. It is particularly useful as it considers the squared differences over all points, providing a more sensitive measure to differences in the tails of the distributions.\nThis statistic assesses the integrated squared distance between the empirical distribution function $F_n(x)$ and the theoretical distribution $S(x)$, weighted by the number of observations $n$.\nAnderson-Darling Statistic (AD) The Anderson-Darling statistic is a modification of the Cramér-von Mises statistic that gives more weight to the tails of the distribution. It is particularly effective in identifying departures from a theoretical distribution in the tails.\nThis weighted approach makes the AD statistic more sensitive to discrepancies in the distribution\u0026rsquo;s tails than the CvM statistic.\nBoth CvM and AD tests are powerful tools for statistical hypothesis testing, especially in scenarios where understanding the tail behavior of distributions is crucial.\nWilcoxon Rank Sum Test The Wilcoxon Rank Sum Test, also known as the Mann-Whitney Rank Sum test, is designed to assess whether two independent samples come from the same distribution. Here’s how the test statistic is calculated:\nCombine and Rank the Data: Combine all observations from both samples into a single dataset. Rank all observations from the smallest to largest. Ties are given a rank equal to the average of the ranks they would have otherwise occupied. Calculate the Rank Sums: Calculate the sum of the ranks for observations from each sample separately. Let $T_1$ be the sum of ranks for the first sample, and $T_2$ for the second sample. Compute the Test Statistic: The test statistic $U$ is calculated using: $$ U = T_1 - \\frac{n_1(n_1+1)}{2} $$ where $n_1$ is the number of observations in the first sample. $U$ can also be computed for the second sample, and the smaller of the two $U$ values is often used as the test statistic. Determine the Significance: The significance of the observed $T_1$ value is determined by comparing it to values in a reference distribution, which approximates a normal distribution under the null hypothesis when the sample sizes are sufficiently large. The mean and standard deviation of $T_1$ are used to compute a z-score: $$ |z| = \\left| \\frac{T_1 - \\text{mean}(T_1)}{\\text{std dev}(T_1)} \\right| $$ where $\\text{mean}(T_1) = \\frac{n_1(n_1+n_2+1)}{2}$, and $\\text{Var}(T_1) = \\frac{n_1 n_2(n_1+n_2+1)}{2}$. The p-value is then calculated from the normal distribution. The Wilcoxon Rank Sum Test does not require the data to follow a specific distribution, making it a robust and widely applicable non-parametric method for comparing two samples.\nKruskal-Wallis Test The Kruskal-Wallis (KW) test is used to determine if there are statistically significant differences between the distributions of two or more groups of an independent variable. It generalizes the Wilcoxon Rank Sum Test to more than two groups. The null hypothesis assumes that all groups come from identical distributions. The test statistic follows a chi-squared ($\\chi^2$) distribution with $k-1$ degrees of freedom, where $k$ is the number of groups.\nComparison of Statistical Distribution Tests Test Key Usage Data Requirement Distribution Assumption Sensitivity KS Test Comparing a sample with a reference distribution One or two samples, continuous or ordinal None Sensitive to differences in location, scale, and shape CvM Statistic Testing goodness of fit to a theoretical distribution One sample, continuous None Integrates squared differences; sensitive across entire distribution AD Statistic Testing goodness of fit with emphasis on tail differences One sample, continuous None Increased weight to tails; highly sensitive to tail discrepancies Wilcoxon Rank Sum Comparing two independent samples Two independent samples, ordinal or continuous None Sensitive to differences in medians Kruskal-Wallis Comparing more than two independent samples Two or more independent samples, ordinal or continuous None Generalization of Wilcoxon, sensitive to differences across multiple samples ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"0b7c09d552c4e9993f61ddab86faf7cd","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/2-1-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/astro-stat/2-1-distribution/","section":"tutorials","summary":"Empirical Distribution Function (EDF) The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as: $$ F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I[x_i \\leq x] $$ where $I[\\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\\frac{1}{n}$. An example plot of an EDF is shown below.","tags":null,"title":"Distribution","type":"docs"},{"authors":null,"categories":null,"content":"Nonparametric correlation tests are essential tools for assessing the strength and direction of a relationship between two datasets, especially when the underlying distributions are unknown or non-normal. These tests are robust alternatives to the Pearson correlation coefficient, suitable for various types of relationships. The null hypothesis for these tests is that there is no correlation between the datasets.\n*Pearson Correlation Coefficient Pearson correlation test is a parametric test, but I put it here for completeness and for comparison.\nThe Pearson correlation coefficient measures the linear relationship between two continuous variables. It is a parametric test and assumes that the data is normally distributed. The coefficient varies between -1 and +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\nSpearman\u0026rsquo;s Rank Correlation Coefficient ($\\rho$) Spearman\u0026rsquo;s correlation assesses how well the relationship between two variables can be described using a monotonic function. It does not require the data to be normally distributed, as it uses the rank of the data rather than the actual values.\nStatistic: $$ \\rho = 1 - \\frac{6 \\sum d_i^2 + T}{n(n^2 - 1)} $$ where $d_i$ is the difference between the ranks of corresponding variables and $n$ is the number of observations. For handling ties, the correction term $T$ is applied: $$ T = \\sum t_x \\left[ \\frac{x^3 - x}{12} \\right] $$ where $t_x$ is the number of ties involving $x$ elements. To further estimate the $p$-value, the $z$ score can be calculated by: $$ z = \\sqrt{n-1} \\rho $$ which approximates a normal distribution under the null hypothesis. One can use the $\\rho$ statistic and compare to the critical values here.\nKendall\u0026rsquo;s Tau ($\\tau$) Kendall\u0026rsquo;s Tau measures the ordinal association between two variables by considering the number of concordant and discordant pairs. It is less sensitive to outliers than Pearson and can be more interpretable in terms of the proportion of concordant pairs.\nStatistic: $$ \\tau = \\frac{N_c - N_d}{\\sqrt{(N_c + N_d + T)(N_c + N_d + U)}} $$ where $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $T$ is the number of ties on one variable, and $U$ is the number of ties on the other variable.\nThe critical values for $\\tau$ can be find here.\nComparison of Correlation Tests Test Best Use Case Sensitivity Data Requirements Pearson Linear relationships Highly sensitive to linear trends Normal distribution, continuous data Spearman\u0026rsquo;s rho Monotonic relationships, not necessarily linear Sensitive to monotonic trends, not affected by outliers Ordinal data or non-normal distributions Kendall\u0026rsquo;s tau General increasing or decreasing trends, less intensive computation Less sensitive to errors in data, good for small samples or data with many ties Ordinal data, robust against outliers ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"8f140172b73383b9d75b33f29e755d07","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/2-2-correlation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/astro-stat/2-2-correlation/","section":"tutorials","summary":"Nonparametric correlation tests are essential tools for assessing the strength and direction of a relationship between two datasets, especially when the underlying distributions are unknown or non-normal. These tests are robust alternatives to the Pearson correlation coefficient, suitable for various types of relationships. The null hypothesis for these tests is that there is no correlation between the datasets.\n*Pearson Correlation Coefficient Pearson correlation test is a parametric test, but I put it here for completeness and for comparison.","tags":null,"title":"Correlation","type":"docs"},{"authors":null,"categories":null,"content":"Proper smoothing of data is crucial in various applications, especially for interpolating and visualizing results. A classic example involves choosing the right bin size for histograms. Incorrect bin sizes can either obscure significant data features by being too large or create misleading features if they are too small.\nScott\u0026rsquo;s Rule for Bin Width Scott\u0026rsquo;s rule provides a method to determine the optimal bin width for a histogram, balancing detail and smoothness. The formula for Scott\u0026rsquo;s bin width is: $$ h_{\\text{Scott}} = \\frac{3.5 \\cdot \\text{std}}{n^{1/3}} \\quad \\text{or} \\quad \\frac{2 \\cdot \\text{IQR}}{n^{1/3}} $$ where $\\text{std}$ is the standard deviation, $\\text{IQR}$ is the interquartile range, and $n$ is the number of data points. This method aims to minimize potential distortion in the histogram by accounting for the variability and size of the data set.\nAverage Smoothing Histogram Average smoothing, applied to histograms, involves averaging adjacent bins to reduce variance within the bins. This technique smooths out fluctuations that might be random and highlights broader trends in the data distribution.\nKernel Density Estimation (KDE) Kernel Density Estimation is a non-parametric way to estimate the probability density function of a random variable. KDE smooths the data by convolving it with a kernel, which is a predefined function, typically Gaussian. The formula for KDE is: $$ \\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - x_i}{h}\\right) $$ where $K$ is the kernel function, $x_i$ are the data points, $h$ is the bandwidth, and $n$ is the number of data points. The choice of bandwidth $h$ crucially affects the estimator\u0026rsquo;s bias and variance.\nAdaptive Smoothing Adaptive smoothing techniques adjust the smoothing parameters locally, depending on the density or other characteristics of the data. These methods aim to achieve better smoothing in areas with higher variability or lower density, allowing more detailed features to emerge in dense regions while smoothing out noise in sparse regions.\nNadaraya-Watson Estimator The Nadaraya-Watson estimator is a type of kernel regression that uses locally weighted averages to estimate conditional expectations. It is particularly useful in regression analysis to model the relationship between variables.\n","date":1714604400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"98d2575a67c40f0d81b35e50764e7d87","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/3-1-data-smoothing/","publishdate":"2024-05-02T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/3-1-data-smoothing/","section":"tutorials","summary":"Proper smoothing of data is crucial in various applications, especially for interpolating and visualizing results. A classic example involves choosing the right bin size for histograms. Incorrect bin sizes can either obscure significant data features by being too large or create misleading features if they are too small.\nScott\u0026rsquo;s Rule for Bin Width Scott\u0026rsquo;s rule provides a method to determine the optimal bin width for a histogram, balancing detail and smoothness.","tags":null,"title":"Data Smoothing-Density Estimation","type":"docs"},{"authors":null,"categories":null,"content":"Regression analysis is fundamental in statistical modeling, used for predicting and forecasting, and understanding relationships between variables. Below are various regression methods:\nLeast-Square Linear Regression (without Uncertainty) Ordinary Least Squares (OLS): This method minimizes the residual sum of squares (RSS) between the observed values in the dataset and the values predicted by the linear model. $$ \\text{min RSS} = \\text{min} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2 $$ where $X_i$ and $Y_i$ are the observed values, and $\\beta_0$ and $\\beta_1$ are the intercept and slope of the linear model, respectively.\nSymmetric Least Squares-Orthogonal Regression: This method minimizes the summed squares of the residuals orthogonal to the regression line, providing a more general approach than OLS when errors in both variables need to be considered.\nRobust Regression - Thiel-Sen Estimator: This technique uses the median of the slopes between pairs of points as the estimator, providing robustness against outliers.\nQuantile Regression: Focuses on estimating either the conditional median or other quantiles of the response variable, providing a more comprehensive view of the possible outcome distribution than mean regression.\nMaximum Likelihood Estimation: Assumes a probability distribution model for the data, often a normal distribution, and finds the parameter values that maximize the likelihood of observing the data.\nWeighted Least Squares (with Uncertainty) In scenarios with heteroscedastic errors (errors that varies), weighted least squares (WLS) is more appropriate:\n$$ S_{r,wt} = \\sum_{i=1}^n \\frac{(Y_i - \\beta_0 - \\beta_1 X_i)^2}{\\sigma_{Y,i}^2} $$ where $\\sigma_{Y,i}^2$ is the variance associated with each observation, weighting the residuals accordingly. This is related to the $\\chi^2$ statistic used in minimum $\\chi^2$ regression:\n$$ \\chi^2_{me} = \\sum_{i=1}^n (O_i - M_i)^2 / \\sigma_{i,me}^2 $$\nLogistic Regression Logistic regression is used for modeling binary outcome variables by using the logistic function to estimate probabilities that can be transformed into binary values.\nModel Validation and Selection Coefficient of Determination (R²): Measures the proportion of variability in a dataset that is explained by the regression model. $$ R^2 = 1 - \\frac{ \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2 }{ \\sum_{i=1}^n (Y_i - \\bar{Y})^2 } $$ where\n$\\hat{Y_i}$ is the predicted value of the dependent variable for the $i$-th observation from the model $\\bar{Y}$ is the mean of the observed data $Y_i$. An $R^2$ value of 1 implies a perfect fit, meaning that the model explains all the variability of the response data around its mean. Conversely, an $R^2$ value of 0 indicates at bad fit.\nAdjusted $R^2$: Adjusts $R²$ for the number of predictors in the model, preventing overfitting by penalizing excessive use of parameters. $$ R_a^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1} $$ where $p$ is the number of parameters, and $n$ is the number of data points.\nNormal quantile-quantile plot (Q-Q Plot): Used to check the normality of residuals. If the points lie along a straight line, the residuals are normally distributed, an assumption in many regression models.\nAkaike Information Criterion (AIC): The AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model. $$ \\text{AIC} = 2k - 2\\ln(\\hat{L}) $$ where $k$ is the number of parameters in the model, and $\\hat{L}$ is the maximum likelihood value.\nThe model with the lowest AIC among a set of models is typically chosen. The AIC is particularly useful when a model is being fit to data: minimizing the AIC maximizes the likelihood function given the data while penalizing for increasing numbers of parameters.\nBayesian Information Criterion (BIC): The BIC is similar to the AIC but introduces a stronger penalty for including additional variables to the model. It is derived from Bayesian probability. $$ \\text{BIC} = \\ln(n)k - 2\\ln(\\hat{L}) $$ where $n$ is the number of observations, $k$ is the number of parameters in the model, and $\\hat{L}$ is the maximum likelihood of the model.\nThe BIC is generally stricter than the AIC and can be preferable for models with large $n$, penalizing free parameters more heavily. Like the AIC, the model with the lowest BIC is generally preferred.\n","date":1714604400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"5f02cfbdb0284566587e7aca8fda8510","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/4-1-regression/","publishdate":"2024-05-02T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/4-1-regression/","section":"tutorials","summary":"Regression analysis is fundamental in statistical modeling, used for predicting and forecasting, and understanding relationships between variables. Below are various regression methods:\nLeast-Square Linear Regression (without Uncertainty) Ordinary Least Squares (OLS): This method minimizes the residual sum of squares (RSS) between the observed values in the dataset and the values predicted by the linear model. $$ \\text{min RSS} = \\text{min} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2 $$ where $X_i$ and $Y_i$ are the observed values, and $\\beta_0$ and $\\beta_1$ are the intercept and slope of the linear model, respectively.","tags":null,"title":"Regression","type":"docs"},{"authors":null,"categories":null,"content":" Principal Component Analysis (PCA) Procedure PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It\u0026rsquo;s especially powerful when dealing with high-dimensional data. Here\u0026rsquo;s how it works:\nStep 1: Create the Data Matrix: Construct matrix $\\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person). $$ \\mathbf{d} = \\begin{bmatrix} d_{11} \u0026amp; d_{21} \u0026amp; \\cdots \u0026amp; d_{t1} \\\\ d_{12} \u0026amp; d_{22} \u0026amp; \\cdots \u0026amp; d_{t2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ d_{1n} \u0026amp; d_{2n} \u0026amp; \\cdots \u0026amp; d_{tn} \\end{bmatrix} $$\nStep 2: Center the Data: Subtract the mean of each attribute from the corresponding values to center the data around the origin. This helps in aligning the PCA with the directions of maximum variance. $$ \\mathbf{x} = \\mathbf{d} - \\mathbf{\\langle\\beta\\rangle} $$ where $\\mathbf{\\langle\\beta_j\\rangle} = \\frac{1}{t} \\sum_{j=1}^t d_{ji}$ represents the mean of each attribute across all data points. The centered data matrix $\\mathbf{x}$ looks like this: $$ \\mathbf{x} = \\begin{bmatrix} x_{11} \u0026amp; x_{21} \u0026amp; \\cdots \u0026amp; x_{t1} \\\\ x_{12} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{t2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ x_{1n} \u0026amp; x_{2n} \u0026amp; \\cdots \u0026amp; x_{tn} \\end{bmatrix} $$\nStep 3: Construct the Covariance Matrix: The covariance matrix $\\mathbf{\\Sigma_x}$ is constructed from $\\mathbf{x}$. Since $\\mathbf{x}$ is centered, its expected value matrix $\\mathbf{E(x)}$ is zero. The covariance matrix is then:\n$$ \\mathbf{\\Sigma_x} = \\frac{1}{t} \\mathbf{x} \\mathbf{x}^T $$\nThis matrix captures the variance shared between the attributes, and its diagonal elements represent the variance of each attribute.\nStep 4: Eigenvalue Decomposition: Eigenvalues and eigenvectors of the covariance matrix $\\mathbf{\\Sigma_x}$ are computed. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues represent the magnitude of the variance along each principal component.\nTo find the principal components:\nMaximize $\\vec{w}^T \\mathbf{\\Sigma_x} \\vec{w}$ subject to $\\vec{w}^T \\vec{w} = 1$. This leads to solving $(\\mathbf{\\Sigma_x} - \\lambda I) \\vec{w} = 0$, where $\\lambda$ is the eigenvalue. The principal component associated with the largest eigenvalue ($\\lambda_1$) captures the most variance.\nStep 5: Transform Data to PCA Space Each data point is then projected onto the PCA space using the eigenvectors, effectively reducing dimensionality while retaining the most significant variance features.\nStep 6: Interpretation Interpret the physical meaning of each principal component, which might involve understanding how original attributes combine to form the component.\nIndependent Component Analysis (ICA) Independent Component Analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents that are maximally independent. This technique is particularly useful in the field of blind source separation, where the goal is to separate a mixture of signals (like audio tracks) into their individual, independent components without prior knowledge about the source signals.\nUnderstanding the Context In applications like audio processing, ICA can be used to separate different speaking voices from a single audio track recorded with multiple voices overlapping. Each recording may capture the same set of voices but with varying amplitudes depending on the distance and orientation of each voice relative to the microphone. Unlike Principal Component Analysis (PCA), which seeks directions of maximum variance and might mix sources further, ICA focuses on maximizing statistical independence between the components. This characteristic makes ICA suitable for tasks where the underlying sources are non-Gaussian and independent.\nProcedure for Applying ICA Preprocessing with PCA: Dimensionality Reduction: Initially, PCA is applied to reduce the dimensionality of the data. This step is crucial because it removes noise and reduces the complexity of the data, which simplifies the subsequent ICA. Whitening: The data is transformed into components that are uncorrelated and have unit variance. This transformation, often performed as part of PCA, is also known as \u0026ldquo;whitening\u0026rdquo;. It ensures that the ICA algorithm focuses only on finding the independent sources without being misled by possible correlations in the data. Applying ICA: Identify Independent Components (ICs): After whitening, the ICA algorithm seeks to rotate the whitened data to a new coordinate system where the statistical independence of the resulting signals is maximized. This involves optimizing an objective function that measures non-Gaussianity (since independence implies non-Gaussianity under certain conditions, see below section on Non-Gaussianity). Extraction of Sources: The independent components correspond to the original signals/sources that were mixed in the observed data. Each component should represent one source, such as a single voice in the context of audio processing. Example Application For instance, if you have a recording from a busy restaurant, ICA can help isolate single voices from the background noise and other voices. This technique is invaluable in environments where multiple sources overlap significantly but retain their independence in terms of their statistical signatures.\nUnderstanding Independence, Non-Gaussianity, and the Central Limit Theorem The Central Limit Theorem (CLT) states that the sum of a large number of independent random variables, each with finite mean and variance, tends toward a Gaussian distribution, regardless of the original distributions of the variables. This principle is critical in the context of Independent Component Analysis (ICA), which is used to separate mixed signals into their original, independent components.\nImplications for ICA Role of the CLT: The CLT implies that a mixture of multiple independent, non-Gaussian signals tends to be more Gaussian-like than any of the individual signals. ICA utilizes this property by assuming that the original sources are non-Gaussian. The mixed signal, being more Gaussian, provides a clue that separation is possible by identifying and maximizing the non-Gaussian aspects of its components. Non-Gaussianity as a Tool: ICA algorithms focus on maximizing non-Gaussianity to separate the independent components. This is because non-Gaussian signals, when summed, lose some of their distinct statistical features, making the mixture more Gaussian. By finding projections of the data that are maximally non-Gaussian, ICA can effectively identify and separate the original independent sources. Challenge with Gaussian Sources: If the original sources were Gaussian, their sum would also be Gaussian, offering no statistical advantage for separation. This is why ICA is particularly powerful for non-Gaussian data sources, where it can exploit statistical differences to distinguish between the sources. Practical Considerations ICA is particularly effective in scenarios where the underlying sources of a signal are known to be non-Gaussian, such as in audio signal processing or in biomedical signal analysis. The ability to reverse the effects of the CLT and uncover the original signals from a complex mixture is what makes ICA a valuable tool in modern data analysis.\n","date":1714604400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714777730,"objectID":"4d3e645abe5e7171407b3b18d9408930","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/","publishdate":"2024-05-02T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/5-1-multivariate-analysis/","section":"tutorials","summary":"Principal Component Analysis (PCA) Procedure PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It\u0026rsquo;s especially powerful when dealing with high-dimensional data. Here\u0026rsquo;s how it works:\nStep 1: Create the Data Matrix: Construct matrix $\\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person). $$ \\mathbf{d} = \\begin{bmatrix} d_{11} \u0026amp; d_{21} \u0026amp; \\cdots \u0026amp; d_{t1} \\\\ d_{12} \u0026amp; d_{22} \u0026amp; \\cdots \u0026amp; d_{t2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ d_{1n} \u0026amp; d_{2n} \u0026amp; \\cdots \u0026amp; d_{tn} \\end{bmatrix} $$","tags":null,"title":"Multivariate Analysis","type":"docs"},{"authors":null,"categories":null,"content":"Clustering and classification are fundamental techniques in data analysis and machine learning, used to group data points based on similarities and to categorize them into distinct classes.\nHierarchical Clustering Hierarchical clustering builds nested clusters by progressively merging or splitting them based on the distance between data points or groups. The method relies heavily on the choice of distance calculation:\nSingle Linkage (Nearest Neighbor): This method, also known as the friend-of-a-friend technique, considers the shortest distance between points in two clusters (see the first plot below). It can result in elongated, \u0026ldquo;chain-like\u0026rdquo; clusters that capture local structure but might miss broader data groupings (see the second plot below). Complete Linkage: Use the maximum distance between points in two clusters. This method tends to produce more compact and well-separated clusters, reducing the chain effect seen in single linkage. Average Linkage: Calculates the average distance between all pairs of points in two clusters. This method provides a balance between the sensitivity of single linkage and the strictness of complete linkage. Ward\u0026rsquo;s Minimum Variance: Minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum increase in total variance are merged. This method tends to create more regular-sized clusters (e.g., spherical or ellipsoidal), which can be advantageous for certain datasets. (Figure credit: https://www.researchgate.net/figure/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and_fig57_281014334)\nOnce distances are calculated, the hierarchical clustering algorithm uses these distances to merge or split clusters:\nInitialization: Start by assigning each data point to its own cluster. Merge Step: At each step, merge the two clusters that are closest together, based on the distance calculation method chosen. Update Distances: Recalculate the distances between the new cluster and each of the old clusters. Repeat: Continue merging clusters until all data points are merged into a single cluster or until a desired number of clusters is reached. These different linkage criteria can significantly impact the shapes and sizes of the clusters formed. A nice demonstration in various clustering scenarios can be found on the scikit-learn page, also shown below: (Figure credit: https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py)\nk-Means Clustering k-Means clustering partitions the data into $k$ mutually exclusive clusters, and returns the index of the cluster each point belongs to. This method aims to minimize the within-cluster sum of squares.\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) DBSCAN groups together closely packed points and marks points in low-density regions as outliers. This method does not require specifying the number of clusters a priori, making it suitable for data with irregular or complex structures.\nParameters: $\\mu$: Minimum number of points required to form a dense region. $\\varepsilon$: Specifies the \u0026ldquo;reach\u0026rdquo;, that is, the distance threshold within which points are considered neighbors. k-Nearest Neighbor (k-NN) k-Nearest Neighbors (k-NN) is primarily a classification technique renowned for its simplicity and effectiveness, especially suited for datasets where the decision boundaries between classes are not linear. The k-NN algorithm classifies new data points based on the majority vote of their k nearest neighbors in the feature space.\nTo apply k-NN effectively, data is typically split into two sets:\nTraining Set: This dataset is used to \u0026rsquo;train\u0026rsquo; or \u0026lsquo;fit\u0026rsquo; the model. It includes both the input features and the corresponding classification labels which are known. Test Set: This dataset is used solely for testing the performance of the trained model. It helps to evaluate how well the k-NN model generalizes to new, previously unseen data. Once the model has been trained on the training set, it can be used to predict the class labels of new data in the test set, providing a measure of its classification accuracy.\n","date":1597100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"63614f66db2e7d0ed20454ef1a77b485","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/6-clustering-and-classification/","publishdate":"2020-08-11T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/6-clustering-and-classification/","section":"tutorials","summary":"Clustering and classification are fundamental techniques in data analysis and machine learning, used to group data points based on similarities and to categorize them into distinct classes.\nHierarchical Clustering Hierarchical clustering builds nested clusters by progressively merging or splitting them based on the distance between data points or groups. The method relies heavily on the choice of distance calculation:\nSingle Linkage (Nearest Neighbor): This method, also known as the friend-of-a-friend technique, considers the shortest distance between points in two clusters (see the first plot below).","tags":null,"title":"Clustering and Classification","type":"docs"},{"authors":null,"categories":null,"content":"In the real world, data collection is often incomplete or constrained by various factors. To fully utilize available data, it is sometimes necessary to handle \u0026lsquo;censored\u0026rsquo; and \u0026rsquo;truncated\u0026rsquo; data, particularly in fields such as medical studies, reliability engineering, and astronomy.\nCensored Data Censored data occurs when the value of a measurement exists, but we only know that it falls above or below certain limits:\nLeft-Censored Data (Upper Limit): The actual data point is less than a certain value, but the exact value is unknown. This is common in astronomy; for example, a star\u0026rsquo;s luminosity might be below the detection limit of a telescope. We know only the upper limit of the star\u0026rsquo;s luminosity. Right-Censored Data (Lower Limit): The actual data point is greater than a certain value, but the exact value is unknown. Truncated Data Truncated data occurs when data points below or above a certain threshold are not just unknown but completely absent from the dataset. Unlike censoring, with truncation, we do not have any information that data points exist outside of the observed range.\nThese characteristics have implications for statistical analysis, including distribution function estimation, correlation analysis, regression modeling, and hypothesis testing.\nSurvival and Hazard Functions in Censoring Censoring techniques often use concepts from survival analysis:\nSurvival Function ($S(x)$): Represents the probability that a variable $X$ exceeds a certain value $x$. $$ S(x) = P(X \u0026gt; x) = 1 - F(x) $$ Hazard Rate ($h(x)$): Represents the conditional failure rate at a certain value. $$ h(x) = \\frac{f(x)}{S(x)} $$ Here, $f(x)$ is the PDF at $x$, and the hazard rate can be interpreted as the likelihood of an event occurring at $x$ given that it has not occurred before $x$. Clarifying the Example on Hazard Rate Consider an example where:\n$f(95) = 1\\%$: The probability of dying at the age of 95 is 1%. $S(95) = 2\\%$: The probability of surviving past the age of 95 is 2%. To find the hazard rate $h(x)$ at age 95: $$ h(95) = \\frac{0.01}{0.02} = 0.5 \\text{ or } 50\\% $$ This indicates that, given reaching age 95, there is a 50% chance of dying at that age.\nAdvanced Estimators for Censored and Truncated Data Kaplan-Meier Nonparametric Estimator (Censored): The Kaplan-Meier estimator is crucial for analyzing survival data, particularly in medical research. It measures the fraction of subjects living for a certain amount of time after treatment. This estimator is particularly effective in handling right-censored data, where the survival time is only known to exceed a certain duration but the exact time of event (e.g., death, failure) is unknown. The Kaplan-Meier estimator uses the available data to estimate the survival function, $S(t)$, which provides insights into the likelihood of survival beyond observed time points. Below is an example plot of the KM estimated survival curve of synthetic data on stellar luminosity data. Figure data credit: Sanya Arora\nLynden-Bell–Woodroofe Estimator (Truncated): Commonly utilized in astronomical studies, the Lynden-Bell–Woodroofe estimator addresses issues with truncated samples, where observations below or above certain thresholds are missing entirely from the dataset. This estimator operates under the assumption that all observations derive from the same underlying distribution. It uses the observed distribution of the available data to estimate the distribution functions of the truncated segments, facilitating a more comprehensive understanding of the overall data distribution. Both estimators are tailored to specific types of incomplete data: Kaplan-Meier for censored data, where some information about the survival time is available, and Lynden-Bell–Woodroofe for truncated data, where parts of the data are completely missing. Understanding their applications and differences is essential for accurately analyzing datasets characterized by incomplete observations.\n","date":1714604400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714707060,"objectID":"63c5d1cf2d3c39464ac118aa1a45df23","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/7-censored-and-truncated-data/","publishdate":"2024-05-02T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/7-censored-and-truncated-data/","section":"tutorials","summary":"In the real world, data collection is often incomplete or constrained by various factors. To fully utilize available data, it is sometimes necessary to handle \u0026lsquo;censored\u0026rsquo; and \u0026rsquo;truncated\u0026rsquo; data, particularly in fields such as medical studies, reliability engineering, and astronomy.\nCensored Data Censored data occurs when the value of a measurement exists, but we only know that it falls above or below certain limits:\nLeft-Censored Data (Upper Limit): The actual data point is less than a certain value, but the exact value is unknown.","tags":null,"title":"Nondetections-Censored and Truncated Data","type":"docs"},{"authors":null,"categories":null,"content":"Evenly Spaced Time Series Data Autocorrelation Function (ACF) The Autocorrelation Function (ACF) measures the correlation between a time series and a lagged version of itself over various time intervals. It\u0026rsquo;s akin to performing a Pearson correlation test between the original time series data and the same data shifted by a lag time $k$. Notably, ACF(k=0) always equals 1, reflecting perfect self-correlation at zero lag.\nPartial Autocorrelation Function (PACF) The Partial Autocorrelation Function (PACF) quantifies the correlation between observations in a time series separated by $k$ time units, specifically adjusting for the correlations at shorter lags. This adjustment helps isolate the direct influence of past data points on the current observation:\nAt lag 1, PACF equals ACF as there are no previous terms to adjust for. At lag 2, PACF assesses the correlation between points two units apart (e.g., $X_i$ - $X_{i+2}$), adjusting for the influence of the intervening lag (e.g., corelation between $X_i$ - $X_{i+1}$ and $X_{i+1}$ - $X_{i+2}$). PACF is crucial for model building, especially in autoregressive models, as it helps identify the effective number of past observations (lags) that significantly influence future values.\nAutoregressive (AR) Model In an AR model, each point in the series is modeled as a linear combination of its previous values: $$ X_t = \\alpha_1 X_{t-1} + \\alpha_2 X_{t-2} + \\cdots + \\alpha_p X_{t-p} + \\varepsilon_t $$ where $\\alpha_i$ are coefficients to be estimated and $\\varepsilon_t$ is white noise.\nMoving Average (MA) Model An MA model expresses each point in the series as a linear combination of past noise terms: $$ X_t = \\varepsilon_t + \\beta_1 \\varepsilon_{t-1} + \\beta_2 \\varepsilon_{t-2} + \\cdots + \\beta_q \\varepsilon_{t-q} $$ where $\\varepsilon_t = N(0, \\sigma^2)$ and $\\beta_i$ are coefficients to be estimated.\nAutoregressive Moving Average (ARMA) Model The ARMA $(p,q)$ model effectively combines autoregressive (AR) and moving average (MA) components: $$ X_t = \\alpha_1 X_{t-1} + \\cdots + \\alpha_p X_{t-p} + \\varepsilon_t + \\beta_1 \\varepsilon_{t-1} + \\cdots + \\beta_q \\varepsilon_{t-q} $$ In this model, the parameters $p$ and $q$ denote the orders of the AR and MA components, respectively. These parameters are crucial as they determine the \u0026ldquo;memory\u0026rdquo; length of the model, indicating how far back in time the data\u0026rsquo;s dependencies and shocks influence the current observation.\nAutoregressive Integrated Moving Average (ARIMA) Model Building on the ARMA model, the ARIMA $(p,d,q)$ model incorporates an additional differencing component to render non-stationary time series data stationary. This adjustment is necessary for dealing with underlying trends or seasonality:\nIn ARIMA, $d$ represents the degree of differencing required to flatten trends in the data, addressing long-term drift and ensuring stationarity. The parameters $p$ and $q$ continue to represent the AR and MA orders, respectively. ARIMA models are extensively used not only in modeling but also in forecasting scenarios, such as predicting financial market trends or stock prices.\nFourier Power Spectrum Fourier analysis converts time series data into the frequency domain, providing insights into periodicity and cyclic patterns within the series.\nUnevenly Spaced Time Series Data Lomb-Scargle Periodogram (LSP) The LSP method extends Fourier analysis to unevenly spaced time series data, facilitating the detection of periodic signals. It also has another form that use leat-squares regression of the dataset to sine and cosine waves in a range of frequencies. A common form is: $$ P_{LS}(\\nu) = \\frac{1}{2\\sigma^2} \\left[ \\frac{\\left(\\sum_{i=1}^n X_i \\cos(2\\pi \\nu t_i)\\right)^2}{\\sum_{i=1}^n \\cos^2(2\\pi \\nu (t_i - \\tau(\\nu)))} + \\frac{\\left(\\sum_{i=1}^n X_i \\sin(2\\pi \\nu t_i)\\right)^2}{\\sum_{i=1}^n \\sin^2(2\\pi \\nu (t_i - \\tau(\\nu)))} \\right] $$ where $\\tau$ is defined as $$ \\tan(4\\pi \\nu \\tau) = \\frac{\\sum_{i=1}^n \\sin(4\\pi \\nu t_i)}{\\sum_{i=1}^n \\cos(4\\pi \\nu t_i)} $$\nOther Analysis Techniques Wavelet Analysis Fourier Transform (FT) is a powerful tool for analyzing frequency components within a time series. However, a key limitation of a single FT is its lack of temporal resolution: it provides frequency information ($\\nu$) but no insight into when these frequencies occur, especially if the data is non-stationary—meaning the frequency content changes over time.\nProblem with Standard Fourier Transform For instance, if a time series exhibits seasonal variations, the frequency of these variations might change over different time intervals. A simple FT would average these changes across the entire dataset, potentially obscuring meaningful patterns.\nSolution via Wavelet Analysis Wavelet analysis addresses this limitation by allowing the examination of different frequencies at different times. This is achieved by dividing the time series into segments and analyzing each segment separately—a process that optimizes the extraction of temporal information at various frequencies.\nVisualizing Wavelet Transforms A wavelet plot typically displays frequency space ($\\nu$) versus time ($t$). It offers a detailed view where, at shorter periods on the y-axis, there is higher time resolution, and conversely, longer periods offer broader frequency insights but lower time specificity. The parabolic shapes in the plot, often shaded, mark the boundary edge effects, indicating where data becomes less reliable due to edge artifacts.\n(Credit: ResearchGate)\nWavelets Versus Fourier Transforms Instead of using Fourier transforms, wavelet analysis involves convoluting the time-sliced data with a wavelet function. This method is more effective because it adjusts to the localized variations in the time series through the use of variable-sized \u0026lsquo;windows\u0026rsquo;—smaller windows for higher frequencies and larger ones for lower frequencies.\nCommon Wavelets Commonly used wavelet functions include the Morlet and Daubechies wavelets, each with specific characteristics that make them suitable for different types of data analysis tasks.\n(Credit: ResearchGate)\nMathematical Expression The mathematical expression for a wavelet transform is given by: $$ W[f(\\lambda,t)] = \\int_{-\\infty}^{\\infty} f(\\nu) \\frac{1}{\\sqrt{\\lambda}} \\overline{\\phi}\\left(\\frac{u-t}{\\lambda}\\right) du $$ where $\\lambda$ is the scaling factor that stretches or compresses the wavelet, adapting it to the signal\u0026rsquo;s local characteristics.\nWavelet analysis thus provides a more flexible and nuanced approach to understanding time series data, especially when the signal contains non-stationary or frequency-varying components.\nNyquist Frequency The Nyquist frequency $\\nu_N = 1/(2 \\Delta t)$ is the maximum frequency that can be effectively captured by the data, where $\\Delta t$ is the sampling interval. It represents the boundary beyond which the sampling rate is insufficient to capture detailed variations in the data.\n","date":1714690800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714755098,"objectID":"7b5f25942538c9cc546057fbf01c2439","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/8-timeseries-analysis/","publishdate":"2024-05-03T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/8-timeseries-analysis/","section":"tutorials","summary":"Evenly Spaced Time Series Data Autocorrelation Function (ACF) The Autocorrelation Function (ACF) measures the correlation between a time series and a lagged version of itself over various time intervals. It\u0026rsquo;s akin to performing a Pearson correlation test between the original time series data and the same data shifted by a lag time $k$. Notably, ACF(k=0) always equals 1, reflecting perfect self-correlation at zero lag.\nPartial Autocorrelation Function (PACF) The Partial Autocorrelation Function (PACF) quantifies the correlation between observations in a time series separated by $k$ time units, specifically adjusting for the correlations at shorter lags.","tags":null,"title":"Timeseries Analysis","type":"docs"},{"authors":null,"categories":null,"content":"Introduction to Variograms A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: \u0026ldquo;nugget,\u0026rdquo; \u0026ldquo;sill,\u0026rdquo; and \u0026ldquo;range.\u0026rdquo;\nNugget: Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling. Sill: The plateau reached by the variogram, beyond which the increments in distance do not significantly increase the variance. It represents the level of total variance within the dataset. Range: The distance at which the variogram reaches the sill, beyond which locations are no longer correlated. Example of Spatial Data and Variogram Plots below show an example of spatial data (left) and its associated variogram (right). The plot on the left shows synthetic data in spatial X-Y coordinates color-coded by the level of toxicity measured at that location. The size of the circle is associated with the measurement error, which is not used here. The variogram on the right shows how the semi-variance between points increases with distance. It features a \u0026ldquo;nugget\u0026rdquo; effect at the origin, indicating measurement noise or microscale variability. The curve approaches a \u0026ldquo;sill,\u0026rdquo; beyond which the variance stabilizes, suggesting that points beyond this \u0026ldquo;range\u0026rdquo; do not influence each other. This range is critical for understanding the spatial continuity and predicting values at unsampled locations.\nFigure data credit: Charles M Pacheco\nExample R Code for the Variogram library(gstat) library(sp) # Defining spatial coordinates # df_2 is a dataframe with colums x and y coordinates(df_2) \u0026lt;- ~x+y # Creating Variograms variogram_tox \u0026lt;- variogram(toxicity ~ 1, df_2) # Fit the variogram and plot it out. # gamma: the semi-variance # vgm: \u0026quot;variogram model,\u0026quot; model_tox \u0026lt;- fit.variogram( variogram_tox, model = vgm(psill = max(variogram_tox$gamma), model = \u0026quot;Sph\u0026quot;, range = 30)) # Plot the empirical variogram and the fitted model plot(variogram_tox, model = model_tox, main = \u0026quot;Toxicity Variogram with Model\u0026quot;) ","date":1714518000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"a6cd38ef97ae86ba6fa0fdfcfb697193","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/9-1-variogram/","publishdate":"2024-05-01T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/9-1-variogram/","section":"tutorials","summary":"Introduction to Variograms A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: \u0026ldquo;nugget,\u0026rdquo; \u0026ldquo;sill,\u0026rdquo; and \u0026ldquo;range.\u0026rdquo;\nNugget: Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling.","tags":null,"title":"Variogram","type":"docs"},{"authors":null,"categories":null,"content":"Introduction to Kriging Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:\nSimple Kriging: Assumes the mean of the random field is known and constant throughout the region of interest. Ordinary Kriging: Assumes the mean is unknown but constant within the region of interest and is the most commonly used form as one do not know the mean in real world. More on Variogram The semivariance $\\gamma(x_1, x_2)$ between two points can be expressed as: $$ \\gamma(d) = \\frac{1}{2n} \\sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 $$ where $n$ is the number of pairs, $d$ is the distance between two points, and $z$ represents the values at the locations. The calculation of $\\gamma(d)$ involves several key assumptions:\nStationary (homogeneous): Assumes that the statistical properties (mean, variance) of the process do not change over space. This implies that the mean and variance are constant throughout the region of interest, and the covariance between any two points depends only on the distance and direction between them, not on their absolute locations. Isotropy: Assumes that the statistical properties are the same in all directions. This means that the variogram is a function only of the distance between sample points, not of the direction. In Kriging, we use $\\gamma$ to weight the data for interpolation:\n$$\\begin{align} \\gamma(d) \u0026amp;= \\frac{1}{2n} \\sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 \\\\ \u0026amp;= \\frac{1}{2} E\\left(\\left[z(x+d) - z(x)\\right]^2\\right) \\text{(homogeneous assumption)} \\\\ \u0026amp;= \\frac{1}{2} \\left\\{ E\\left[ z^2(x+d) \\right] + E\\left[ z^2(x) \\right] - 2E\\left[ z(x+d) z(x) \\right]\\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\left\\{ 2E\\left[ z^2(x) \\right] - 2E\\left[ z(x+d) z(x) \\right]\\right\\} \\text{(homogeneous assumption)} \\\\ \u0026amp;= \\sigma_x^2 + \\mu_x^2 - \\text{cov}[z(x), z(x+d)] - \\mu_x^2 \\end{align}$$\nwhere the relationships: $E[x^2] = \\sigma_x^2 + [E(x)]^2$ and $\\text{cov}[X,Y] = E(XY) - E(X)E(Y)$ were used to get the second to last equation. We have: $$\\begin{align} \\gamma(d) \u0026amp;= \\sigma^2 - \\text{cov}[z(x), z(x+d)] \\\\ \u0026amp;= \\sigma^2 - \\text{c}(d) \\end{align}$$\nTherefore, we have the semivariance at distance $d$ is the variance minus the covariance between points at this distance.\nExample of Spatial Data and the Kriging Result The plots below show an example of spatial data (left) and its associated Kriging map (right). The data used here is the same as in the Variogram page, and the Kriging map uses the variogram model shown on the Variogram page to predict values at unsampled locations. This example demonstrates how Kriging utilizes the spatial structure of the data, as defined by the variogram, to provide a statistically optimal interpolation.\nFigure data credit: Charles M Pacheco\nExample R Code for the Kriging # Fit a variogram model vgm_model \u0026lt;- vgm(psill = max(variogram_tox$gamma), model = \u0026quot;Sph\u0026quot;, range = 30) x.range \u0026lt;- range(df_2$x) y.range \u0026lt;- range(df_2$y) grid.points \u0026lt;- expand.grid( x = seq(from = x.range[1], to = x.range[2], by = 1), y = seq(from = y.range[1], to = y.range[2], by = 1)) # Convert to SpatialPoints grid \u0026lt;- SpatialPoints(grid.points) # Perform ordinary kriging kriged \u0026lt;- krige(toxicity ~ 1, df_2, model = vgm_model, newdata = grid) options(repr.plot.width=5, repr.plot.height=5, repr.plot.res=200) # Create a map with overlay contours spplot(kriged, \u0026quot;var1.pred\u0026quot;, main = \u0026quot;Kriging Map for Toxicity\u0026quot;, xlab = \u0026quot;X\u0026quot;, ylab = \u0026quot;Y\u0026quot;, sp.layout = list(\u0026quot;sp.points\u0026quot;, df_2, col = \u0026quot;green\u0026quot;), colorkey = TRUE, scales = list(draw = TRUE)) ","date":1714518000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714701184,"objectID":"b2c29b107f3564daf29be07c2ee6925b","permalink":"https://shihyuntang.github.io/tutorials/astro-stat/9-2-kriging/","publishdate":"2024-05-01T00:00:00+01:00","relpermalink":"/tutorials/astro-stat/9-2-kriging/","section":"tutorials","summary":"Introduction to Kriging Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:\nSimple Kriging: Assumes the mean of the random field is known and constant throughout the region of interest. Ordinary Kriging: Assumes the mean is unknown but constant within the region of interest and is the most commonly used form as one do not know the mean in real world.","tags":null,"title":"Kriging","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575898820,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://shihyuntang.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Shih-Yun Tang","Christopher M. Johns-Krull","L. Prato","Asa G. Stahl"],"categories":null,"content":" Image credit: Figure 7 in Tang et al. 2024.: The empirical Teff vs. equivalent width ratio plot.\n","date":1718841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718992329,"objectID":"e709928db2399a70476ab8bc61f51dcc","permalink":"https://shihyuntang.github.io/publication/202406_ewr_obs_apj/","publishdate":"2024-06-20T00:00:00Z","relpermalink":"/publication/202406_ewr_obs_apj/","section":"publication","summary":"The Astrophysical Journal (in press.)","tags":["Exoplanet","Stellar activity"],"title":"Measuring the Spot Variability of T Tauri Stars Using Near-IR Atomic Fe and Molecular OH Lines","type":"publication"},{"authors":["Xiaoying Pang","Yifan Wang","Shih-Yun Tang","Yicheng Rui","Jing Bai","Chengyuan Li","Fabo Feng","M.B.N. Kouwenhoven","Wen-Ping Chen","Rwei-ju Chuang"],"categories":null,"content":" Image credit: Figure 5 in Pang et al. 2023.: Box plot showing radial binary fraction as a function of cluster-centric distance as half-mass radius (rh). For each panel shows four types of clusters: (a) filamentary , (b) fractal, (c) halo , and (d) tidal-tail.\n","date":1689465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"ec50ffb373be165bfa1a1775b06e3ca3","permalink":"https://shihyuntang.github.io/publication/202307_binaryetar_evo_env_aj/","publishdate":"2023-07-16T00:00:00Z","relpermalink":"/publication/202307_binaryetar_evo_env_aj/","section":"publication","summary":"The Astronomical Journal 166:110 (16pp)","tags":["Open Cluster"],"title":"Binary Star Evolution in Different Environments: Filamentary, Fractal, Halo and Tidal-tail Clusters","type":"publication"},{"authors":["Shih-Yun Tang","Asa G. Stahl","L. Prato","G. H. Schaefer","Christopher M. Johns-Krull","Brian A. Skiff","Charles A. Beichman","Taichi Uyama"],"categories":null,"content":" Image credit: Figure 2 in Tang et al. 2023.: Best model fit for DITau AB\n","date":1679011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"47b002fc13ba789cfc25b0e6d7b0c450","permalink":"https://shihyuntang.github.io/publication/202303_ditau_apj/","publishdate":"2023-03-17T00:00:00Z","relpermalink":"/publication/202303_ditau_apj/","section":"publication","summary":"The Astrophysical Journal 950:92 (17pp)","tags":["Binary Star","DI Tau"],"title":"Star-Crossed Lovers DI Tau A and B: Orbit Characterization and Physical Properties Determination","type":"publication"},{"authors":["Xiaoying Pang","Yuqian Li","Shih-Yun Tang","Long Wang","Yanshu Wang","Zhaoyu Li","Danchen Wang","M.B.N. Kouwenhoven","Mario Pasquato"],"categories":null,"content":" Image credit: Figure 3 in Pang et al. 2022.: 3D morphology of the Collinder 132-Gulliver 21 stream. The oldest population surrounded by young populations can be seen in panel (b) with age of each group labeled.\n","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"77d26019f57e0feb613c75f578bf1365","permalink":"https://shihyuntang.github.io/publication/202209_col132gul21_apjl/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/202209_col132gul21_apjl/","section":"publication","summary":"The Astrophysical Journal Letter 937:L7 (11pp)","tags":["Open Cluster"],"title":"Dynamical Origin for the Collinder 132-Gulliver 21 Stream: A Mixture of three Co-Moving Populations with an Age Difference of 250 Myr","type":"publication"},{"authors":["Xiaoying Pang","Shih-Yun Tang","Yuqian Li","Zeqiu Yu","Long Wang","Jiayu Li","Yezhang Li","Yifan Wang","Yanshu Wang","Teng Zhang","Mario Pasquato","M.B.N. Kouwenhoven"],"categories":null,"content":" Image credit: Figure 3 in Pang et al. 2022.: 3D morphology of 85 open clusters in the solar neighborhood with the color of the cluster scaled with the logarithm of age. An interactive version of this figure is available at http://3doc-morphology.lowell.edu\n","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"15cbc3c1fd5d1278ff9dd74993d0ad48","permalink":"https://shihyuntang.github.io/publication/202204_oc3dii_apj/","publishdate":"2022-06-06T00:00:00Z","relpermalink":"/publication/202204_oc3dii_apj/","section":"publication","summary":"The Astrophysical Journal 931:156 (25pp)","tags":["Open Cluster"],"title":"3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3 II: Hierarchical Star Formation Revealed by Spatial and Kinematic Substructures","type":"publication"},{"authors":["Xiaoying Pang","Zeqiu Yu","Shih-Yun Tang","Jongsuk Hong","Zhen Yuan","Mario Pasquato","M.B.N. Kouwenhoven"],"categories":null,"content":" Image credit: Figure 8 in Pang et al. 2021.: IRAS-IRIS infrared image of the 60 $\\mu m$ band. The members of Huluwa 1\u0026ndash;5, and the cluster pair Collinder135 and UBC7 are displayed as colored dots.\n","date":1638921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"e825f7be2a730a71ffdb1b97c32e7fa7","permalink":"https://shihyuntang.github.io/publication/202109_velob_apj/","publishdate":"2021-12-08T00:00:00Z","relpermalink":"/publication/202109_velob_apj/","section":"publication","summary":"The Astrophysical Journal 923:20 (22pp)","tags":["Open Cluster"],"title":"Disruption of Hierarchical Clustering in the Vela OB2 Complex and the Cluster Pair Collinder 135 and UBC 7 with Gaia EDR3: Evidence of Supernova Quenching","type":"publication"},{"authors":["Shih-Yun Tang","Tyler D. Robinson","Mark S. Marley","Natasha E. Batalha","Roxana Lupu","L. Prato"],"categories":null,"content":" Image credit: Figure 1 in Tang et al. 2021.: Thermal structure profiles from converged solutions of the radiative-convective model at logg = 4.0 for YB ([M/H] of 0.0, here) and YP ([M/H] of 0.7 and 1.5) cases.\n","date":1637366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"ded01d22ed6ee3bd108a906e7d8c6dc9","permalink":"https://shihyuntang.github.io/publication/202105_ydwarf_moist_apj/","publishdate":"2021-11-15T00:00:00Z","relpermalink":"/publication/202105_ydwarf_moist_apj/","section":"publication","summary":"The Astrophysical Journal 922:26 (12pp)","tags":["Brown Dwarf","Planet"],"title":"Impacts of Water Latent Heat on the Thermal Structure of Ultra-Cool Objects: Brown Dwarfs and Free-Floating Planets","type":"publication"},{"authors":["Shih-Yun Tang","Asa G. Stahl","Christopher M. Johns-Krull","L. Prato","Joe Llama"],"categories":null,"content":"","date":1623283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"d0f14ff813f56894933c1687097d9837","permalink":"https://shihyuntang.github.io/publication/202104_igrinsrv_joss/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/publication/202104_igrinsrv_joss/","section":"publication","summary":"Journal of Open Sourse Softerware 6:62","tags":["radial velocities","IGRINS"],"title":"IGRINS RV: A Python Package for Precision Radial Velocities with Near-Infrared Spectra","type":"publication"},{"authors":["Asa G. Stahl","Shih-Yun Tang","Christopher M. Johns-Krull","L. Prato","Joe Llama","Gregory N. Mace","Jae Joon Lee","Heeyoung Oh","Jessica Luna","Daniel T. Jaffe"],"categories":null,"content":" Image credit: Figure 5 in Stahl et al. 2021.: Final RVs for RV standard stars\n","date":1621987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"a1646d27e783db138a1aa6ada94fdcbe","permalink":"https://shihyuntang.github.io/publication/202104_igrinsrv_aj/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/publication/202104_igrinsrv_aj/","section":"publication","summary":"The Astronomical Journal 161:283 (26pp)","tags":["radial velocities","IGRINS","planets and satellites detection"],"title":"IGRINS RV: A Precision RV Pipeline for IGRINS Using Modified Forward-Modeling in the Near-Infrared","type":"publication"},{"authors":["Xiaoying Pang","Yuqian Li","Zeqiu Yu","Shih-Yun Tang","František Dinnbier","Pavel Kroupa","Mario Pasquato","M.B.N. Kouwenhoven"],"categories":null,"content":" Image credit: Figure 8 in Pang et al. 2021.: Elipsoid fitting for the 3D spatial positions for the cluster members within tidal radius of NGC 2516, after distance correction through a Bayesian approach (see Section 4.1).\n","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"584421564f285ff8dad3f65a88c109fb","permalink":"https://shihyuntang.github.io/publication/202102_oc3d_apj/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/publication/202102_oc3d_apj/","section":"publication","summary":"The Astrophysical Journal 912:162 (27pp)","tags":["Open Cluster"],"title":"3D Morphology of Open Clusters in the Solar Neighborhood with Gaia EDR3: its Relation to Cluster Dynamics","type":"publication"},{"authors":["Xiaoying Pang","Yuqian Li","Shih-Yun Tang","Mario Pasquato","M.B.N. Kouwenhoven"],"categories":null,"content":" Image credit: Figure 5 in Pang et al. 2020.: Number density, mass density, and mean mass distributions along a clustercentric distance r for NGC 2232 (blue curves) and LP 2439 (red curves).\n","date":1596931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"588ee48530655dedf1721785f24c7bc1","permalink":"https://shihyuntang.github.io/publication/202006_ngc2232_apjl/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/publication/202006_ngc2232_apjl/","section":"publication","summary":"The Astrophysical Journal Letters 900:L4 (10pp)","tags":["Open Cluster","NGC 2232"],"title":"Different Fates of Young Star Clusters After Gas Expulsion","type":"publication"},{"authors":["Yu Zhang","Shih-Yun Tang","W. P. Chen","Xiaoying Pang","J. Z. Liu"],"categories":null,"content":" Image credit: Figure 4 in Zhang et al. 2020. (a) Top view of Blanco1; (b) PM plot\n","date":1580256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"b5dd8cc8eba076bba93a50bc9083619d","permalink":"https://shihyuntang.github.io/publication/202001_blanco1_apj/","publishdate":"2020-01-29T00:00:00Z","relpermalink":"/publication/202001_blanco1_apj/","section":"publication","summary":"The Astrophysical Journal 889:99 (12pp)","tags":["Open Cluster","Blanco 1","Tidal tails"],"title":"Diagnosing the Stellar Population and Tidal Structure of the Blanco1 Star Cluster","type":"publication"},{"authors":["Shih-Yun Tang","Xiaoying Pang","Zhen Yuan","W. P. Chen","Jongsuk Hong","Bertrand Goldman","Andreas Just","Bekdaulet Shukirgaliyev","Chien-Cheng Lin"],"categories":null,"content":" Image credit: Tang et al. 2019.: 3D spatial, proper motion, and radial velocity plot for Coma Ber \u0026amp; Group-X.\n","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"00adb46b40eaf701e9d7bb8c42a18dde","permalink":"https://shihyuntang.github.io/publication/201905_comaber_tail_apj/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/publication/201905_comaber_tail_apj/","section":"publication","summary":"The Astrophysical Journal 877:12 (12pp)","tags":["Open Cluster","Coma Ber","Tidal tails"],"title":"Discovery of Tidal Tails in Disrupting Open Clusters: Coma Berenices and a Neighbor Stellar Group","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575898820,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://shihyuntang.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Shih-Yun Tang","W. P. Chen","P. S. Chiang","Jessy Jose","Gregory J. Herczeg","Bertrand Goldman"],"categories":null,"content":" Image credit: Tang et al. 2018. Figure 17: The present day mass function (MF) of the ~800 Myr old Coma Ber star cluster.\nKey Takeaways The Mass Function (MF) of Coma Berenices peaks at 0.3 $M_\\odot$: The slope of the MF, $\\alpha$, is approximately 0.49 $\\pm$ 0.03 for the mass range 0.3 to 2.3 $M_{\\odot}$. The value of $\\alpha$ for Coma Berenices is lower than that of the Salpeter initial mass function, which is 2.35 for masses between 1\u0026ndash;10 $M_{\\odot}$ in the solar neighborhood. The value of $\\alpha$ for Coma Berenices is also lower than the present-day MF for field M dwarfs, which is approximately 1.3 for masses between 0.1\u0026ndash;0.7 $M_{\\odot}$ (Reid et al. 2002). ","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718936365,"objectID":"1d4364901b903f9999223a4950058d44","permalink":"https://shihyuntang.github.io/publication/201808_comber_apj/","publishdate":"2018-07-27T00:00:00Z","relpermalink":"/publication/201808_comber_apj/","section":"publication","summary":"The Astrophysical Journal 862:106 (28pp)","tags":["Open Cluster","Coma Ber","Brown Dwarf"],"title":"Characterization of Stellar and Substellar Members in the Coma Berenices Star Cluster","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592781206,"objectID":"b1cabbde5c01279ade2d383eb54a27e6","permalink":"https://shihyuntang.github.io/project_example/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_example/external-project/","section":"project_example","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project_example"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592781206,"objectID":"0df3ce072e63bc01dfa19af9d026ab1d","permalink":"https://shihyuntang.github.io/project_example/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project_example/internal-project/","section":"project_example","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project_example"}]