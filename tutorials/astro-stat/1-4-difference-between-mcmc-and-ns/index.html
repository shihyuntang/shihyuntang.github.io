<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.3">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shih-Yun Tang">

  
  
  
    
  
  <meta name="description" content="Both Markov Chain Monte Carlo (MCMC) and Nested Sampling (NS) are Bayesian inference techniques. They both rely on Bayes’ theorem, but they explore parameter space in fundamentally different ways. The biggest conceptual contrast is:
 MCMC samples in the posterior space. NS samples in the prior space, gradually restricting it to higher-likelihood regions.  Before digging into the differences, let’s recall Bayes’ theorem: $$ \text{Posterior}(\mathcal{P}) = \frac{ \text{Likelihood}(\mathcal{L}) \times \text{Prior($\pi$)}}{\text{Evidence($\mathcal{Z}$)}} $$ $$ \mathcal{P}(\theta|x) = \frac{ \mathcal{L}(x|\theta)\times \pi(\theta) }{\int \mathcal{L}(x|\theta) \pi(\theta) d\theta } $$">

  
  <link rel="alternate" hreflang="en-us" href="https://shihyuntang.github.io/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/">

  


  
  
  
  <meta name="theme-color" content="#ff704d">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://shihyuntang.github.io/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Shih-Yun Tang">
  <meta property="og:url" content="https://shihyuntang.github.io/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/">
  <meta property="og:title" content="Difference Between MCMC and NS | Shih-Yun Tang">
  <meta property="og:description" content="Both Markov Chain Monte Carlo (MCMC) and Nested Sampling (NS) are Bayesian inference techniques. They both rely on Bayes’ theorem, but they explore parameter space in fundamentally different ways. The biggest conceptual contrast is:
 MCMC samples in the posterior space. NS samples in the prior space, gradually restricting it to higher-likelihood regions.  Before digging into the differences, let’s recall Bayes’ theorem: $$ \text{Posterior}(\mathcal{P}) = \frac{ \text{Likelihood}(\mathcal{L}) \times \text{Prior($\pi$)}}{\text{Evidence($\mathcal{Z}$)}} $$ $$ \mathcal{P}(\theta|x) = \frac{ \mathcal{L}(x|\theta)\times \pi(\theta) }{\int \mathcal{L}(x|\theta) \pi(\theta) d\theta } $$"><meta property="og:image" content="https://shihyuntang.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://shihyuntang.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2024-05-02T00:00:00&#43;01:00">
    
    <meta property="article:modified_time" content="2025-11-20T17:37:40-06:00">
  

  



  


  


  





  <title>Difference Between MCMC and NS | Shih-Yun Tang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Shih-Yun Tang</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Shih-Yun Tang</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/tutorials/"><span>Tutorials and Notes</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





  




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  

  
  
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/1-1-some-basic/">1 Probability</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/1-1-some-basic/">Probability Basic</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/1-2-probability/">Distributions</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/1-3-bayesian/">Bayes&#39; theorem</a>
      </li>
      
      <li class="active">
        <a href="/tutorials/astro-stat/1-4-difference-between-mcmc-and-ns/">MCMC vs. NS</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/2-1-distribution/">2 Nonparametric</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/2-1-distribution/">Distribution</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/2-2-correlation/">Correlation</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/3-1-data-smoothing/">3 Data Smoothing</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/3-1-data-smoothing/">Data Smoothing</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/4-1-regression/">4 Regression</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/4-1-regression/">Regression</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/5-1-multivariate-analysis/">5 Multivariate Analysis</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/5-1-multivariate-analysis/">Multivariate Analysis</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/6-clustering-and-classification/">6 Clustering and Classification</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/6-clustering-and-classification/">Clustering and Classification</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/7-censored-and-truncated-data/">7 Censored and Truncated Data</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/7-censored-and-truncated-data/">Censored and Truncated Data</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/8-timeseries-analysis/">8 Timeseries Analysis</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/8-timeseries-analysis/">Timeseries Analysis</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/9-1-variogram/">9 Spatial Point Processes</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/9-1-variogram/">Variogram</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/9-2-kriging/">Kriging</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</a></li>
    <li><a href="#nested-sampling-ns">Nested Sampling (NS)</a></li>
    <li><a href="#more-about-ns">More About NS</a>
      <ul>
        <li><a href="#evidence-mathcalz">Evidence ($\mathcal{Z}$)</a></li>
        <li><a href="#unnormalized-posterior-the-weight-w_i">Unnormalized Posterior (the weight $w_i$)</a></li>
        <li><a href="#posterior-mathcalp">Posterior ($\mathcal{P}$)</a></li>
      </ul>
    </li>
    <li><a href="#ns-in-the-python-dynesty-package">NS in the Python <code>dynesty</code> Package</a></li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          <h1>Difference Between MCMC and NS</h1>

          <div class="article-style">
            <p>Both <strong>Markov Chain Monte Carlo (MCMC)</strong> and <strong>Nested Sampling (NS)</strong> are Bayesian inference techniques. They both rely on Bayes’ theorem, but they explore parameter space in fundamentally different ways. The biggest conceptual contrast is:</p>
<ul>
<li><strong>MCMC samples in the <em>posterior</em> space</strong>.</li>
<li><strong>NS samples in the <em>prior</em> space</strong>, gradually restricting it to higher-likelihood regions.</li>
</ul>
<p>Before digging into the differences, let’s recall Bayes’ theorem:
$$
\text{Posterior}(\mathcal{P}) = \frac{
\text{Likelihood}(\mathcal{L}) \times \text{Prior($\pi$)}}{\text{Evidence($\mathcal{Z}$)}}
$$
$$
\mathcal{P}(\theta|x)
= \frac{
\mathcal{L}(x|\theta)\times \pi(\theta)
}{\int \mathcal{L}(x|\theta) \pi(\theta) d\theta
}
$$</p>
<hr>
<h2 id="markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</h2>
<p>MCMC focuses directly on sampling from the <strong>posterior distribution</strong>. The idea is simple:</p>
<ol>
<li><strong>Start with an initial guess</strong> for the parameters ($\theta$) for each <em>walker</em>.</li>
<li>For each proposed step, compute
<ul>
<li>the likelihood: $\mathcal{L}(\theta) = P(x|\theta)$</li>
<li>the prior: $\pi(\theta) = P(\theta)$</li>
</ul>
</li>
<li>Combine them to get the <strong>unnormalized posterior</strong>:
$$
\mathcal{P}(\theta|x) \propto \mathcal{L}(\theta)\pi(\theta)
$$
(We ignore the evidence because it’s just a constant normalization factor.)</li>
<li>Use this posterior to decide whether the walker accepts or rejects the proposed move&hellip;</li>
</ol>
<p>Over time, the chain spends more time in regions where the posterior is high. This produces samples distributed according to the true posterior — which is why we say that <strong>MCMC samples from the posterior</strong>.</p>
<p>MCMC is excellent for <strong>parameter estimation</strong>, but it does not naturally compute the <strong>evidence</strong>, which is required for comparing between different models.</p>
<hr>
<h2 id="nested-sampling-ns">Nested Sampling (NS)</h2>
<p>Nested Sampling takes a very different approach from MCMC.<br>
Instead of wandering around the posterior, NS <strong>systematically explores the prior</strong>, gradually shrinking it toward regions of higher likelihood.</p>
<p>The basic flow looks like this:</p>
<ol>
<li>
<p><strong>Initialize a set of random parameters</strong> (called <em>live points</em>)<br>
These are drawn directly from the prior distribution $\pi(\theta)$.</p>
</li>
<li>
<p>For each live point, compute the likelihood:<br>
$$
\mathcal{L}(\theta) = P(x \mid \theta).
$$</p>
</li>
<li>
<p><strong>Identify the live point with the lowest likelihood</strong> and remove it.<br>
This becomes a <em>dead point</em> (a point we won&rsquo;t revisit, but it <em>will</em> help build the posterior).</p>
</li>
<li>
<p><strong>Replace it</strong> with a new sample drawn from the prior, <strong>subject to a constraint:</strong><br>
the new point must have a likelihood higher than the one just removed.<br>
This gradually forces the live points into smaller and smaller regions of higher likelihood.</p>
</li>
<li>
<p>We repeat steps 3–4 many times. Eventually, the remaining prior volume becomes so small (or the change in the evidence becomes negligible) that continuing does not help, and we stop.</p>
</li>
</ol>
<p>And this is why we say that <strong>NS samples from the prior</strong>!</p>
<hr>
<h2 id="more-about-ns">More About NS</h2>
<p>At the stopping point (after step 5), what do we have?</p>
<ul>
<li>A list of <strong>dead points</strong> $\theta_i$ with likelihoods<br>
$$
\mathcal{L}_i = \mathcal{L}(\theta_i).
$$</li>
<li>An estimate of the <strong>remaining prior volume</strong> $X_i$ at each iteration,<br>
which tells us how much of the prior space is left when that point is removed.</li>
</ul>
<p>To compute the posterior, we first need the normalization factor: the <strong>Evidence</strong>.</p>
<h3 id="evidence-mathcalz">Evidence ($\mathcal{Z}$)</h3>
<p>Recall that the evidence is:</p>
<p>$$
\mathcal{Z} = \int P(x \mid \theta)\ \pi(\theta)\ d\theta,
$$</p>
<p>the <strong>volume under the likelihood curve, weighted by the prior</strong>.</p>
<p>Directly computing this in many dimensions is hard.<br>
But NS cleverly tracks how the prior volume shrinks as we move to higher likelihoods.</p>
<p>Define the remaining prior volume as:</p>
<p>$$
X(\lambda) = \int_{\mathcal{L}(\theta) &gt; \lambda} \pi(\theta) d\theta,
$$</p>
<p>the fraction of prior volume where $\mathcal{L} &gt; \lambda$.</p>
<p>Skilling (2004) showed that the evidence ($\mathcal{Z}$) then becomes a <strong>1D integral</strong> over this volume:
$$
\mathcal{Z} = \int_0^1 \mathcal{L}(X)\ dX.
$$</p>
<p>NS approximates this integral by a sum over the dead points:</p>
<p>$$
Z \approx \sum_i L_i \ \Delta X_i
= \sum_i L_i (X_{i-1} - X_i).
$$</p>
<h3 id="unnormalized-posterior-the-weight-w_i">Unnormalized Posterior (the weight $w_i$)</h3>
<p>Each dead point represents a slice of the shrinking prior volume. The <strong>unnormalized posterior contribution</strong> for each dead point is:</p>
<p>$$
w_i = L_i\ (X_{i-1} - X_i),
$$
which reads as: <strong>likelihood of that slice × size of that slice in prior volume</strong>.</p>
<p>This automatically gives:
$$
\mathcal{Z} \approx \sum_i w_i.
$$</p>
<h3 id="posterior-mathcalp">Posterior ($\mathcal{P}$)</h3>
<p>Once we have the weights and the evidence, the posterior samples are simply:</p>
<p>$$
\mathcal{P}(\theta_i \mid x) = \frac{w_i}{\mathcal{Z}} = \frac{w_i}{\sum_i w_i}.
$$</p>
<p>That’s it — the dead points (with the right weights) become your posterior samples.</p>
<hr>
<h2 id="ns-in-the-python-dynesty-package">NS in the Python <code>dynesty</code> Package</h2>
<p>Here we connect the quantities discussed above to the actual outputs of a Nested Sampling
run using the Python package <strong>dynesty</strong>:</p>
<pre><code class="language-python">logl      = results.logl       # log-likelihoods of dead points
logwt     = results.logwt      # log-weights (unnormalized posterior weights)
logvol    = results.logvol     # log prior volumes of dead points
logz      = results.logz       # cumulative log-evidence estimates
</code></pre>
<p>All of these correspond directly to the core definitions of Nested Sampling.</p>
<p>Note, dynesty includes an extra final contribution from the last batch of
live points. That means:</p>
<ul>
<li><code>logwt[:-1]</code>: weights from dead points only</li>
<li><code>logwt[-1]</code>: contribution from remaining live points at termination</li>
</ul>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/tutorials/astro-stat/1-3-bayesian/" rel="next">Bayes&#39; theorem</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/tutorials/astro-stat/2-1-distribution/" rel="prev">Distribution</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on Nov 20, 2025</p>

          





  
  

<p class="edit-page">
  <a href="https://github.com/gcushen/hugo-academic/edit/master/content/tutorials/astro-stat/1-4%20Difference%20between%20MCMC%20and%20NS.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>




          

        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    © 2018-2025 Shih-Yun Tang &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.dab4f259dccb5050d23e41678cee70f2.js"></script>

    






  
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
