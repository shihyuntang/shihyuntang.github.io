<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.3">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shih-Yun Tang">

  
  
  
    
  
  <meta name="description" content="Principal Component Analysis (PCA) Procedure PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It&rsquo;s especially powerful when dealing with high-dimensional data. Here&rsquo;s how it works:
Step 1: Create the Data Matrix: Construct matrix $\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person). $$ \mathbf{d} = \begin{bmatrix} d_{11} &amp; d_{21} &amp; \cdots &amp; d_{t1} \\ d_{12} &amp; d_{22} &amp; \cdots &amp; d_{t2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ d_{1n} &amp; d_{2n} &amp; \cdots &amp; d_{tn} \end{bmatrix} $$">

  
  <link rel="alternate" hreflang="en-us" href="https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/">

  


  
  
  
  <meta name="theme-color" content="#ff704d">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Shih-Yun Tang">
  <meta property="og:url" content="https://shihyuntang.github.io/tutorials/astro-stat/5-1-multivariate-analysis/">
  <meta property="og:title" content="Multivariate Analysis | Shih-Yun Tang">
  <meta property="og:description" content="Principal Component Analysis (PCA) Procedure PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It&rsquo;s especially powerful when dealing with high-dimensional data. Here&rsquo;s how it works:
Step 1: Create the Data Matrix: Construct matrix $\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person). $$ \mathbf{d} = \begin{bmatrix} d_{11} &amp; d_{21} &amp; \cdots &amp; d_{t1} \\ d_{12} &amp; d_{22} &amp; \cdots &amp; d_{t2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ d_{1n} &amp; d_{2n} &amp; \cdots &amp; d_{tn} \end{bmatrix} $$"><meta property="og:image" content="https://shihyuntang.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://shihyuntang.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2024-05-02T00:00:00&#43;01:00">
    
    <meta property="article:modified_time" content="2024-05-03T18:08:50-05:00">
  

  



  


  


  





  <title>Multivariate Analysis | Shih-Yun Tang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Shih-Yun Tang</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Shih-Yun Tang</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/tutorials/"><span>Tutorials and Notes</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





  




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  

  
  
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/1-1-some-basic/">1 Probability</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/1-1-some-basic/">Probability Basic</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/1-2-probability/">Distributions</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/1-3-bayesian/">Bayes&#39; theorem</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/2-1-distribution/">2 Nonparametric</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/2-1-distribution/">Distribution</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/2-2-correlation/">Correlation</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/3-1-data-smoothing/">3 Data Smoothing</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/3-1-data-smoothing/">Data Smoothing</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/4-1-regression/">4 Regression</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/4-1-regression/">Regression</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/5-1-multivariate-analysis/">5 Multivariate Analysis</a>
    <ul class="nav docs-sidenav">
      
      <li class="active">
        <a href="/tutorials/astro-stat/5-1-multivariate-analysis/">Multivariate Analysis</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/6-clustering-and-classification/">6 Clustering and Classification</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/6-clustering-and-classification/">Clustering and Classification</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/7-censored-and-truncated-data/">7 Censored and Truncated Data</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/7-censored-and-truncated-data/">Censored and Truncated Data</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/8-timeseries-analysis/">8 Timeseries Analysis</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/8-timeseries-analysis/">Timeseries Analysis</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorials/astro-stat/9-1-variogram/">9 Spatial Point Processes</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorials/astro-stat/9-1-variogram/">Variogram</a>
      </li>
      
      <li >
        <a href="/tutorials/astro-stat/9-2-kriging/">Kriging</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a>
      <ul>
        <li><a href="#procedure">Procedure</a></li>
      </ul>
    </li>
    <li><a href="#independent-component-analysis-ica">Independent Component Analysis (ICA)</a>
      <ul>
        <li><a href="#understanding-the-context">Understanding the Context</a></li>
        <li><a href="#procedure-for-applying-ica">Procedure for Applying ICA</a></li>
        <li><a href="#example-application">Example Application</a></li>
      </ul>
    </li>
    <li><a href="#understanding-independence-non-gaussianity-and-the-central-limit-theorem">Understanding Independence, Non-Gaussianity, and the Central Limit Theorem</a>
      <ul>
        <li><a href="#implications-for-ica">Implications for ICA</a></li>
        <li><a href="#practical-considerations">Practical Considerations</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          <h1>Multivariate Analysis</h1>

          <div class="article-style">
            <!-- 
## Principle Component Analysis (PCA)

### Procedure

1. Make data matrix $\mathbf{d}$
2. Center data to form matrix $\mathbf{x}$
3. Construct covariance matrix $\mathbf{\Sigma_x}$ from $\mathbf{x}$
4. Find eigenvalues and eigenvectors of $\mathbf{\Sigma_x}$. The eigenvalues is are the variance and the eigenvectors is the PCA components.
5. Plot each data point in PCA space
6. Try to make sense to what each PCA vector physically means.

- **Step 1**
  The matrix $\mathbf{d}$ has it rows as attribute, like the properties of each targets, or people, etc. And in columns are for different targets or people for example. 
  $$
    \mathbf{d} = \begin{bmatrix}
    d_{11} & d_{21} & \cdots & d_{t1} \\\
    d_{12} & d_{22} & \cdots & d_{t2} \\\
    \vdots & \vdots & \ddots & \vdots \\\
    d_{1n} & d_{2n} & \cdots & d_{tn}
    \end{bmatrix}
  $$
- **Setp 2**
   Center data to form matrix $\mathbf{x}$ via 
   $$
    \mathbf{x} = \mathbf{d} - \mathbf{<\beta>}
   $$
   where $\mathbf{<\beta_j>} = \frac{1}{t} \sum_{j=1}^t d_{ji}$. This is basically means that for each attribute (each row) in $\mathbf{d}$, each values in it minus that attribute's mean value. So we have centered data 
  $$
    \mathbf{x} = \begin{bmatrix}
    x_{11} & x_{21} & \cdots & x_{t1} \\\
    x_{12} & x_{22} & \cdots & x_{t2} \\\
    \vdots & \vdots & \ddots & \vdots \\\
    x_{1n} & x_{2n} & \cdots & x_{tn}
    \end{bmatrix}
  $$
- **Setp 3**
  Construct covariance matrix $\mathbf{\Sigma_x}$ from $\mathbf{x}$. We have
  $$
    \frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T) = E\begin{bmatrix}
    \frac{x_{11}^2+x_{21}^2+\cdots+x_{t1}^2}{t} & \frac{x_{11}x_{12}+x_{21}x_{22}+\cdots+x_{t1}x_{t2}}{t} & \cdots \\\
    \frac{x_{11}x_{12}+x_{21}x_{22}+\cdots+x_{t1}x_{t2}}{t} & \frac{x_{12}^2+x_{22}^2+\cdots+x_{t2}^2}{t} & \cdots \\\
    \vdots & \vdots & \ddots
    \end{bmatrix}
  $$
  Because $\mathbf{x}$ has been centered, meaning that $E(x_i)^2 = 0$, thus $\sigma_{x_i}^2 = E(x_i^2)$. And also that $\sigma_{x_i,x_j} = E(x_ix_j)$. We have $\frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T)$ equals to the covariance matrix $\mathbf{\Sigma_x}$:
  $$
    \frac{1}{t}E( \mathbf{x} \cdot \mathbf{x}^T) = \mathbf{\Sigma_x} = \begin{bmatrix}
    \sigma_{1}^2 & \sigma_{12} & \cdots & \cdots \\\
    \sigma_{12} & \sigma_{2}^2 & \cdots & \cdots \\\
    \vdots & \vdots & \ddots & \vdots \\\
    \cdots & \cdots & \cdots & \sigma_{tn}^2
    \end{bmatrix}
  $$
- **Setp 4**
  Now, want to find a new axis that have a vectors which is at the direction that give is largest variance of the data. This new axis is then the PCA axes, or say the eigenvector of PC1. This new axis, is often made of linear combinations of wights $w_i$ $i = 1->n$ (where $n$ is the number of attributes). These weights will give a maximum $E(y_1^2)$ where 
  $$
    y_{1j} = w_{11}x_{j1} + w_{21}x_{j2} + \cdots + w_{n1}x_{jn}.
  $$
  where $j=1->t$, for data points. Note that $E(y_1)=0$, so $var(y_1)$ maximized. Now set:
  $$
    \vec{w_1} = \begin{bmatrix}
    w_{11}  \\\
    w_{21}  \\\
    \vdots  \\\
    w_{n1} 
    \end{bmatrix}
  $$ 
  and 
  $$
    \vec{w_1}^T = \begin{bmatrix}
    w_{11}  & w_{21} & \cdots & w_{n1} 
    \end{bmatrix}
  $$ 
  we get
  $$
    \vec{y_1}^T = \vec{w_1}^T \cdot \mathbf{x}
  $$
  Therefore, we get
  $$
    E(y_1^2) = \frac{1}{t}E(\vec{y_1}^T \cdot \vec{y_1}) 
              = \frac{1}{t}E(\vec{w_1}^T \cdot \mathbf{x} \cdot \mathbf{x}^T \cdot \vec{w_1} ) 
              = \vec{w_1}^T E\left( \frac{\mathbf{x} \cdot \mathbf{x}^T }{t} \right)  \vec{w_1}
              = \vec{w_1}^T \mathbf{\Sigma_x}  \vec{w_1}
  $$
  Then we just need to maximize $\vec{w_1}^T \mathbf{\Sigma_x}  \vec{w_1}$ with the constrain of $\vec{w_1}^T \cdot \vec{w_1} = 1$. This involves Lagrange multipliers which I will skip here (for now...). In the end one will get 
  $$
    (\Sigma_x - \lambda_1 I) \vec{w_1} = 0
  $$
  So, $w_1$ is an eigenvector of $\Sigma_x$, and that $\lambda_1$ is the eigenvalue. Want to maximize $\vec{w_1}^T \mathbf{\Sigma_x} \vec{w_1} = \vec{w_1}^T \lambda_1 \vec{w_1} = \lambda_1$. The first PCA component, PC1, is the one with the largest $\lambda$.

--- -->
<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<h3 id="procedure">Procedure</h3>
<p>PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It&rsquo;s especially powerful when dealing with high-dimensional data. Here&rsquo;s how it works:</p>
<ul>
<li>
<p><strong>Step 1: Create the Data Matrix</strong>:
Construct matrix $\mathbf{d}$ where each row represents an attribute (like properties of each target or person), and each column corresponds to different subjects or observations (e.g., targets or person).
$$
\mathbf{d} = \begin{bmatrix}
d_{11} &amp; d_{21} &amp; \cdots &amp; d_{t1} \\
d_{12} &amp; d_{22} &amp; \cdots &amp; d_{t2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
d_{1n} &amp; d_{2n} &amp; \cdots &amp; d_{tn}
\end{bmatrix}
$$</p>
</li>
<li>
<p><strong>Step 2: Center the Data</strong>:
Subtract the mean of each attribute from the corresponding values to center the data around the origin. This helps in aligning the PCA with the directions of maximum variance.
$$
\mathbf{x} = \mathbf{d} - \mathbf{\langle\beta\rangle}
$$
where $\mathbf{\langle\beta_j\rangle} = \frac{1}{t} \sum_{j=1}^t d_{ji}$ represents the mean of each attribute across all data points. The centered data matrix $\mathbf{x}$ looks like this:
$$
\mathbf{x} = \begin{bmatrix}
x_{11} &amp; x_{21} &amp; \cdots &amp; x_{t1} \\
x_{12} &amp; x_{22} &amp; \cdots &amp; x_{t2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{tn}
\end{bmatrix}
$$</p>
</li>
<li>
<p><strong>Step 3: Construct the Covariance Matrix</strong>:
The covariance matrix $\mathbf{\Sigma_x}$ is constructed from $\mathbf{x}$. Since $\mathbf{x}$ is centered, its expected value matrix $\mathbf{E(x)}$ is zero. The covariance matrix is then:</p>
<p>$$
\mathbf{\Sigma_x} = \frac{1}{t} \mathbf{x} \mathbf{x}^T
$$</p>
<p>This matrix captures the variance shared between the attributes, and its diagonal elements represent the variance of each attribute.</p>
</li>
<li>
<p><strong>Step 4: Eigenvalue Decomposition</strong>:
Eigenvalues and eigenvectors of the covariance matrix $\mathbf{\Sigma_x}$ are computed. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues represent the magnitude of the variance along each principal component.</p>
<p>To find the principal components:</p>
<ol>
<li>Maximize $\vec{w}^T \mathbf{\Sigma_x} \vec{w}$ subject to $\vec{w}^T \vec{w} = 1$.</li>
<li>This leads to solving $(\mathbf{\Sigma_x} - \lambda I) \vec{w} = 0$, where $\lambda$ is the eigenvalue.</li>
</ol>
<p>The principal component associated with the largest eigenvalue ($\lambda_1$) captures the most variance.</p>
</li>
<li>
<p><strong>Step 5: Transform Data to PCA Space</strong>
Each data point is then projected onto the PCA space using the eigenvectors, effectively reducing dimensionality while retaining the most significant variance features.</p>
</li>
<li>
<p><strong>Step 6: Interpretation</strong>
Interpret the physical meaning of each principal component, which might involve understanding how original attributes combine to form the component.</p>
</li>
</ul>
<hr>
<h2 id="independent-component-analysis-ica">Independent Component Analysis (ICA)</h2>
<p>Independent Component Analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents that are maximally independent. This technique is particularly useful in the field of blind source separation, where the goal is to separate a mixture of signals (like audio tracks) into their individual, independent components without prior knowledge about the source signals.</p>
<h3 id="understanding-the-context">Understanding the Context</h3>
<p>In applications like audio processing, ICA can be used to separate different speaking voices from a single audio track recorded with multiple voices overlapping. Each recording may capture the same set of voices but with varying amplitudes depending on the distance and orientation of each voice relative to the microphone. Unlike Principal Component Analysis (PCA), which seeks directions of maximum variance and might mix sources further, ICA focuses on maximizing statistical independence between the components. This characteristic makes ICA suitable for tasks where the <em>underlying sources are non-Gaussian and independent.</em></p>
<h3 id="procedure-for-applying-ica">Procedure for Applying ICA</h3>
<ol>
<li><strong>Preprocessing with PCA</strong>:
<ul>
<li><strong>Dimensionality Reduction</strong>: Initially, PCA is applied to reduce the dimensionality of the data. This step is crucial because it removes noise and reduces the complexity of the data, which simplifies the subsequent ICA.</li>
<li><strong>Whitening</strong>: The data is transformed into components that are uncorrelated and have unit variance. This transformation, often performed as part of PCA, is also known as &ldquo;whitening&rdquo;. It ensures that the ICA algorithm focuses only on finding the independent sources without being misled by possible correlations in the data.</li>
</ul>
</li>
<li><strong>Applying ICA</strong>:
<ul>
<li><strong>Identify Independent Components (ICs)</strong>: After whitening, the ICA algorithm seeks to rotate the whitened data to a new coordinate system where the statistical independence of the resulting signals is maximized. This involves optimizing an objective function that measures non-Gaussianity (since independence implies non-Gaussianity under certain conditions, see below section on Non-Gaussianity).</li>
<li><strong>Extraction of Sources</strong>: The independent components correspond to the original signals/sources that were mixed in the observed data. Each component should represent one source, such as a single voice in the context of audio processing.</li>
</ul>
</li>
</ol>
<h3 id="example-application">Example Application</h3>
<p>For instance, if you have a recording from a busy restaurant, ICA can help isolate single voices from the background noise and other voices. This technique is invaluable in environments where multiple sources overlap significantly but retain their independence in terms of their statistical signatures.</p>
<hr>
<h2 id="understanding-independence-non-gaussianity-and-the-central-limit-theorem">Understanding Independence, Non-Gaussianity, and the Central Limit Theorem</h2>
<p>The Central Limit Theorem (CLT) states that the sum of a large number of independent random variables, each with finite mean and variance, tends toward a Gaussian distribution, regardless of the original distributions of the variables. This principle is critical in the context of Independent Component Analysis (ICA), which is used to separate mixed signals into their original, independent components.</p>
<h3 id="implications-for-ica">Implications for ICA</h3>
<ul>
<li><strong>Role of the CLT</strong>: The CLT implies that a mixture of multiple independent, non-Gaussian signals tends to be <em>more Gaussian-like</em> than any of the individual signals. ICA utilizes this property by assuming that the original sources are non-Gaussian. The mixed signal, being more Gaussian, provides a clue that separation is possible by identifying and maximizing the non-Gaussian aspects of its components.</li>
<li><strong>Non-Gaussianity as a Tool</strong>: ICA algorithms focus on maximizing non-Gaussianity to separate the independent components. This is because non-Gaussian signals, when summed, lose some of their distinct statistical features, making the mixture more Gaussian. By finding projections of the data that are maximally non-Gaussian, ICA can effectively identify and separate the original independent sources.</li>
<li><strong>Challenge with Gaussian Sources</strong>: If the original sources were Gaussian, their sum would also be Gaussian, offering no statistical advantage for separation. This is why ICA is particularly powerful for non-Gaussian data sources, where it can exploit statistical differences to distinguish between the sources.</li>
</ul>
<h3 id="practical-considerations">Practical Considerations</h3>
<p>ICA is particularly effective in scenarios where the underlying sources of a signal are known to be non-Gaussian, such as in audio signal processing or in biomedical signal analysis. The ability to reverse the effects of the CLT and uncover the original signals from a complex mixture is what makes ICA a valuable tool in modern data analysis.</p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/tutorials/astro-stat/4-1-regression/" rel="next">Regression</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/tutorials/astro-stat/6-clustering-and-classification/" rel="prev">Clustering and Classification</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on May 3, 2024</p>

          





  
  

<p class="edit-page">
  <a href="https://github.com/gcushen/hugo-academic/edit/master/content/tutorials/astro-stat/5-1%20Multivariate%20Analysis.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>




          

        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    © 2018-2024 Shih-Yun Tang &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.dd460abf651c4e91377a0cc642a37eec.js"></script>

    






  
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
