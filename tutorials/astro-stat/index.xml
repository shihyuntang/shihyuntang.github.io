<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview | Shih-Yun Tang</title>
    <link>https://shihyuntang.github.io/tutorials/astro-stat/</link>
      <atom:link href="https://shihyuntang.github.io/tutorials/astro-stat/index.xml" rel="self" type="application/rss+xml" />
    <description>Overview</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018-2024 Shih-Yun Tang</copyright><lastBuildDate>Sun, 28 Apr 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shihyuntang.github.io/img/icon-192.png</url>
      <title>Overview</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/</link>
    </image>
    
    <item>
      <title>Probability Basic</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/0-some-basic/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/0-some-basic/</guid>
      <description>&lt;h2 id=&#34;some-terminology-in-statistics&#34;&gt;Some Terminology in Statistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;i.i.d. (Independent and Identically Distributed):&lt;/strong&gt;
This term refers to a set of random variables that are all independent of each other and share the same probability distribution. The assumption of i.i.d. is crucial in many statistical methods because it simplifies the mathematical analysis and inference processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Null Hypothesis:&lt;/strong&gt;
In hypothesis testing, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups tested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P-value:&lt;/strong&gt;
The p-value is the probability of observing test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A p-value less than a pre-determined significance level, often 0.05, leads to the rejection of the null hypothesis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Confidence Interval (CI):&lt;/strong&gt;
A confidence interval is a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter. The confidence level represents the probability that this interval will capture this parameter in repeated samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Type I and Type II Errors:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type I Error (False Positive):&lt;/strong&gt; Occurs when the null hypothesis is incorrectly rejected when it is actually true.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type II Error (False Negative):&lt;/strong&gt; Occurs when the null hypothesis is not rejected when it is actually false.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;first-and-second-moments&#34;&gt;First and Second Moments&lt;/h2&gt;
&lt;p&gt;In probability and statistics, the concepts of first and second moments are central to understanding the distributions of random variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;First Moment (Mean):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The first moment is the expected value of a random variable $X$, denoted as $E(X)$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt;
$$E(X) = \mu$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Second Moment (Variance):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The second moment about the mean is the variance of the random variable $X$, denoted as $\sigma^2$. It measures the spread of the data points around the mean.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The variance is calculated using the expectation of the &lt;em&gt;squared deviations from the mean&lt;/em&gt;:
$$
\sigma^2 = E[(X-\mu)^2] = E[X^2] - E(X)^2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$$\begin{align}
\sigma^2 &amp;amp;= E[(X-\mu)^2] \\
&amp;amp;= E[X^2 - 2X\mu + \mu^2] \\
&amp;amp;= E[X^2] - 2\mu E(X) + E(\mu^2) \\
&amp;amp;= E[X^2] - 2\mu^2 + \mu^2 \\
&amp;amp;= E[X^2] - \mu^2 \\
&amp;amp;= E[X^2] - E(X)^2
\end{align}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Third Moment (Skewness):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; The third central moment describes the skewness of the distribution of a random variable, indicating the degree of asymmetry around the mean. Skewness can reveal whether the distribution tails off more on one side than the other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The third moment is calculated using the expectation of the cubed deviations from the mean:
$$
E[(X - E(X))^3]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive skewness indicates that the distribution has a long tail to the right (more positive side), while a negative skewness indicates a long tail to the left (more negative side).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Covariance:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Definition:&lt;/strong&gt; Covariance measures the joint variability of two random variables, $X$ and $Y$. It assesses the degree to which two variables change together. If the greater values of one variable mainly correspond to the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The covariance between two variables $X$ and $Y$ is given by:
$$
\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))]
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt; A positive covariance indicates that as $X$ increases, $Y$ tends to increase. A negative covariance suggests that as $X$ increases, $Y$ tends to decrease. Zero covariance indicates that the variables are independent, assuming they are also uncorrelated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;standardization&#34;&gt;Standardization&lt;/h3&gt;
&lt;p&gt;Transforming the random variable to with zero mean and unit variance. This tranasformation also removes the unit on the random variable.&lt;/p&gt;
&lt;p&gt;$$
X_{std} = \frac{X - \mu}{\sigma}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;z-test-vs-t-test&#34;&gt;Z-Test vs. T-Test&lt;/h2&gt;
&lt;p&gt;Both the z-test and the t-test are statistical methods used to test hypotheses about means, but they are suited to different situations based on the distribution of the data and sample sizes.&lt;/p&gt;
&lt;h3 id=&#34;z-test&#34;&gt;Z-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The z-test is used when the population &lt;em&gt;variance is known and the sample size is large&lt;/em&gt; (typically, n &amp;gt; 30). It can also be used for small samples if the data is known to follow a normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population variance is known.&lt;/li&gt;
&lt;li&gt;The sample size is large enough for the Central Limit Theorem to apply, which ensures that the means of the samples are normally distributed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a z-test is calculated as follows:
$$
Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t-test&#34;&gt;T-Test&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usage:&lt;/strong&gt; The t-test is used when the population &lt;em&gt;variance is unknown and the sample size is small&lt;/em&gt;. It is the appropriate test when dealing with estimates of the standard deviation from a normally distributed sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The population from which samples are drawn is normally distributed.&lt;/li&gt;
&lt;li&gt;The population variance is unknown, and the sample variance is used as an estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formula:&lt;/strong&gt; The test statistic for a t-test is calculated as follows:
$$
T = \frac{\bar{X} - \mu_0}{s/\sqrt{n}}
$$
where $\bar{X}$ is the sample mean, $\mu_0$ is the hypothesized population mean, $s$ is the sample standard deviation, and $n$ is the sample size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Degrees of Freedom:&lt;/strong&gt; The degrees of freedom for the t-test are $n-1$, which affects the shape of the t-distribution used to determine the p-value.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-differences&#34;&gt;Key Differences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Deviation:&lt;/strong&gt; The z-test uses the population standard deviation, while the t-test uses the sample&amp;rsquo;s standard deviation as an estimate of the population’s standard deviation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size:&lt;/strong&gt; The z-test is typically used for larger sample sizes or when the population standard deviation is known, whereas the t-test is used for smaller sample sizes or when the population standard deviation is unknown.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution:&lt;/strong&gt; The z-test statistic follows a normal distribution, while the t-test statistic follows a t-distribution, which is more spread out with heavier tails, providing a more conservative test for small sample sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;A comparison plot between the t-distribution and the standard normal distribution can be find &lt;a href=&#34;https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html#:~:text=What&#39;s%20the%20key%20difference%20between,on%20the%20sample%20standard%20deviation.&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributions</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/1-probability/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/1-probability/</guid>
      <description>&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;Probability distributions describe how the probabilities of a &lt;strong&gt;random variable&lt;/strong&gt; are distributed. Here are the two main types of probability functions associated with these distributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Probability Density Function (PDF, continuous):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PDF is used to specify the probability of a random variable falling within a particular range of values, rather than taking any one specific value. This function is applicable only to continuous variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The area under the PDF curve between two points represents the probability of the variable falling within that range.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Probability Mass Function (PMF, discrete):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The PMF is used for discrete random variables and specifies the probability that a random variable is exactly equal to some value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The sum of all the probabilities represented by the PMF must equal 1, as it accounts for every possible discrete value the variable can take.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; The binomial distribution, and the Poisson distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Distribution Function (CDF):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; The CDF is used to determine the probability that a random variable (X) is $\lesssim$ to a certain value. CDF can be applied to both discrete and continuous random variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key Characteristic:&lt;/strong&gt; The CDF is a non-decreasing function that ranges from 0 to 1. For continuous variables, it is obtained by integrating the PDF over the range of possible values. For discrete variables, it is the cumulative sum of the PMF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; For normal distribution, the CDF is used to calculate the probability that the variable is less than a specific threshold, which is central to hypothesis testing and confidence interval estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;chebyshevs-theorem&#34;&gt;Chebyshev&amp;rsquo;s Theorem&lt;/h2&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem, also known as Chebyshev&amp;rsquo;s Inequality, is a fundamental result in probability theory that provides a way &lt;strong&gt;to estimate the probability that a random variable differs from its mean&lt;/strong&gt;. This theorem is not restricted to normally distributed data, making it very versatile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem Statement:&lt;/strong&gt;
$$
P(|X-\mu| &amp;lt; k\sigma) \geq 1 - \frac{1}{k^2}
$$
where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;($\mu$) is the mean of the random variable ( X ),&lt;/li&gt;
&lt;li&gt;($\sigma$) is the standard deviation of ( X ),&lt;/li&gt;
&lt;li&gt;($k$) is a positive number greater than 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generality:&lt;/strong&gt; Chebyshev&amp;rsquo;s theorem applies to any probability distribution where the mean and variance are defined.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implication:&lt;/strong&gt; The inequality tells us that no matter the shape of the distribution, the proportion of values that fall within ($k$) standard deviations of the mean is at least $( 1 - \frac{1}{k^2})$. For example, at least (75%) of the values lie within 2 standard deviations of the mean (since ($k=2$) gives $( 1-\frac{1}{4} = 0.75)$).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; While the theorem provides a lower bound, it does not give exact probabilities, and the bounds can be quite loose, especially for distributions that tightly clustered around the mean.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chebyshev&amp;rsquo;s Theorem is particularly useful for analysts and statisticians dealing with samples from unknown distributions, as it provides a safe, conservative estimate of the spread of data around the mean.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;discrete-distributions&#34;&gt;Discrete Distributions&lt;/h2&gt;
&lt;h3 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The Binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, with each trial having two possible outcomes, typically labeled as &amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are only two possible outcomes for each trial: success (1) and failure (0).&lt;/li&gt;
&lt;li&gt;The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability mass function (PMF) for the Binomial distribution is expressed as:
$$
P(X=x) = C^{n}_{x} \theta^x (1-\theta)^{n-x}
$$
where $\theta$ is the probability of success on a single trial, $x$ is the number of successes, and $n$ is the total number of trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Consider flipping a fair coin five times. What is the probability of getting exactly two heads?&lt;/p&gt;
&lt;p&gt;The calculation is as follows:
$$
P(X=2) = C^{5}_{2} 0.5^2 (1-0.5)^3 = \frac{5!}{2!3!} \times 0.25 \times 0.125 = 31.25%
$$&lt;/p&gt;
&lt;h3 id=&#34;geometric-distribution&#34;&gt;Geometric Distribution&lt;/h3&gt;
&lt;p&gt;The Geometric distribution describes the probability of observing the first success on the $x$-th trial in a sequence of Bernoulli trials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF:&lt;/strong&gt;
$$
g(x; \theta) = \theta(1-\theta)^{x-1}
$$
where, $\theta$ is the probability of success on each trial, and $x$ is the trial number of the first success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Properties:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mean ($\mu$):&lt;/strong&gt; The expected number of trials to get the first success is given by:
$$
\mu = \frac{1}{\theta}
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variance ($\sigma^2$):&lt;/strong&gt; The variance of the number of trials to get the first success is:
$$
\sigma^2 = \frac{1-\theta}{\theta^2}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Interpretation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mean and variance provide insights into the &amp;ldquo;spread&amp;rdquo; or variability of trials needed to achieve the first success, with higher values of $\theta$ leading to fewer expected trials.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distribution is commonly used in &lt;em&gt;quality control&lt;/em&gt;, &lt;em&gt;reliability testing&lt;/em&gt;, and other areas where the &amp;ldquo;time&amp;rdquo; or number of trials until the first success is of interest.&lt;/p&gt;
&lt;h3 id=&#34;poisson-distribution&#34;&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space. It assumes these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;
&lt;p&gt;A practical example is photometry using a Charge-Coupled Device (CCD), where light—specifically, the number of photons—hits the CCD at a constant rate. Importantly, the arrival of photons is assumed to be independent of previous arrivals, provided the CCD is not saturated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Independence of events holds only if the CCD is not saturated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rate of event occurrence, $\lambda$, is constant over time. For example, the number of events occurring between $t$ and $t + \Delta t$ is $\lambda \Delta t$.&lt;/li&gt;
&lt;li&gt;The probability of an event in the interval ${t, t+\Delta t}$ is independent of previous events.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The probability mass function (PMF) of the Poisson distribution is defined as:
$$
P(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
$$
where $\lambda$ is the expected number of events in a given interval, and $x$ is the observed number of events.&lt;/p&gt;
&lt;p&gt;For the Poisson distribution, the expected value (mean) is $\mu = \lambda$, and the variance is $\sigma^2 = \lambda$.&lt;/p&gt;
&lt;h4 id=&#34;poisson-noise-shot-noise&#34;&gt;Poisson Noise (Shot Noise)&lt;/h4&gt;
&lt;p&gt;Since $\sigma^2 = \lambda$, the standard deviation (Poisson noise) is $\sigma = \sqrt{\lambda}$. In photometry, where $N$ represents the number of photons hitting the CCD, this implies $\sigma = \sqrt{N}$.&lt;/p&gt;
&lt;p&gt;Due to the characteristics of the Poisson distribution, the signal-to-noise ratio (SNR) is calculated as follows:
$$
SNR = \frac{\mu}{\sigma} = \frac{\mu}{\sqrt{\mu}} = \sqrt{\mu}
$$
This relationship highlights the inherent noise properties in photon-counting measurements like those in CCD photometry.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Tests for Assessing Distribution</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-1-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-1-distribution/</guid>
      <description>&lt;h2 id=&#34;empirical-distribution-function-edf&#34;&gt;Empirical Distribution Function (EDF)&lt;/h2&gt;
&lt;p&gt;The Empirical Distribution Function (EDF) is a discrete version of the Cumulative Distribution Function (CDF). It is defined as:
$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n I[x_i \leq x]
$$
where $I[\xi]$ is the indicator function, equating to 1 if the condition is true and 0 otherwise. This means each step in the EDF has a height of $\frac{1}{n}$. An example plot of an EDF is shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/edf.png&#34; alt=&#34;edf&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kolmogorov-smirnov-test-ks-test&#34;&gt;Kolmogorov-Smirnov Test (KS Test)&lt;/h2&gt;
&lt;p&gt;The Kolmogorov-Smirnov (KS) Test measures the maximum distance ($D$) between two distribution functions. This can be used to compare an empirical distribution with a theoretical model (one-sample test) or two empirical distributions (two-sample test). If the distributions are identical, $D$ equals zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistic:&lt;/strong&gt;
$$
D_n = \max_x |F_n(x) - S_n(x)|
$$
A table for the critical values of $D_n$ can be found &lt;a href=&#34;https://people.cs.pitt.edu/~lipschultz/cs1538/prob-table_KS.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If $D_n$ exceeds the critical value, we can &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis that there is no significant difference between the two distributions.&lt;/p&gt;
&lt;h2 id=&#34;cramér-von-mises-statistic-cvm&#34;&gt;Cramér-von Mises Statistic (CvM)&lt;/h2&gt;
&lt;p&gt;The Cramér-von Mises statistic is used to quantify the goodness of fit of an empirical distribution to a theoretical model. It is particularly useful as it considers the squared differences over all points, providing a more sensitive measure to differences &lt;strong&gt;in the tails&lt;/strong&gt; of the distributions.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
C_n = n \int_{-\infty}^\infty [F_n(x) - S(x)]^2 dS(x)
$$ --&gt;
&lt;p&gt;This statistic assesses the integrated squared distance between the empirical distribution function $F_n(x)$ and the theoretical distribution $S(x)$, weighted by the number of observations $n$.&lt;/p&gt;
&lt;h2 id=&#34;anderson-darling-statistic-ad&#34;&gt;Anderson-Darling Statistic (AD)&lt;/h2&gt;
&lt;p&gt;The Anderson-Darling statistic is a modification of the Cramér-von Mises statistic that gives more weight to the tails of the distribution. It is particularly effective in identifying departures from a theoretical distribution in the tails.&lt;/p&gt;
&lt;!-- **Statistic:**
$$
A^2 = n \int_{-\infty}^\infty \frac{[F_n(x) - S(x)]^2}{S(x)(1 - S(x))} \, dx
$$ --&gt;
&lt;p&gt;This weighted approach makes the AD statistic more sensitive to discrepancies in the distribution&amp;rsquo;s tails than the CvM statistic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Both CvM and AD tests are powerful tools for statistical hypothesis testing, especially in scenarios where understanding the tail behavior of distributions is crucial.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;wilcoxon-rank-sum-test&#34;&gt;Wilcoxon Rank Sum Test&lt;/h2&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test, also known as the Mann-Whitney Rank Sum test, is designed to assess &lt;em&gt;whether two independent samples come from the same distribution&lt;/em&gt;. Here’s how the test statistic is calculated:&lt;/p&gt;
&lt;!-- It is especially useful when the data does not meet the assumptions necessary for the t-test, primarily concerning normality --&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Combine and Rank the Data:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Combine all observations from both samples into a single dataset.&lt;/li&gt;
&lt;li&gt;Rank all observations from the smallest to largest. Ties are given a rank equal to the average of the ranks they would have otherwise occupied.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculate the Rank Sums:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Calculate the sum of the ranks for observations from each sample separately. Let $T_1$ be the sum of ranks for the first sample, and $T_2$ for the second sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute the Test Statistic:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The test statistic $U$ is calculated using:
$$
U = T_1 - \frac{n_1(n_1+1)}{2}
$$
where $n_1$ is the number of observations in the first sample. $U$ can also be computed for the second sample, and the smaller of the two $U$ values is often used as the test statistic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine the Significance:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;The significance of the observed $T_1$ value is determined by comparing it to values in a reference distribution, which approximates a normal distribution under the null hypothesis when the sample sizes are sufficiently large. The mean and standard deviation of $T_1$ are used to compute a z-score:
$$
|z| = \left| \frac{T_1 - \text{mean}(T_1)}{\text{std dev}(T_1)} \right|
$$
where $\text{mean}(T_1) = \frac{n_1(n_1+n_2+1)}{2}$, and $\text{Var}(T_1) = \frac{n_1 n_2(n_1+n_2+1)}{2}$.&lt;/li&gt;
&lt;li&gt;The p-value is then calculated from the normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ### Interpretation

- If the p-value is less than the chosen significance level (commonly 0.05), then there is sufficient evidence to reject the null hypothesis, suggesting that there is a statistically significant difference in the distributions of the two groups.

- If the p-value is greater, then we do not reject the null hypothesis, suggesting that any observed differences could reasonably occur by random chance under the assumption of identical distributions. --&gt;
&lt;p&gt;The Wilcoxon Rank Sum Test does not require the data to follow a specific distribution, making it a robust and widely applicable non-parametric method for comparing two samples.&lt;/p&gt;
&lt;h2 id=&#34;kruskal-wallis-test&#34;&gt;Kruskal-Wallis Test&lt;/h2&gt;
&lt;p&gt;The Kruskal-Wallis (KW) test is used to determine if there are statistically significant differences between the distributions of two or more groups of an independent variable. It generalizes the Wilcoxon Rank Sum Test to more than two groups. The null hypothesis assumes that all groups come from identical distributions. The test statistic follows a chi-squared ($\chi^2$) distribution with $k-1$ degrees of freedom, where $k$ is the number of groups.&lt;/p&gt;
&lt;h2 id=&#34;comparison-of-statistical-distribution-tests&#34;&gt;Comparison of Statistical Distribution Tests&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Key Usage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Data Requirement&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Distribution Assumption&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;KS Test&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing a sample with a reference distribution&lt;/td&gt;
&lt;td&gt;One or two samples, continuous or ordinal&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in location, scale, and shape&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CvM Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit to a theoretical distribution&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Integrates squared differences; sensitive across entire distribution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AD Statistic&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Testing goodness of fit with emphasis on tail differences&lt;/td&gt;
&lt;td&gt;One sample, continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Increased weight to tails; highly sensitive to tail discrepancies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Wilcoxon Rank Sum&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing two independent samples&lt;/td&gt;
&lt;td&gt;Two independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Sensitive to differences in medians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Kruskal-Wallis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Comparing more than two independent samples&lt;/td&gt;
&lt;td&gt;Two or more independent samples, ordinal or continuous&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Generalization of Wilcoxon, sensitive to differences across multiple samples&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/5-2-correlation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/5-2-correlation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data smoothing-density estimation</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/6-data-smoothing/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/6-data-smoothing/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/7-regression/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/7-regression/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multivariate Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/8-multivariate-analysis/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/8-multivariate-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustering and Classification</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/9-clustering-and-classification/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/9-clustering-and-classification/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Censored and Truncated Data</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/10-censored-and-truncated-data/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/10-censored-and-truncated-data/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timeseries Analysis</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/11-timeseries-analysis/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/11-timeseries-analysis/</guid>
      <description>&lt;p&gt;Coming Soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variogram</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/12-1-variogram/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/12-1-variogram/</guid>
      <description>&lt;h2 id=&#34;introduction-to-variograms&#34;&gt;Introduction to Variograms&lt;/h2&gt;
&lt;p&gt;A variogram is a fundamental tool in spatial statistics used to describe the spatial dependence and variability of data. It quantifies how data values at different locations relate to one another over space, essentially measuring the degree of spatial correlation. The variogram has lek features of: &amp;ldquo;nugget,&amp;rdquo; &amp;ldquo;sill,&amp;rdquo; and &amp;ldquo;range.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nugget:&lt;/strong&gt; Represents the variation at small distances attributable to measurement errors or spatial microscale variation not resolved by the sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sill:&lt;/strong&gt; The plateau reached by the variogram, beyond which the increments in distance do not significantly increase the variance. It represents the level of total variance within the dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; The distance at which the variogram reaches the sill, beyond which locations are no longer correlated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-variogram&#34;&gt;Example of Spatial Data and Variogram&lt;/h2&gt;
&lt;p&gt;Plots below show an example of spatial data (left) and its associated variogram (right). The plot on the left shows synthetic data in spatial X-Y coordinates color-coded by the level of toxicity measured at that location. The size of the circle is associated with the measurement error, which is not used here. The variogram on the right shows how the semi-variance between points increases with distance. It features a &amp;ldquo;nugget&amp;rdquo; effect at the origin, indicating measurement noise or microscale variability. The curve approaches a &amp;ldquo;sill,&amp;rdquo; beyond which the variance stabilizes, suggesting that points beyond this &amp;ldquo;range&amp;rdquo; do not influence each other. This range is critical for understanding the spatial continuity and predicting values at unsampled locations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/variogram.png&#34; alt=&#34;targets&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-variogram&#34;&gt;Example R Code for the Variogram&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(gstat)
library(sp)

# Defining spatial coordinates
# df_2 is a dataframe with colums x and y 
coordinates(df_2) &amp;lt;- ~x+y  

# Creating Variograms
variogram_tox &amp;lt;- variogram(toxicity ~ 1, df_2)

# Fit the variogram and plot it out.
# gamma: the semi-variance
# vgm: &amp;quot;variogram model,&amp;quot; 
model_tox &amp;lt;- fit.variogram(
  variogram_tox, model = vgm(psill = max(variogram_tox$gamma), 
  model = &amp;quot;Sph&amp;quot;, 
  range = 30))

# Plot the empirical variogram and the fitted model
plot(variogram_tox, model = model_tox, 
  main = &amp;quot;Toxicity Variogram with Model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kriging</title>
      <link>https://shihyuntang.github.io/tutorials/astro-stat/12-2-kriging/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0100</pubDate>
      <guid>https://shihyuntang.github.io/tutorials/astro-stat/12-2-kriging/</guid>
      <description>&lt;h2 id=&#34;introduction-to-kriging&#34;&gt;Introduction to Kriging&lt;/h2&gt;
&lt;p&gt;Kriging is a geostatistical interpolation technique that uses spatial correlation models, such as variograms, to predict values at unsampled locations based on the values at sampled locations. There are several types of Kriging, each with specific assumptions and applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Kriging:&lt;/strong&gt; Assumes the mean of the random field is known and constant throughout the region of interest.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ordinary Kriging:&lt;/strong&gt; Assumes the &lt;em&gt;mean is unknown&lt;/em&gt; but constant within the region of interest and is the most commonly used form as one do not know the mean in real world.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-on-variogram&#34;&gt;More on Variogram&lt;/h2&gt;
&lt;p&gt;The semivariance $\gamma(x_1, x_2)$ between two points can be expressed as:
$$
\gamma(d) = \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2
$$
where $n$ is the number of pairs, $d$ is the distance between two points, and $z$ represents the values at the locations. The calculation of $\gamma(d)$ involves several key assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stationary (homogeneous):&lt;/strong&gt; Assumes that the statistical properties (mean, variance) of the process do not change over space. This implies that the mean and variance are constant throughout the region of interest, and the covariance between any two points depends only on the distance and direction between them, not on their absolute locations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isotropy:&lt;/strong&gt; Assumes that the statistical properties are the same in all directions. This means that the variogram is a function only of the distance between sample points, not of the direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Kriging, we use $\gamma$ to weight the data for interpolation:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\gamma(d) &amp;amp;= \frac{1}{2n} \sum_{i=1}^n [z(x_i + d) - z(x_i)]^2 \\
&amp;amp;= \frac{1}{2} E\left(\left[z(x+d) - z(x)\right]^2\right)
\text{(homogeneous assumption)} \\
&amp;amp;= \frac{1}{2} \left\{ E\left[ z^2(x+d) \right] + E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\} \\
&amp;amp;= \frac{1}{2} \left\{ 2E\left[ z^2(x) \right] - 2E\left[ z(x+d) z(x) \right]\right\}
\text{(homogeneous assumption)} \\
&amp;amp;= \sigma_x^2 + \mu_x^2 - \text{cov}[z(x), z(x+d)] - \mu_x^2
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the relationships: $E[x^2] = \sigma_x^2 + [E(x)]^2$ and $\text{cov}[X,Y] = E(XY) - E(X)E(Y)$ were used to get the second to last equation. We have:
$$\begin{align}
\gamma(d) &amp;amp;= \sigma^2 - \text{cov}[z(x), z(x+d)] \\
&amp;amp;= \sigma^2 - \text{c}(d)
\end{align}$$&lt;/p&gt;
&lt;p&gt;Therefore, we have the semivariance at distance $d$ is the variance minus the covariance between points at this distance.&lt;/p&gt;
&lt;h2 id=&#34;example-of-spatial-data-and-the-kriging-result&#34;&gt;Example of Spatial Data and the Kriging Result&lt;/h2&gt;
&lt;p&gt;The plots below show an example of spatial data (left) and its associated Kriging map (right). The data used here is the same as in the Variogram page, and the Kriging map uses the variogram model shown on the Variogram page to predict values at unsampled locations. This example demonstrates how Kriging utilizes the spatial structure of the data, as defined by the variogram, to provide a statistically optimal interpolation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shihyuntang.github.io/ast-stat/kriging.png&#34; alt=&#34;kriging&#34;&gt;
&lt;em&gt;Figure data credit: Charles M Pacheco&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-r-code-for-the-kriging&#34;&gt;Example R Code for the Kriging&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Fit a variogram model
vgm_model &amp;lt;- vgm(psill = max(variogram_tox$gamma), 
       model = &amp;quot;Sph&amp;quot;, range = 30)
       
x.range &amp;lt;- range(df_2$x)
y.range &amp;lt;- range(df_2$y)

grid.points &amp;lt;- expand.grid(
       x = seq(from = x.range[1], to = x.range[2], by = 1),
       y = seq(from = y.range[1], to = y.range[2], by = 1))

# Convert to SpatialPoints
grid &amp;lt;- SpatialPoints(grid.points)

# Perform ordinary kriging
kriged &amp;lt;- krige(toxicity ~ 1, df_2, model = vgm_model, newdata = grid)

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res=200)
# Create a map with overlay contours
spplot(kriged, &amp;quot;var1.pred&amp;quot;, main = &amp;quot;Kriging Map for Toxicity&amp;quot;, 
       xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;,
       sp.layout = list(&amp;quot;sp.points&amp;quot;, df_2, col = &amp;quot;green&amp;quot;),
       colorkey = TRUE,
       scales = list(draw = TRUE))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
